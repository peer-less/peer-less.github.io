<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2016-CLA-A New Practical Tool for Performing Interactive Exploration over Concept Lattices</title>
    <link href="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/"/>
    <url>/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/</url>
    
    <content type="html"><![CDATA[<h1 id="LatViz-A-New-Practical-Tool-for-Performing-Interactive-Exploration-over-Concept-Lattices"><a href="#LatViz-A-New-Practical-Tool-for-Performing-Interactive-Exploration-over-Concept-Lattices" class="headerlink" title="LatViz: A New Practical Tool for Performing Interactive Exploration over Concept Lattices"></a>LatViz: A New Practical Tool for Performing Interactive Exploration over Concept Lattices</h1><p class="note note-info">属性探索、应用工具</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Mehwish Alam，Thi Nhu Nguyen Le，and Amedeo Napoli。</p><p>​            2. 会议：CLA。</p><p>​            3. 时间：2016。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol><li>由于web of Data(WOD) 的增长，在探索、交互、分析和发现方面出现了许多新的挑战。</li><li>一个基本问题：在WOD之上获得这些概念格之后，用户如何通过概念格交互地探索和分析这些数据。</li><li>引入一个新工具：LatViz；原功能：构造概念格及其导航；新功能：专家交互、模式结构的可视化、AOC偏序集、概念标注、基于若干过滤准则的概念格以及蕴涵的直观可视化。</li></ol><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li>WOD 包含的所有信息都以实体和关系的形式表示，从而允许将语义嵌入到该数据的表示中。</li><li>WOD主要以资源描述框架(RDF，Resource Description Framework)的形式表示数据。</li><li>已有几种方法可以访问此数据，并用于数据分析的可视化与交互探索。<ol><li>LODLive$^{[1]}$：用户可以选择 DBpedia 和 Freebase 等数据集，并指定一个实体作为浏览节点弧标记图的起点。</li><li>RelFinder$^{[2]}$：在给定几个实体的情况下，该工具会自动找到连接这些实体的路径。</li></ol></li><li>这些工具有助于深入了解 RDF 图包含的内容，但它们不是为知识发现的目的而构建的。</li><li>本文介绍了一个新的工具 LatViz，它通过引入多种功能来显著改善用户与概念格之间的交互性。</li></ol><h2 id="2-Motivating-Example"><a href="#2-Motivating-Example" class="headerlink" title="2 Motivating Example"></a>2 Motivating Example</h2><p>一个例子：搜索特定团队与其研究领域相关的会议或期刊的论文。</p><p>1.通过特定的关键字或作者名，找到感兴趣的论文。</p><p>2.通过增加更多的查询条件以缩小查询范围，获得关于特定关键字或作者组的论文。</p><p>3.如果专家想要了解团队与团队外其他研究界成员的合作情况，以及团队成员的多样性和专业性，这不是简单的查询就能直接获得的。</p><p>4，基于这个场景，本文展示了如何通过一个合适的可视化工具来指导专家，以便在概念格的帮助下获得这些感兴趣的信息。</p><h2 id="3-Preliminaries"><a href="#3-Preliminaries" class="headerlink" title="3 Preliminaries"></a>3 Preliminaries</h2><h3 id="3-1-Pattern-Structures"><a href="#3-1-Pattern-Structures" class="headerlink" title="3.1 Pattern Structures"></a>3.1 Pattern Structures</h3><p>基本概念：</p><script type="math/tex; mode=display">\begin{align}&模式结构：(G，(D, \sqcap)，\delta) \newline&G：对象集。 \newline&(D，\sqcap)：描述 D 的交半格。 \newline&\delta ：G\rightarrow D，将对象映射到其描述。\end{align}</script><p>算子运算：</p><script type="math/tex; mode=display">\begin{align}A^{\Box} &= \sqcap_{g\in A}\delta(g) \quad &for\ A\subseteq G \newlined^{\Box} &= \{ g\in G | d\sqsubseteq \delta(g) \} \quad &for\ d\in D \end{align}</script><p>偏序关系：</p><script type="math/tex; mode=display">\begin{align}&c\sqsubseteq d \Leftrightarrow c\sqcap d = c \newline&(A_1, d_1) \le (A_2, d_2) \Leftrightarrow A_1\subseteq A_2(d_2\sqsubseteq d_1)\end{align}</script><p>概念：</p><script type="math/tex; mode=display">\begin{align}A^{\Box} &= d \newlineA &= d^{\Box}\end{align}</script><p>区间模式结构（Interval Pattern Structures）：</p><script type="math/tex; mode=display">\begin{align}&c = (a_i, b_i) \quad d = (e_i, f_i) \newline&c \sqcap d = (min(a_i, e_i)，max(b_i, f_i))\end{align}</script><h3 id="3-2-Web-of-Data-and-its-Classification"><a href="#3-2-Web-of-Data-and-its-Classification" class="headerlink" title="3.2 Web of Data and its Classification"></a>3.2 Web of Data and its Classification</h3><p>RDF被写作三元组：&lt;$subject, predicate, object$&gt;。</p><p>DBLP的RDF存储示例：</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806113720037.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806113720037.png" srcset="/img/loading.gif" alt></p><p>$t_1$：论文$s_1$具有“模式结构”的关键字。</p><p>为了允许对RDF数据进行交互式数据探索，专家通过定义任务需求提出了一组限制条件，并根据该任务需求创建SPARQL查询以获得特定的数据。</p><h2 id="4-LatViz-for-Interactive-Exploration-of-Concept-Lattices"><a href="#4-LatViz-for-Interactive-Exploration-of-Concept-Lattices" class="headerlink" title="4 LatViz for Interactive Exploration of Concept Lattices"></a>4 LatViz for Interactive Exploration of Concept Lattices</h2><h3 id="4-1-User-Interface"><a href="#4-1-User-Interface" class="headerlink" title="4.1 User Interface"></a>4.1 User Interface</h3><p>LatViz：实现两种从二进制的形式背景中构建概念格的算法。</p><p>其中一种在$^{[7]}$，另一种构建概念格的高效算法是 AddIntent$^{[8]}$。</p><p>演示地址：<a href="http://latviz.loria.fr/latviz/。" target="_blank" rel="noopener">http://latviz.loria.fr/latviz/。</a></p><p>一个例子</p><p>基于 Table 1 将$Subject$作为对象集，将$Object$作为属性集。</p><p>RDF三元组：LORIA知识发现团队的出版物。</p><p>对象：343个。</p><p>属性：1516个。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806115136696.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806115136696.png" srcset="/img/loading.gif" alt></p><h3 id="4-2-AOC-Posets"><a href="#4-2-AOC-Posets" class="headerlink" title="4.2 AOC-Posets"></a>4.2 AOC-Posets</h3><p>AOC-Posets：属性和对象概念的偏序集合，在$^{[9, 10]}$中首次引入。</p><script type="math/tex; mode=display">\begin{align}&对象概念：(g^{"}, g') \quad &g\in G \newline&属性概念：(m', m^{"}) \quad &m\in M\end{align}</script><p>对象概念$(g^{“}, g’)$外延包含$g$的低概念集合，从下到上。</p><p>属性概念$(m^{“}, m’)$内涵包含$m$的高概念集合，从上到下。</p><h3 id="4-3-Displaying-Concept-Lattice-Level-wise"><a href="#4-3-Displaying-Concept-Lattice-Level-wise" class="headerlink" title="4.3 Displaying Concept Lattice Level-wise"></a>4.3 Displaying Concept Lattice Level-wise</h3><p>LatViz允许通过交互来层次化地创建概念格。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121302798.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121302798.png" srcset="/img/loading.gif" alt></p><p>第一层：专家使用 Amedeo Napoli 来定位论文，显示由 Amedeo Napoli 撰写的论文数量为152.</p><p>第二层：加入formal concept analysis来定位论文，结果由152降到了55。</p><h3 id="4-4-Display-Sub-Super-Concepts-of-a-Concept"><a href="#4-4-Display-Sub-Super-Concepts-of-a-Concept" class="headerlink" title="4.4 Display Sub/Super Concepts of a Concept"></a>4.4 Display Sub/Super Concepts of a Concept</h3><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121541550.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121541550.png" srcset="/img/loading.gif" alt></p><h3 id="4-5-Display-Hide-the-Sub-lattice"><a href="#4-5-Display-Hide-the-Sub-lattice" class="headerlink" title="4.5 Display/Hide the Sub-lattice"></a>4.5 Display/Hide the Sub-lattice</h3><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121758814.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121758814.png" srcset="/img/loading.gif" alt></p><p>专家只对Amedeo Napoli 关于知识表示的论文感兴趣的概念子格。</p><h3 id="4-6-Interval-Pattern-Structures"><a href="#4-6-Interval-Pattern-Structures" class="headerlink" title="4.6 Interval Pattern Structures"></a>4.6 Interval Pattern Structures</h3><p>本文提取了论文的三个属性，即论文发表的年份、论文发表时的会议排名以及最终的页数。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123124284.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123124284.png" srcset="/img/loading.gif" alt></p><h3 id="4-7-Lattice-Filtering-Criteria"><a href="#4-7-Lattice-Filtering-Criteria" class="headerlink" title="4.7 Lattice Filtering Criteria"></a>4.7 Lattice Filtering Criteria</h3><p>两类过滤：一是二值形式背景构建的概念格；二是区间模式结构构建的概念格。</p><p>过滤条件：stability, lift, extent size, intent size。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123454487.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123454487.png" srcset="/img/loading.gif" alt></p><p>查找Amedeo Napoli发表的关于模式结构和FCA主题的论文。</p><p>属性数量设为 3（Amedeo Napoli、formal concept analysis、pattern structure）。</p><p>寻找2012-2015年在排名1-4的会议上发表的论文，并且具有不少于2页且不超过42页的页数，则可以为所有三个属性的值设置相应的过滤器。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124240193.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124240193.png" srcset="/img/loading.gif" alt></p><h3 id="4-8-Attribute-Implications"><a href="#4-8-Attribute-Implications" class="headerlink" title="4.8  Attribute Implications"></a>4.8  Attribute Implications</h3><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124601090.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124601090.png" srcset="/img/loading.gif" alt></p><h2 id="5-Related-Tools"><a href="#5-Related-Tools" class="headerlink" title="5   Related Tools"></a>5   Related Tools</h2><p>RV-Xplorer（RDF View Explorer）$^{[3]}$：一个在RDF图形上可视化视图的工具，主要用于识别数据中感兴趣的部分并允许数据分析。它还被扩展为集群SPARQL查询答案。</p><p>CREDO$^{[12]}$和FooCA$^{[13]}$：从针对搜索引擎提出的查询中获取答案，并创建概念网格，然后将其显示给专家进行交互。</p><p>CEM$^{[15]}$：一个电子邮件管理器，允许快速搜索电子邮件，通常处理较小的概念格。</p><p>Gamelis$^{[16]}$：一个基于FCA的文档组织系统，允许几个导航操作。</p><p>Sewelis$^{[17]}$和Sparkis$^{[18]}$：允许在知识图上导航/交互。</p><p>ToscanaJ$^{[19]}$：在算法$^{[7]}$的帮助下，重用了构建概念格的源代码。它不仅可以应用于WOD，而且已经扩展到可以解释任何类型的数据。</p><h2 id="6-Discussion-and-Future-Improvements"><a href="#6-Discussion-and-Future-Improvements" class="headerlink" title="6 Discussion and Future Improvements"></a>6 Discussion and Future Improvements</h2><p>1.从未来的角度看，希望增加实现模式结构的变体。</p><p>2.将蕴涵扩展到关联规则。</p><p>3.考虑矩阵因子分解。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>应用工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学与探索-蕴涵的决策蕴涵表示研究</title>
    <link href="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/"/>
    <url>/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="蕴涵的决策蕴涵表示研究"><a href="#蕴涵的决策蕴涵表示研究" class="headerlink" title="蕴涵的决策蕴涵表示研究"></a>蕴涵的决策蕴涵表示研究</h1><div class="note note-primary">            <p>属性探索、决策蕴涵</p>          </div> <a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：王亚丽、翟岩慧、张少霞、贾楠、李德玉。</p><p>​            2.期刊：计算机科学与探索。</p><p>​            3.时间：2020/07/23。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.决策蕴涵是一种特殊的蕴涵，而决策蕴涵的研究就是在蕴涵中建立并研究一个/多个封闭的子系统(包括决策蕴涵子系统及相应的语义和语构子系统)。</p><p>2.为了进一步厘清蕴涵和决策蕴涵之间的关系，对由决策蕴涵子系统能不能得到整蕴涵系统进行了研究。</p><p>3.首先给出了蕴涵可以由决策蕴涵表示的充要条件；</p><p>接着通过实例表明，存在一些蕴涵不可由决策蕴涵表示，因此，进一步区分了直接表示和间接表示；</p><p>随后，通过研究决策背景中只有一个决策属性时不可被直接表示的蕴涵所具有的特点，给出了蕴涵不可由决策蕴涵直接表示的充要条件，并给出了不可被直接表示蕴涵的生成方法。</p><p>4.这种研究为蕴涵和规范基的研究提供了一种新视角，同时也为形式概念分析更深入的理论研究工作奠定了基础。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>1.形式概念分析(Formal Concept Analysis，FCA)是由德国 Wille R 教授于 1982 年提出的一种通过形式背景建立概念格来对数据进行分析和提取规则的一个强有力的工具[1-2]。</p><p>2.目前，FCA 已被广泛地应用到机器学习、社会网络、软件工程、信息检索、基于认知的概念学习、知识约简等相关领域[3-15]。</p><p>3.形式概念分析(概念格)中对知识获取的研究就是对蕴涵的研究[16-21]，但由于蕴涵数目庞大，无法满足用户的需求，因此如何获取完备无冗余的蕴涵规则集仍是研究的热点[17]。</p><p>4.文献[17]从逻辑方面对完备性和无冗余性进行了讨论，其中 Ganter 等已经讨论了蕴涵的语义特征和语构特征，提出了三条蕴涵推理规则，而且证明了这三条推理规则相对于蕴涵的语义是完备的，并给出了源于文献[22]的一个蕴涵基(完备无冗余的蕴涵集合)。。已经证明，该蕴涵基在所有蕴涵基中所含的蕴涵个数最少。</p><p>5.曲开社等进一步讨论蕴涵和逻辑的关系，提出了一种新的蕴涵基，并给出一种有效的方法来生成该蕴涵基[23]。</p><p>6.为了减少蕴涵的数目，曲开社等提出了决策背景及决策蕴涵的概念[24]，并讨论了一种直观的推理规则( 推理规则)，该推理规则通过增加决策蕴涵的前提或者减小决策蕴涵的结论来导出新的决策蕴涵[20]。</p><p>7.文献[20]还讨论了基于该推理规则的完备性和冗余性，并提出了一种基于最小生成子[25]的决策蕴涵规范基生成算法。另外，文献[26]提出了一种基于真前提的决策蕴涵规范基生成算法，实验结果表明该算法效率更高。</p><p>8.研究发现， 推理规则在语构特征上并非是完备的，因此文献[24]提出了合并推理规则，并证明合并推理规则和 推理规则(扩增推理规则)是完备的推理规则集。</p><p>9.在此基础上，文献[27]又给出一条新的推理规则——后件合并推理规则，它只对前件相同的决策蕴涵的后件进行合并。因此，后件合并推理规则在形式上更简洁。文献[27]也证明了扩增推理规则和后件合并推理规则是合理的、完备的并且是无冗余的。</p><p>10.此外，文献[28]给出一个决策蕴涵基(称为决策蕴涵规范基)。该规范基基于决策前提[29]，即由决策前提作为该决策蕴涵集的前提，由决策前提相对于决策子背景的闭包作为该决策蕴涵集的结论。文献[28]还证明了该决策蕴涵规范基是完备的、无冗余的、并且在所有完备的决策蕴涵集中所含的决策蕴涵最少，因而决策蕴涵规范基是最精简的和最优的。</p><p>11.研究结果表明，决策蕴涵规范基是蕴涵规范基[17]在决策背景下的对应概念，并且具有蕴涵规范基的所有优点。</p><p style="text-indent:2em">本文将从语构方面深入研究由这些子系统(决策蕴涵)能不能得到整个系统(所有的蕴涵)。如果蕴涵可以由决策蕴涵推出，那么关于蕴涵的研究就可以转化为决策蕴涵的研究。这样就可以由决策蕴涵来生成蕴涵，甚至可以由决策蕴涵规范基生成蕴涵规范基。</p><h2 id="2-FCA基本概念"><a href="#2-FCA基本概念" class="headerlink" title="2 FCA基本概念"></a>2 FCA基本概念</h2><h2 id="3-蕴涵"><a href="#3-蕴涵" class="headerlink" title="3 蕴涵"></a>3 蕴涵</h2><p>蕴涵的语义特征：</p><p><strong>定义6$^{[17]}$</strong> 设$K=(G, M, I)$是一个形式背景，$T\subseteq M$且$A\rightarrow B$是形式背景$K$下的一个蕴涵。如果属性子集$A\nsubseteq T$或$B\subseteq T$，则称$T$是蕴涵$A\rightarrow B$的一个模型，记为$T\models (A\rightarrow B)$。设$\Theta$为一个蕴涵集，如果对于每一个$(A\rightarrow B)\in \Theta$都有$T\models (A\rightarrow B)$，则称$T$是$\Theta$的一个模型，记为$T\models \Theta$。</p><p><strong>定义7$^{[17]}$</strong> 设$K=(G, M, I)$$是一个形式背景，$ $\Theta$为$K$的一个蕴涵集，若对任意的$T\subseteq M$，$T\models \Theta$蕴涵$T\models A\rightarrow B$，则称$A\rightarrow B$可以从$\Theta$语义导出。记为$\Theta \vdash A\rightarrow B$。若对任意的$(A\rightarrow B)\in \Theta$且$(\Theta \backslash (A\rightarrow B)) \vdash (A\rightarrow B)$，则称$A\rightarrow B$相对于$\Theta$是冗余的。</p><p><strong>定义8$^{[17]}$</strong> 设$K=(G, M, I)$$是一个形式背景，$ $\Theta$为$K$的一个蕴涵集，若对任意的$A\rightarrow B,\ \Theta \vdash A\rightarrow B$均成立，则称$\Theta$是$K$的一个完备集。</p><p>蕴涵的语构特征：</p><p>（1）$X\rightarrow X,\ X\subseteq M$；（自反性）</p><p>（2）若$X\rightarrow Y \in \Theta$，则$X\cup Z \rightarrow Y \in \Theta,\ X, Y, Z \subseteq M$；（增广性）</p><p>（3）若$X\rightarrow Y \in \Theta$且$Y\cup Z \rightarrow W \in \Theta$，则$X\cup Z \rightarrow W \in \Theta,\ X, Y, Z \subseteq M$。（伪传递性）</p><p>文献[30]已经证明这三条推理规则相对于蕴涵的语义是完备的，即从$\Theta$中导出的任意蕴涵都可以重复使用上述三条推理规则从$\Theta$中推出。</p><h2 id="4-决策蕴涵"><a href="#4-决策蕴涵" class="headerlink" title="4 决策蕴涵"></a>4 决策蕴涵</h2><h3 id="4-1-决策蕴涵的语义特征"><a href="#4-1-决策蕴涵的语义特征" class="headerlink" title="4.1 决策蕴涵的语义特征"></a>4.1 决策蕴涵的语义特征</h3><p><strong>定义9$^{[20]}$</strong> 设$K=(G, M, I)<script type="math/tex">是一个形式背景，如果令</script>M=C\cup D,\ I=I_C \cup I_D$，其中$C$是条件属性集，$D$是决策属性集，$C\cap D = \varnothing,\ I_C = G \times C$是条件关系，$I_D = G \times D$是决策关系，此时$K=(G, C, D, I)$为一个以$C$为条件，$D$为决策的决策背景。</p><p><strong>定义11$^{[20]}$</strong> 设$K=(G, C\cup D, I_C \cup I_D)$是一个决策背景，若$A\subseteq C$且$B\subseteq D$，则称$A\rightarrow B$是一个决策蕴涵。此时，$A$为该决策蕴涵的前提，$B$为该决策蕴涵的结论。</p><h3 id="4-1-决策蕴涵的语构特征"><a href="#4-1-决策蕴涵的语构特征" class="headerlink" title="4.1 决策蕴涵的语构特征"></a>4.1 决策蕴涵的语构特征</h3><p>决策蕴涵的语构方面主要研究推理规则的合理性、完备性和无冗余性。</p><p>文献[24]提出两条推理规则：</p><script type="math/tex; mode=display">\begin{align}扩增推理规则&：\frac{A\rightarrow B, A_1 \supseteq A, B_1 \subseteq B}{A_1 \rightarrow B_1} \newline合并推理规则&：\frac{A\rightarrow B, A_1 \rightarrow B_1}{A\cup A_1 \rightarrow B \cup B_1}\end{align}</script><p>并且证明了这两条规则是合理、完备和非冗余的。</p><p>文献[27]在此基础上提出了一条新的推理规则：</p><script type="math/tex; mode=display">\begin{align}后件合并推理规则&：\frac{A \rightarrow B_1, A\rightarrow B_2}{A\rightarrow B_1 \cup B_2}\end{align}</script><p>并且证明了这条规则是合理、完备和非冗余的。</p><h2 id="5-决策蕴涵表示蕴涵"><a href="#5-决策蕴涵表示蕴涵" class="headerlink" title="5 决策蕴涵表示蕴涵"></a>5 决策蕴涵表示蕴涵</h2><h3 id="5-1-蕴涵表示的逻辑简化"><a href="#5-1-蕴涵表示的逻辑简化" class="headerlink" title="5.1 蕴涵表示的逻辑简化"></a>5.1 蕴涵表示的逻辑简化</h3><p><strong>引理1</strong> 设$A\rightarrow B$是形式背景$K$下的一个蕴涵，则蕴涵$A\rightarrow B$成立，当且仅当$\forall b_i \in B, A\rightarrow b_i$均成立。</p><p><strong>证明</strong> </p><script type="math/tex; mode=display">\begin{align*}必要性：\newline因为& b_i \in B，所以有B=\{b_i\} \cup \{B-\{b_i\}\} \newline又因为& A\rightarrow B成立，所以A \rightarrow \{b_i\} \cup \{B-\{b_i\}\} \newline由自反&性推理规则可知，\{b_i\} \rightarrow \{b_i\}成立。 \newline再由增&广性推理规则可知，\{b_i\} \cup (B-\{b_i\}) \rightarrow \{b_i\}成立。\newline最后由&伪传递性推理规则及 \begin{cases} A \rightarrow \{b_i\} \cup \{B-\{b_i\}\} \newline\newline\{b_i\} \cup (B-\{b_i\}) \rightarrow \{b_i\} \end{cases}成立。\newline可知&A \rightarrow b_i成立。\newline\newline充分性: \newline我们&先证明 A\rightarrow \{b_1\} \cup \{b_2\}成立。\newline由自反&性推理规则可知 \{b_1\} \cup \{b_2\}\rightarrow \{b_1\} \cup \{b_2\}成立。\newline而因为& A \rightarrow b_i成立则由伪传递性推理规则及 \begin{cases}A \rightarrow b_1 \newline\newline\{b_1\} \cup \{b_2\}\rightarrow \{b_1\} \cup \{b_2\}\end{cases}成立。\newline可知& A \cup \{b_2\} \rightarrow \{b_1\} \cup \{b_2\}成立。\newline因为& A\cup \{b_2\} = A，即A \rightarrow \{b_1\} \cup \{b_2\} \newline接下来&，令B_1 \triangleq \{b_1\} \cup \{b_2\} \newline同理&，可证A \rightarrow B_1 \cup \{b_3\} \newline以此类&推，即可证明A \rightarrow B成立。\end{align*}</script><p><strong>引理2</strong> 设$A \rightarrow b$是形式背景$K$的一个蕴涵，$A_1 \subseteq A$。若$A_1 \rightarrow b$成立，则由增广性推理规则可知$A \rightarrow b$成立。</p><p>引理2 说明，只需求出$b$成立的最小前件$A_1$，就可得到蕴涵$A \rightarrow b$。显然此时$b \notin A$。 </p><h3 id="5-2-蕴涵表示的几种情况"><a href="#5-2-蕴涵表示的几种情况" class="headerlink" title="5.2 蕴涵表示的几种情况"></a>5.2 蕴涵表示的几种情况</h3><p>在形式背景$K=(G, M, I)$中，令$M=C \cup D，I = I_C \cup I_D$，即$K=(G, C \cup D, I_C \cup I_D)$，则蕴涵$A \rightarrow b$在$K$中存在六种情形：</p><p>（1）$A \subseteq C，b \in D$；</p><p>（2）$A \subseteq D，b \in D$；</p><p>（3）$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D$；</p><p>（4）$A \subseteq C，b \in C$；</p><p>（5）$A \subseteq D，b \in C$；</p><p>（6）$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C$；</p><p>由于情形（1）（5）为决策蕴涵，情形（2）（4）为子背景上的蕴涵，则需判断情形（3）（6）能否由（1）（2）（4）（5）推出，而互换$C、D$，则情形（3）（6）可互换。则仅考虑情形（3）即可。</p><p>首先，有</p><p><strong>引理 3</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D$。则如果$A \rightarrow b$可以由决策蕴涵表示，则$A \rightarrow b$必然是由伪传递性推理规则推导出的。</p><p><strong>证明</strong></p><script type="math/tex; mode=display">\begin{align*}对于自反性推理规则&，由于b \notin A，则A \rightarrow b不具有自反性。\newline\newline对于增广性推理规则&，若X \cup Z \rightarrow Y可应用于A \rightarrow b，则X \cup Z = A 和 Y = \{b\}\newline由于A为使蕴涵成立&的最小前件，则\forall N\subset A，N \rightarrow b均不成立。\newline显然为了使推理规则&有意义，则Z \neq \varnothing，此时X \subseteq A，\begin{cases} 当X \subset A，X \rightarrow b不成立。\newline当X=A时，X\rightarrow b即为 A \rightarrow b  \end{cases}\newline\newline对于伪传递性推理规则&，如果A\rightarrow b可由\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}推导出，\newline则有，X \cup Z = A\end{align*}</script><p><strong>定理 1</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p><strong>证明</strong></p><script type="math/tex; mode=display">\begin{align*}必要性& \newline&若X=\varnothing，则Z=A，而\varnothing \rightarrow Y成立，则Y=\varnothing，\newline&从而Y\cup Z \rightarrow b即为A \rightarrow b，因此X \neq \varnothing \newline\newline&证Y \neq \varnothing，由于A为使蕴涵成立的最小前件，则\forall N\subset A，N \rightarrow b均不成立。\newline&由X\cup Z = A，可知Z\subseteq A。\newline&当Z\subset A时，Z\rightarrow b不成立。为使Y\cup Z \rightarrow b成立，有Y \neq \varnothing。\newline&当Z=A时，Z \rightarrow b即为A \rightarrow b，因此Y \neq \varnothing。\newline\newline&现假设b\in Y，因为X\rightarrow Y成立。则由引理1可知X \rightarrow b成立。\newline&此时，由X \cup Z = A，可知X\subseteq A。\newline&由于X\cup Z为使蕴涵X \cup Z \rightarrow b成立的最小前件。\newline&因此，当X\subset A时，X \rightarrow b不成立。显然矛盾。\newline&当X=A时，X\rightarrow b 即为A \rightarrow b。则b\notin Y。\newline\newline&现假设Y \cup Z \subset A。由于A为使蕴涵成立的最小前件，\newline&因此，当Y\cup Z \subset A时，Y \cup Z \rightarrow b不成立，矛盾，则Y\cup Z \nsubseteq A。\newline\newline&现假设A\subseteq Y\cup Z，由增广性推理规则可知Y \cup Z \rightarrow b可由A \rightarrow b推出。则A\nsubseteq Y \cup Z。\newline\newline充分性& \newline&由于A为使蕴涵成立的最小前件，则\forall N\subset A，N \rightarrow b均不成立，\newline&则Y \cup Z \nsubseteq A保证了Y \cup Z \rightarrow b的可成立性。\newline&为证明A\rightarrow b可由伪传递性推理规则推出，\newline&只需证X \rightarrow Y和Y \cup Z \rightarrow b不等于且不依赖于A\rightarrow b。\newline&\begin{cases}X\rightarrow Y依赖于A \rightarrow b：指A\subseteq X且b\in Y。\newlineY\cup Z \rightarrow b依赖于A \rightarrow b：指A\subseteq Y\cup Z。\end{cases} \newline&由b\notin Y可知，X\rightarrow Y 不等于且不依赖于A \rightarrow b。\newline&由A\nsubseteq Y \cup Z可知，Y \cup Z \rightarrow b不等于且不依赖于A \rightarrow b。\end{align*}</script><p>直接表示：可由情形（1）（2）（4）（5）推导出情形（3）。</p><p>间接表示：情形（3）必须依赖于情形（3）/（6）才能推导出。</p><p>本文只考虑直接表示。</p><p>首先给出$A \rightarrow b$直接表示的判定条件。</p><p><strong>定理 2</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且满足以下条件之一：$A_2\subseteq C，A_3\subseteq D$</p><p>（1）$X=A_2，Z=A_3$，且$\exists\ Y \subseteq D$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>（2）$X=A_3，Z=A_2$，且$\exists\ Y \subseteq $C使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>证明</p><script type="math/tex; mode=display">\begin{align*}必要性& \newline&根据题设，A\rightarrow b属于情形（3）中的蕴涵。若A\rightarrow b可由\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}直接表示。\newline&则X\rightarrow b和Y\cup Z \rightarrow b均不属于情形（3）/（6）。\newline&因此，X不满足X\cap C \neq \varnothing，X\cap D \neq \varnothing；\newline&Z不满足Z\cap C \neq \varnothing，Z\cap D \neq \varnothing。\newline&即为（1）（2）（4）（5）的情形，即X=A_2，Z=A_3或X=A_3，Z=A_2。\newline&当X=A_2，Z=A_3时，X\rightarrow Y必不为（3）/（6）。则Y\cup Z \rightarrow b均不属于情形（3）/（6）时，\newline&有Y\cap C = \varnothing，即Y\subseteq D；\newline&当X=A_3，Z=A_2时，X\rightarrow Y必不为（3）/（6）。则Y\cup Z \rightarrow b均不属于情形（3）/（6）时，\newline&有Y\cap D = \varnothing，即Y\subseteq C；\newline\newline充分性& \newline&由假设和定理1，可知A\rightarrow b可由伪传递性推理规则表示。\newline&现假设条件（1）（2）成立时，X\rightarrow Y和Y\cup Z\rightarrow b必不属于或依赖于情形（3）/（6）。\newline&当X=A_2，Z=A_3，且\exists\ Y \subseteq D时，显然X\rightarrow Y属于情形（1），Y\cup Z \rightarrow b属于情形（2）。\newline&当X=A_3，Z=A_2，且\exists\ Y \subseteq C时，显然X\rightarrow Y属于情形（5），Y\cup Z \rightarrow b属于情形（4）。\newline\end{align*}</script><p>下面给出一个例子说明情形（3）不是冗余的，即存在情形（3）所示的蕴涵不能被决策蕴涵表示。</p><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730115313378.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730115313378.png" srcset="/img/loading.gif" alt></p><p>因为$\{a,d\}^{“} = \{x_2\}^{‘} = \{a,b,d\}$，显然$\{a\} \cup \{d\} \rightarrow \{b\}$是该决策背景的蕴涵。</p><p>令$A_2 \triangleq \{a\}，A_3 \triangleq \{d\}$，则$\{a\} \cup \{d\} \rightarrow \{b\}$属于情形（3）。接下来，利用伪传递性推理规则来推导该蕴涵。</p><script type="math/tex; mode=display">\begin{align*}由定理1，&我们首先假设X \neq \varnothing，Y \neq \varnothing。\newline&首先考虑Z = \varnothing的情况。此时，需要找到Y使 \frac{\{a\} \cup \{d\} \rightarrow Y,Y\rightarrow b}{\{a\} \cup \{d\} \rightarrow \{b\} }成立。\newline&而Y所有可能的取值为\{a,b,d,ab,ad,bd,abd\}。\newline&当Y取\{a,d\}中任意一个时，Y\rightarrow b均不成立。\newline&当Y取\{b,ab,ad,bd,abd\}中任意一个时，用到了\{a\} \cup \{d\} \rightarrow \{b\}自身。\newline&因此，当Z=\varnothing时，不存在Y使\{a\} \cup \{d\} \rightarrow \{b\}可由其他蕴涵推出，即\{a\} \cup \{d\} \rightarrow \{b\}不冗余。\newline\newline&接下来，考虑Z \neq \varnothing的情况，需要找到Y使 \frac{\{a\} \rightarrow Y,Y \cup \{d\} \rightarrow b}{\{a\} \cup \{d\} \rightarrow \{b\} }或 \frac{ \{d\} \rightarrow Y,Y \cup \{a\}  \rightarrow b}{\{a\} \cup \{d\} \rightarrow \{b\} }成立。\newline&而Y所有可能的取值为\{a,b,d,ab,ad,bd,abd\}。\newline&当Y取\{a,b,d,ab,ad,bd,abd\}。\newline&当Y取\{a\}时，用到了\{a\} \cup \{d\} \rightarrow \{b\}本身。\newline&当Y取\{b,d,ab,ad,bd,abd\}中的任意一个时，\{a\} \rightarrow Y不成立。\newline&类似的，当Y取\{d\}时，用到了\{a\} \cup \{d\} \rightarrow \{b\}本身。\newline&当Y取\{a,b,ab,ad,bd,abd\}中的任意一个时，\{d\} \rightarrow Y不成立。\newline&因此，当Z \neq \varnothing时，不存在Y使\{a\} \cup \{d\} \rightarrow \{b\}可由其他蕴涵推出，即\{a\} \cup \{d\} \rightarrow \{b\}不冗余。\end{align*}</script><h3 id="5-3-蕴涵表示的具体方法"><a href="#5-3-蕴涵表示的具体方法" class="headerlink" title="5.3 蕴涵表示的具体方法"></a>5.3 蕴涵表示的具体方法</h3><p>例1 说明某些蕴涵确实不可直接归结为决策蕴涵和子背景的蕴涵。</p><p>因此，我们需找到蕴涵不可以被直接表示时 应满足的条件，并生成相应的蕴涵。</p><p>为此，我们首先讨论决策属性只有一个属性的情形下， $Y$存在或不存在的情况。</p><p><strong>引理 5</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D$。若决策属性$D$中只有一个属性，则$A \rightarrow b$是冗余的。</p><p>证明</p><script type="math/tex; mode=display">\begin{align*}根据题设&，A \rightarrow b属于情形（3）的蕴涵。\newline&若决策属性D只有一个属性，由于b\in D，所以A_3 = \{b\}，\newline&则由自反性推理规则\{b\} \rightarrow \{b\}以及增广性推理规则b\in A，\newline&则可知，A \rightarrow b是冗余的。\end{align*}</script><p>接下来，分析情形（6）中的蕴涵是否可被直接表示。</p><p>容易证明定理1、2对于情形（6）也是成立的。可得</p><p><strong>推论 1</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p><strong>推论 2</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且满足以下条件之一：$A_2\subseteq C，A_3\subseteq D$</p><p>（1）$X=A_2，Z=A_3$，且$\exists\ Y \subseteq D$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>（2）$X=A_3，Z=A_2$，且$\exists\ Y \subseteq C$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>下面的例子表明，当决策属性$D$中只有一个属性时，对于推论2中的两种情况，情形（6）中的蕴涵不总是可以被直接表示。</p><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131326376.png" srcset="/img/loading.gif" class><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131536900.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131326376.png" srcset="/img/loading.gif" alt><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131536900.png" srcset="/img/loading.gif" alt></p><p>首先考虑推论2中的（1）。此时，需要找到$Y\subseteq D$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。则$Y=\{d\}$，由$\{d\} \rightarrow b$不成立可知$\{a\} \cup \{d\} \rightarrow \{b\}$不可以被直接表示。</p><p>接下来，考虑推论2中的（2）。此时，需要找到$Y\subseteq C$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>$Y$所有可能的取值为$\{a,b,ab\}$。无论$Y$取何值，$\{d\} \rightarrow Y$都不成立，因此$\{a\} \cup \{d\} \rightarrow \{b\}$不可以被直接表示。</p><p>由例2可知，当决策属性$D$中只有一个属性时，对于推论 2 所示的两种情况，情形（6）中的蕴涵$A\rightarrow b$不可以被直接表示的充要条件及发现算法。</p><p><strong>引理 5</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C$。若决策属性$D$中只有一个属性，则$X=A_2，Z=A_3$，且$\exists\ Y \subseteq D$时，$A \rightarrow b$不可以被直接表示。</p><p>证明</p><script type="math/tex; mode=display">\begin{align*}&当决策属性D中只有一个属性d时，由Y \neq \varnothing和Y\subseteq D可知Y=\{d\}。\newline&由Z=A_3 \subseteq D可知Z=\{d\}，由A\cap D \neq \varnothing和A\cap C \neq \varnothing可知Y\cup Z \subsetneq A。\newline&再由于A为使蕴涵成立的最小前件，可知Y \cup Z \rightarrow b 不成立，从而，A \rightarrow b不可以被直接表示。\end{align*}</script><p>接下来，给出推论2中（2）情形（6）中的蕴涵$A \rightarrow b$也不可以被直接表示的判定条件。</p><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730140328255.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730140328255.png" srcset="/img/loading.gif" alt></p><p>当 中只有一个属性 时，由引理 5 可知，情形（3）中的蕴涵都是冗余的，因此不必生成;对于情形（6），定理 3 事实上给出了不可被直接表示的蕴涵的生成方法。</p><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/images/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730140851645.png" srcset="/img/loading.gif" alt></p><p>显然，上述算法的复杂度较高，因此难以应用于具体的数据集中。</p><h2 id="6-结论与展望"><a href="#6-结论与展望" class="headerlink" title="6 结论与展望"></a>6 结论与展望</h2><p>本文使用蕴涵推理规则来研究蕴涵是否可由决策蕴涵表示。</p><ol><li><p>首先给出蕴涵可以被直接表示时应满足的条件。</p></li><li><p>找出不可以直接归结为决策蕴涵的的蕴涵应满足的充要条件。</p></li><li><p>给出了不可以直接归结为决策蕴涵的蕴涵的生成方法。</p></li></ol><p>存在的问题：</p><ol><li>由于蕴涵推理的复杂性，未对蕴涵的间接表示进行深入的研究。</li><li>不能被直接表示蕴涵的生成算法，复杂度较大，无法用于实际的数据集。</li><li>决策蕴涵规范基与蕴涵规范基之间是否存在着联系。</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>决策蕴涵</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第三章 概率论</title>
    <link href="/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    <url>/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="第三章-概率论"><a href="#第三章-概率论" class="headerlink" title="第三章 概率论"></a>第三章 概率论</h1><p class="note note-info">深度学习、花书、概率论</p><a id="more"></a><h2 id="3-1-概率与随机变量"><a href="#3-1-概率与随机变量" class="headerlink" title="3.1 概率与随机变量"></a>3.1 概率与随机变量</h2><ul><li>频率学派概率（Frequentist Probability）：认为概率和事件发生的频率相关。</li><li>贝叶斯学派概率（Bayesian Probability）：认为概率是对某件事发生的确定程度，可以理解成是确信的程度。</li><li>随机变量（Random Variable）：一个可能随机取不同值的变量。例如：抛掷一枚硬币，出现正面或者反面的结果。</li></ul><h2 id="3-2-概率分布"><a href="#3-2-概率分布" class="headerlink" title="3.2 概率分布"></a>3.2 概率分布</h2><p><span style="color:red">概率质量函数（PMF，Probabily Mass Function）</span>：对于离散型变量，我们先定义⼀个随机变量，然后⽤ ~ 符号来说明它遵循的分布：$x \sim P(x)$，函数$P$是随机变量$x$的$PMF$。</p><p>例如，考虑一个离散型随机变量$x$有$k$个不同的值，我们可以假设$x$是均匀分布的，则它的$PMF$可设为：</p><script type="math/tex; mode=display">P(x=x_i) = \frac1 k</script><p>对所有的$i$都成立。</p><p><span style="color:red">概率密度函数（PDF，Probabily Density Function）</span>：对于连续型变量，如果一个函数$p$是$PDF$，则</p><script type="math/tex; mode=display">\begin{align}分布满足非负性条件&：\forall x\in \mathsf{x}， p(x) \ge 0 \newline分布满足归一化条件&：\int_{-\infty}^{\infty} p(x)dx = 1\end{align}</script><p>例如在（a, b）上的均匀分布：</p><script type="math/tex; mode=display">U(x; a, b) = \frac {1_{ab(x)} }{b-a}</script><p>其中$1_{ab(x)}$表示在（a, b）内为1，否则为0。</p><p><span style="color:red">累积分布函数（CDF，Cummulative Distribution Function）</span>：表示对小于$x$的概率的积分：</p><script type="math/tex; mode=display">CDF(x) = \int_{-\infty}^{x} p(t) dt</script><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> uniform%matplotlib inline<span class="hljs-comment"># 生成样本</span>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)r = uniform.rvs(loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>, size=<span class="hljs-number">1000</span>)ax.hist(r, density=<span class="hljs-literal">True</span>, histtype=<span class="hljs-string">'stepfilled'</span>, alpha=<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 均匀分布 pdf</span>x = np.linspace(uniform.ppf(<span class="hljs-number">0.01</span>), uniform.ppf(<span class="hljs-number">0.99</span>), <span class="hljs-number">100</span>)ax.plot(x, uniform.pdf(x), <span class="hljs-string">'r-'</span>, lw=<span class="hljs-number">5</span>, alpha=<span class="hljs-number">0.8</span>, label=<span class="hljs-string">'uniform pdf'</span>)</code></pre><img src="/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200731122057031.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200731122057031.png" srcset="/img/loading.gif" alt></p><h2 id="3-3-条件概率与条件独立"><a href="#3-3-条件概率与条件独立" class="headerlink" title="3.3 条件概率与条件独立"></a>3.3 条件概率与条件独立</h2><p><span style="color:red">边缘概率（Marginal Probability）</span>：定义在子集上的概率分布被称为边缘概率分布。</p><script type="math/tex; mode=display">\forall x\in \mathsf{x}，P(\mathsf{x} = x) = \sum\limits_y P(\mathsf{x}=x, \mathsf{y}=y)</script><p><span style="color:red">条件概率（Conditional Probability）</span>：某个事件在给定其他事件发生时的概率。</p><script type="math/tex; mode=display">P(\mathsf{y}=y | \mathsf{x}=x) = \frac{P(\mathsf{y}=y , \mathsf{x}=x)}{P(\mathsf{x}=x)}</script><p><span style="color:red">条件概率的链式法则（Chan Rule of Conditional Probability）</span>：任何多维随机变量的联合概率分布，都可以分解成只有⼀个变量的条件概率相乘的形式。</p><script type="math/tex; mode=display">P(x_1, ..., x_n) = P(x_1) \Pi_{i=2}^{n} (x_i|x_1, ..., x_{i-1})</script><p><span style="color:red">独立性（Independence）</span>：两个随机变量$x$和$y$的概率分布能够表示成两个因子的乘积形式。</p><script type="math/tex; mode=display">\forall x\in \mathsf{x}, y\in \mathsf{y}, p(\mathsf{x}=x, \mathsf{y}=y) = p(\mathsf{x}=x)p(\mathsf{y}=y)</script><p><span style="color:red">条件独立性（Conditional Independence）</span>：两个随机变量$x$和$y$的条件概率对于$z$的每一个值都可以写成乘积的形式。</p><script type="math/tex; mode=display">\forall x\in \mathsf{x}, y\in \mathsf{y}, z\in \mathsf{z}, p(\mathsf{x}=x, \mathsf{y}=y | \mathsf{z}=z) = p(\mathsf{x}=x | \mathsf{z}=z)p(\mathsf{y}=y | \mathsf{z}=z)</script><h2 id="3-4-随机变量的度量"><a href="#3-4-随机变量的度量" class="headerlink" title="3.4 随机变量的度量"></a>3.4 随机变量的度量</h2><p><span style="color:red">期望（Expectation）</span>：函数$f$关于概率分布$P(x)$或$p(x)$的期望表示由概率分布产生$x$，再计算$f$作用到$x$后$f(x)$的平均值。</p><p>对于离散型随机变量，可以通过求和得到：</p><script type="math/tex; mode=display">\mathbb{E}_{X \sim P}[f(x)] = \sum\limits_x P(x)f(x)</script><p>对于连续型随机变量，可以通过求积分得到：</p><script type="math/tex; mode=display">\mathbb{E}_{X \sim P}[f(x)] = \int\limits P(x)f(x) dx</script><p>另外，期望是线性的：</p><script type="math/tex; mode=display">\mathbb{E}_{X}[\alpha f(x) + \beta g(x)] = \alpha \mathbb{E}_{X}[f(x)] + \beta  \mathbb{E}_{X}[g(x)]</script><p><span style="color:red">方差（Variance）</span>：依据随机变量$x$的概率分布，衡量随机变量$x$的函数值会呈现多大的差异。描述采样得到的函数值在期望上的波动程度：</p><script type="math/tex; mode=display">Var(f(x)) = \mathsf{E}[(f(x) - \mathbb{E}[f(x)])^2]</script><p>将方差开平方即为<span style="color:red">标准差（Standard Deviation）</span>。</p><p><span style="color:red">协方差（Covariance）</span>：衡量两组值之间的线性相关程度。</p><script type="math/tex; mode=display">Cov(f(x), g(y)) = \mathbb{E}[\left(f(x) - \mathbb{E}[f(x)]\right) \left(g(y) - \mathbb{E}[g(y)] \right)]</script><p>注意：独立比0协方差要求更强，因为独立还排除了非线性的相关。</p><img src="/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200801143413314.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200801143413314.png" srcset="/img/loading.gif" alt></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
      <tag>概率论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2016-ICCS-Parallel Attribute Exploration</title>
    <link href="/2020/07/14/2016-ICCS-Parallel%20Attribute%20Exploration/"/>
    <url>/2020/07/14/2016-ICCS-Parallel%20Attribute%20Exploration/</url>
    
    <content type="html"><![CDATA[<h1 id="Parallel-Attribute-Exploration"><a href="#Parallel-Attribute-Exploration" class="headerlink" title="Parallel Attribute Exploration"></a>Parallel Attribute Exploration</h1><p class="note note-info">属性探索、并行算法</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel。</p><p>​            2. 会议：International Conference on Conceptual Structures。</p><p>​            3. 时间：2016。</p><p>吴恩达的三遍读文献法</p><h2 id="1-第一遍：标题、摘要、关键词（5-minutes）"><a href="#1-第一遍：标题、摘要、关键词（5-minutes）" class="headerlink" title="1 第一遍：标题、摘要、关键词（5 minutes）"></a>1 第一遍：标题、摘要、关键词（5 minutes）</h2><p>1.对无法获得的形式背景的属性探索增加了专家交互。</p><p>2.提出并分析一种并行属性探索的算法。</p><h2 id="2-第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30-minutes）"><a href="#2-第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30-minutes）" class="headerlink" title="2 第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30 minutes）"></a>2 第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30 minutes）</h2><h3 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h3><p>1.文献[5]中，Ganter 引入 NextCloure 来计算主基并证明了算法的正确性。</p><p>2.属性探索是 NextCloure 算法的一种扩展。</p><p>3.在文献[10, 11]中，作者引入 NextCloures 算法以非线性顺序实现了算法的并行。</p><p>4.文献[9]中描述了以 Java 8 来实现 NextCloures 算法。</p><p>5.本文向 NextCloures 算法中引入专家交互来处理不完备的形式背景，此外还可利用子形式背景来减少向专家询问的问题数量。</p><h3 id="2-2-Discussion"><a href="#2-2-Discussion" class="headerlink" title="2.2 Discussion"></a>2.2 Discussion</h3><p>1.文献[4-7, 13]对经典属性探索算法引入了多个专家，但对时间性能没有任何提升。</p><p>2.比较本文 Algorithm 1 与经典属性探索算法的不同，经典属性探索算法以字典序来枚举，而本文以蕴涵前提的基数递增来进行枚举。</p><ol><li>一方面，这意味着多个蕴涵式能以并行的方式进行处理。</li><li>另一方面，以前提基数递增向专家询问问题的难度将会增加。</li></ol><p>3.对于多个专家的考虑，</p><ol><li>随机选择一个专家并提出问题。</li><li>向所有专家提出问题，返回第一个答案。</li><li>向所有专家提出问题，所有专家接受才接受。</li><li>向所有专家提出问题，至少一个专家接受才接受。</li></ol><h3 id="2-3-Conclusion"><a href="#2-3-Conclusion" class="headerlink" title="2.3 Conclusion"></a>2.3 Conclusion</h3><p>1.本文考虑了并行属性探索来处理不完备形式背景的问题。</p><p>2.该 Parallel Attribute Exploration 算法是 文献[10, 11] NextCloures 算法的一个扩展。</p><p>3.下一步，作者将考虑将该算法扩展到处理背景知识的情形。</p><p>4.该算法可以扩展到分级完全格描述的数据集的情形。</p><h2 id="3-第三遍：整篇论文，跳过看不懂的公式、技术术语。（2-hours）"><a href="#3-第三遍：整篇论文，跳过看不懂的公式、技术术语。（2-hours）" class="headerlink" title="3 第三遍：整篇论文，跳过看不懂的公式、技术术语。（2 hours）"></a>3 第三遍：整篇论文，跳过看不懂的公式、技术术语。（2 hours）</h2><h3 id="3-1-Abstract"><a href="#3-1-Abstract" class="headerlink" title="3.1 Abstract"></a>3.1 Abstract</h3><p>研究背景：形式背景的主基是最小的完整非冗余蕴涵基。最近一篇论文为主基的并行计算提供了一种新的算法。</p><p>本文提出并分析了一种支持并行属性探索的算法，并引入专家交互以便探索不可访问的形式背景的蕴涵基。</p><h3 id="3-2-Introduction"><a href="#3-2-Introduction" class="headerlink" title="3.2 Introduction"></a>3.2 Introduction</h3><p>1.蕴涵式是一种易于理解的逻辑知识表示方式。</p><p>2.如果形式背景是完备的，则能够计算出该形式背景的蕴涵基是完备非冗余的$^{[8]}$。</p><p>3.Ganter$^{[5]}$ 引入了计算主基的算法 NextCloures，并证明了该算法的正确性。</p><p>4.在数据库领域，Maier$^{[12]}$研究了依赖关系的推导，但没有提供明确的依赖关系依赖基的构造。</p><p>5.对于不完备的形式背景的情况，Ganter$^{[4-6]}$和 Stumme$^{[13]}$已经开发了一种称为属性探索的技术。</p><ul><li>该算法是 NextCloure 算法的扩展。</li><li>计算的顺序为线性序即字典序，无法并行。</li><li>在文献$^{[10, 11]}$中，作者引入 NextCloure 算法以非线性序即蕴涵前提基数递增的顺序并行地计算主基。</li></ul><p>6.在文献$^{[9]}$中可以找到编程语言 Java 8 中的 NextClosure 算法实现。</p><p>本文对 NextClosure 进行了扩展，增加了专家交互的可能性。更具体地说，我们假设有一个描述感兴趣的领域的正式上下文，但它是不可访问的，并且有一位专家(或一组专家)可以正确地决定暗示是否在此上下文中成立，如果她反驳，则还提供了一个反例。此外，可能存在观察到的全域上下文的子上下文，其用于减少向专家提出的问题的数量。使用属性探索技术，可以构造域上下文的最小隐含基础。将在以下各节中介绍的算法 Parallel Attribute Exploration 实现了此技术，并进一步允许并行执行。</p><h3 id="3-3-Formal-Concept-Analysis"><a href="#3-3-Formal-Concept-Analysis" class="headerlink" title="3.3 Formal Concept Analysis"></a>3.3 Formal Concept Analysis</h3><h3 id="3-4-Experts"><a href="#3-4-Experts" class="headerlink" title="3.4 Experts"></a>3.4 Experts</h3><p>专家是在某个感兴趣的领域正确回答问题的先知。这些问题是以与蕴涵式的形式表达的，专家可以接受也可以拒绝。如果专家接受一个蕴涵式，那么它必须适用于感兴趣的领域中的所有对象，否则她必须返回一个反例，即作为拒绝的对象。</p><p>基本概念：</p><ul><li>$M$：属性集。</li><li>M上的专家是一个部分映射$\chi: Imp(M) \rightarrow _p {\wp}(M)$。</li><li>$Imp(\chi)$：专家接受的蕴涵式集合。</li><li>$Cex(\chi) := \{ C|\exist X, Y\subseteq M: \chi(X\rightarrow Y) = C \}$：专家$\chi$拒绝的反例构成的集合。</li></ul><p>1.如果$\chi (X \rightarrow Y) = C \Rightarrow X\subseteq C\ and\ Y \subsetneq C$，则将$C$作为$X\rightarrow Y$的一个反例。</p><p>2.如果$\chi (X \rightarrow Y)$未定义，则专家$\chi$给出的反例必须与接受的蕴涵式不冲突即$\chi (U \rightarrow V) = C \Rightarrow X\subsetneq C\ or\ Y \subseteq C$。</p><p>3.如果专家$\chi$接受蕴涵式$X\rightarrow Y$，则记为$\chi \models X\rightarrow Y$。</p><h3 id="3-5-Parallel-Attribute-Exploration"><a href="#3-5-Parallel-Attribute-Exploration" class="headerlink" title="3.5 Parallel Attribute Exploration"></a>3.5 Parallel Attribute Exploration</h3><h3 id="3-6-Discussion"><a href="#3-6-Discussion" class="headerlink" title="3.6 Discussion"></a>3.6 Discussion</h3><h3 id="3-7-Conclusion"><a href="#3-7-Conclusion" class="headerlink" title="3.7 Conclusion"></a>3.7 Conclusion</h3><p>本文考虑并行属性探索的问题，提出的算法 Parallel Attribute Exploration 是算法 NextClosure 的扩展，并提供了一个原型实现$^{[9]}$，计划将其用于协作知识获取平台。</p><p>下一步，该算法将进一步扩展已处理背景知识，正如经典属性探索所做的那样$^{[4, 13]}$。在此基础上，将该算法推广到数据集在分级完备格上用闭包算子描述的情况。</p><p>词汇积累</p><ul><li>straight-forward：直接的。</li><li>quotient：商。</li><li>oracle：先知。</li></ul><h2 id="4-提问题，检验对文章关键信息的了解。"><a href="#4-提问题，检验对文章关键信息的了解。" class="headerlink" title="4 提问题，检验对文章关键信息的了解。"></a>4 提问题，检验对文章关键信息的了解。</h2><blockquote><p>1.这篇论文作者的目标是什么？或者实现了什么？</p></blockquote><p>作者通过引入专家交互和非线性序即以蕴涵前提基数递增的顺序对 NextClosure 算法的扩展实现了在不可访问的形式背景下属性探索的并行化。</p><blockquote><p>2.文中新方法/技术的关键要素是什么？</p></blockquote><p>专家交互:</p><p>蕴涵前提基数递增的顺序:</p><blockquote><p>3.论文中哪些内容对我有用？</p><p>4.我还想关注哪些文献？</p></blockquote><p>[4]: Bernhard Ganter. “Attribute Exploration with Background Knowledge”. In: Theor.Comput. Sci. 217.2 (1999), pp. 215–233.</p><p>[6]: Bernhard Ganter. “Two Basic Algorithms in Concept Analysis”. In: Formal Concept Analysis, 8th International Conference, ICFCA 2010, Agadir, Morocco, March 15-18, Proceedings. 2010, pp. 312–340.</p><p>[9]: Francesco Kriegel. Concept Explorer FX. Software for Formal Concept Analysis. 2010–2016. url: <a href="https://github.com/francesco-kriegel/conexp-fx" target="_blank" rel="noopener">https://github.com/francesco-kriegel/conexp-fx</a>.</p><p>[11]: Francesco Kriegel and Daniel Borchmann. “NextClosures: Parallel Computation of the Canonical Base”. In: Proceedings of the 12th International Conference on Concept Lattices and Their Applications (CLA 2015), Clermont-F errand, France, October 13-16, 2015, pp. 181–192.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>并行算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第二章 线性代数</title>
    <link href="/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    <url>/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="第二章-线性代数"><a href="#第二章-线性代数" class="headerlink" title="第二章 线性代数"></a>第二章 线性代数</h1><p class="note note-info">深度学习、花书、线性代数</p><a id="more"></a><h2 id="2-1-标量、向量、矩阵和张量"><a href="#2-1-标量、向量、矩阵和张量" class="headerlink" title="2.1 标量、向量、矩阵和张量"></a>2.1 标量、向量、矩阵和张量</h2><p>基本概念</p><ul><li><span style="color:red">标量（scalar）</span>：一个标量就是一个单独的数。</li></ul><p style="text-indent:2em">它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用<i>斜体</i>表示标量。标量通常被赋予<i>小写的变量名称</i>。当我们介绍标量时，会明确它们是哪种类型的数。比如，在定义实数标量时，我们可能会说 ‘‘令 <i>s ∈ R</i> 表示一条线的斜率’’；在定义自然数标量时，我们可能会说 ‘‘令 <i>n ∈ N</i> 表示元素的数目’’。</p><ul><li><span style="color:red">向量（vector）</span>：一个向量就是有序排列的一列数。通过次序中的索引，我们可以确定每个单独的数。</li></ul><p style="text-indent:2em">通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量<b>粗体的小写变量名称</b>，比如 <b>x</b>。向量中的元素可以通过带<i>脚标的斜体</i>表示。向量 <b>x</b> 的第一个元素是 <b>x</b><sub><i>1</i></sub>，第二个元素是 <b>x</b><sub><i>2</i></sub>，等等。</p><p>$\mathbb{R}^n$：如果每个元素都属于 $\mathbb{R}$，并且该向量有 $n$ 个元素，那么该向量属于实数集 $\mathbb{R}$ 的 $n$ 次笛卡尔乘积构成的集合，记为 $\mathbb{R}^n$。</p><ul><li><p><span style="color:red">—-</span>：集合补集的索引。比如 $x_{-1}$ 表示 $x$ 中除 $x_1$ 外的所有元素。</p></li><li><p><span style="color:red">矩阵（matrix）</span>：矩阵是一个二维数组，其中的每一个元素被两个索引（而非一个）所确定。</p><ul><li>$A_{i,:}$：$A$ 的第 $i$ 行（row）。</li><li>$A_{:,i}$： $A$的第 $i$ 列（column）。</li><li>$f(A)_{i,j}$：表示函数$f$ 作用在$A$上输出的矩阵的第 $i$行第 $j$ 列元素。</li></ul></li><li><p><span style="color:red">张量（tensor）</span>：一个数组中的元素分布在若干维坐标（坐标超过两维）的规则网格中，我们称之为张量。我们使用字体 <b>A</b> 来表示张量 “A’’。张量 <b>A</b> 中坐标为 (i, j, k) 的元素记作 $\bf{A_{i,j,k} }$。</p></li><li><p><span style="color:red">主对角线（main diagonal）</span>：从左上角到右下角的对角线。</p></li><li><p><span style="color:red">广播（broadcasting）</span>：向量$b$ 和矩阵$A$ 的每一行相加。这个简写方法使我们无需在加法操作前定义一个将向量 $b$ 复制到每一行而生成的矩阵。这种隐式地复制向量 $b$ 到很多位置的方式称为广播。即$C=A+b\Leftrightarrow C_{i,j}=A_{i,j}+b_j$。</p></li></ul><h2 id="2-2-矩阵与向量相乘"><a href="#2-2-矩阵与向量相乘" class="headerlink" title="2.2 矩阵与向量相乘"></a>2.2 矩阵与向量相乘</h2><p>基本概念</p><ul><li><span style="color:red">元素对应乘积（element-wise product）或者 <strong>Hadamard</strong> 乘积（Hadamard product）</span>：两个矩阵中对应元素的乘积，记为$A\odot B$。</li><li><span style="color:red">点积（dot product）</span>：两个相同维数的向量 <strong>x</strong> 和 <strong>y</strong> 的 点积（dot product）可看作是矩阵乘积$x^Ty$。注：$x^Ty=y^Tx$。</li></ul><h2 id="2-3-线性相关与生成子空间"><a href="#2-3-线性相关与生成子空间" class="headerlink" title="2.3 线性相关与生成子空间"></a>2.3 线性相关与生成子空间</h2><p>基本概念</p><ul><li><span style="color:red">线性组合（linear combination）</span>：<br><p style="text-indent:2em">为了分析方程有多少个解，我们可以将 A 的列向量看作从 原点（origin）（元素都是零的向量）出发的不同方向，确定有多少种方法可以到达向量 b。在这个观点下，向量 x 中的每个元素表示我们应该沿着这些方向走多远，即 xi 表示我们需要沿着第 i 个向量的方向走多远：</p><script type="math/tex; mode=display">  Ax=\sum\limits_i x_iA_{:,i}</script><p style="text-indent:2em">一般而言，这种操作被称为 线性组合（linear combination）。形式上，一组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即：</p><script type="math/tex; mode=display">  \sum\limits_i c_iv^{(i)}</script></li><li><span style="color:red">列空间（column space）或值域（range）</span>：确定 <strong>Ax</strong> = <strong>b</strong> 是否有解相当于确定向量 <strong>b</strong> 是否在 <strong>A</strong> 列向量的生成子空间中。这个特殊的生成子空间被称为 <strong>A</strong> 的 列空间（column space）或者 <strong>A</strong> 的 值域（range）。</li><li><span style="color:red">线性无关（linearly independent）</span>：如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为 线性无关（linearly independent）。</li><li><span style="color:red">奇异的（singular）</span>：一个列向量线性相关的方阵。</li></ul><h2 id="2-4-范数"><a href="#2-4-范数" class="headerlink" title="2.4 范数"></a>2.4 范数</h2><p style="text-indent:2em"><span style="color:red">范数（singular）</span>是将向量映射到非负值的函数。直观上来说，向量 x 的范数衡量从原点到点 x 的距离。更严格地说，范数是满足下列性质的任意函数：</p><ul><li>$f(x)=0\Rightarrow x=0$</li><li>$f(x+y)\le f(x)+f(y)$  （三角不等式（triangle inequality））</li><li>$\forall \alpha\in \mathbb{R},\ f(\alpha x)=|\alpha|f(x)$</li></ul><p>$L^p$范数：</p><script type="math/tex; mode=display">\parallel x\parallel_p=\left(\sum\limits_i |x_i|^p \right)^{\frac1p}\qquad 其中p\in\mathbb{R},\ p\ge1。</script><p style="text-indent:2em"><span style="color:red">L<sup>2</sup> 范数（singular）</span>被称为欧几里得范数（Euclidean norm）。它表示从原点出发到向量 x 确定的点的欧几里得距离。</p><script type="math/tex; mode=display">\parallel x\parallel_2=\left(\sum\limits_i |x_i|^2 \right)^{\frac12}</script><p style="text-indent:2em">两个向量的 点积（dot product）可以用范数来表示。具体地：</p><script type="math/tex; mode=display">x^Ty=\parallel x\parallel_2\parallel y\parallel_2 \cos\theta\qquad 其中\theta表示x和y之间的夹角。</script><p><span style="color:red">平方L<sup>2</sup> 范数（singular）</span>：</p><script type="math/tex; mode=display">\parallel x\parallel_2=\sum\limits_i |x_i|^2=x^Tx</script><p style="text-indent:2em">平方 L<sup>2</sup> 范数对x 中每个元素的导数只取决于对应的元素，而 L<sup>2</sup> 范数对每个元素的导数却和整个向量相关。但是在很多情况下，平方 L<sup>2</sup> 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。</p><p><span style="color:red">L<sup>1</sup> 范数（singular）</span>：</p><script type="math/tex; mode=display">\parallel x\parallel_1=\sum\limits_i |x_i|</script><p style="text-indent:2em">当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 L<sup>1</sup> 范数。每当x 中某个元素从 0 增加 ϵ，对应的 L<sup>1</sup> 范数也会增加 ϵ。</p><p><span style="color:red">L<sup>&infin;</sup> 范数（singular）</span>：表示向量中具有最大幅值的元素的绝对值。</p><script type="math/tex; mode=display">\parallel x\parallel_{\infin}=\max\limits_i |x_i|</script><p><span style="color:red"><strong>Frobenius</strong> 范数（Frobenius norm）</span>：衡量矩阵的大小。</p><script type="math/tex; mode=display">\parallel A\parallel_F=\sqrt{\sum\limits_{i,j} A_{i,j}^2}</script><h2 id="2-5-矩阵分解"><a href="#2-5-矩阵分解" class="headerlink" title="2.5 矩阵分解"></a>2.5 矩阵分解</h2><p><span style="color:red">特征分解（eigendecomposition）</span>：将矩阵分解成一组特征向量和特征值。</p><script type="math/tex; mode=display">Av=\lambda v</script><p>标量$\lambda$为特征向量$v$对应的特征值（eigenvalue）。（类似地，我们也可以定义 左特征向量（left eigenvector）$v^⊤A = v^⊤λ$，但是通常我们更关注右特征向量（right eigenvector））。</p><p>假设矩阵 <strong>A</strong> 有 <em>n</em> 个线性无关的特征向量 $\{ v^{(1)}, . . . , v^{(n)} \}$，对应着特征值$\{ λ_1, . . . , λ_n \}$。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$V = \{ v^{(1)}, . . . , v^{(n)} \}$类似地，我们也可以将特征值连接成一个向量 $λ = \{ λ_1, . . . , λ_n \}^T$。因此 <strong>A</strong> 的 特征分解（eigendecomposition）可以记作</p><script type="math/tex; mode=display">A = V diag(λ) V^{-1}</script><p><span style="color:red">正交分解</span>：每个实对称矩阵都可以分解为实特征向量和实特征值。</p><script type="math/tex; mode=display">A = Q \Lambda Q^T</script><p>其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\lambda_i$对应的特征向量是矩阵$Q$的第$i$列，记作$Q_{:,i}$。我们通常按降序排列 的元素。在该约$\Lambda$定下，特征分解唯一当且仅当所有的特征值都是唯一的。</p><p>我们可以将 A 看作沿方向 $v^{(i)}$ 延展 $λ_i$ 倍的空间。如图2.3 所示的例子。</p><img src="/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/image-20200713160411504.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/image-20200713160411504.png" srcset="/img/loading.gif" alt></p><p>实对称矩阵的特征分解也可以用于优化二次方程 $f(x) = x^⊤Ax$，其中限制 $∥x∥_2 = 1$。当 $x$等于 $A$ 的某个特征向量时，$f$ 将返回对应的特征值。在限制条件下，函数  的最大值$f$是最大特征值，最小值是最小特征值。</p><p><span style="color:red">奇异值分解（singular value decomposition, SVD）</span>：将矩阵分解为 奇异向量（singular vector）和 奇异值（singular value）。</p><script type="math/tex; mode=display">A = U D V^T</script><p>其中假设$A$是一个$m \times n$的矩阵，那么$U$将是一个$m \times m$的方阵，$D$是一个$m \times n$的矩阵，$V$是一个$n \times n$的方阵。</p><p>这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵 $U$ 和 $V$ 都定义为正交矩阵，而矩阵 $D$ 定义为对角矩阵。注意，矩阵 $D$ 不一定是方阵。</p><p>对角矩阵 $D$ 对角线上的元素被称为矩阵 $A$ 的 奇异值（singular value）。矩阵$U$ 的列向量被称为 左奇异向量（left singular vector），矩阵 $V$ 的列向量被称 右奇异向量（right singular vector）。</p><p>$A$的<span style="color:red">左奇异向量（left singular vector）</span>是 $AA^⊤$ 的特征向量。</p><p>$A$的<span style="color:red">右奇异向量（right singular vector）</span>是 $A^⊤A$ 的特征向量。</p><p>$A$的<span style="color:red">非零奇异值</span>是 $A^⊤A$ 特征值的平方根，同时也是$AA^⊤$ 特征值的平方根。</p><h2 id="2-6-Moore-Penrose-伪逆、迹运算、行列式"><a href="#2-6-Moore-Penrose-伪逆、迹运算、行列式" class="headerlink" title="2.6 Moore-Penrose 伪逆、迹运算、行列式"></a>2.6 Moore-Penrose 伪逆、迹运算、行列式</h2><p> <span style="color:red">Moore-Penrose 伪逆（Moore-Penrose pseudoinverse）</span>：矩阵$A$的伪逆定义为</p><script type="math/tex; mode=display">A^+ = \lim\limits_{\alpha \searrow 0}(A^{\top} A + \alpha I)^{-1}A^{\top}</script><p>但实际的伪逆算法没有基于这个定义，而是</p><script type="math/tex; mode=display">A^+ = V D^+ U^{\top}</script><p>其中，矩阵$U,\ D,\ V$分别是矩阵$A$奇异值分解后得到的矩阵。对角矩阵$D$的伪逆$D^+$是其非0元素取倒数之后再转置得到的。</p><p>当矩阵$A$的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+ y$ 是方程所有可行解中欧几里得范数 $∥x∥_2$ 最小的一个。</p><p>当矩阵 $A$ 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的 $x$使得  $Ax$和$y$的欧几里得距离 $∥Ax - y∥_2$ 最小。</p><p><span style="color:red">迹运算</span>：</p><script type="math/tex; mode=display">\begin{align}\parallel A \parallel_F &= \sqrt{Tr(AA^{\top})} \newlineTr(A) &= Tr(A^{\top}) \newlineTr(ABC) &= Tr(BCA) = Tr(CAB) \newlineTr(AB) &= Tr(BA) \end{align}</script><p><span style="color:red">行列式</span>：记作 $det(A)$，是一个将方阵  $A$ 映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。</p><p>如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的体积。如果行列式是 1，那么这个转换保持空间体积不变。</p><h2 id="2-7-PCA（主成分分析，principal-components-analysis）"><a href="#2-7-PCA（主成分分析，principal-components-analysis）" class="headerlink" title="2.7 PCA（主成分分析，principal components analysis）"></a>2.7 PCA（主成分分析，principal components analysis）</h2><hr><p>问题描述：假设我们有 $m$ 个数据点 $x^{(1)}, . . . , x^{(m)}∈\mathbb{R}^n$，对于每个数据点$x^{(i)}$ ，我们希望找到⼀个对应的点 $c^{(i)} \in \mathbb{R}^l,\ l\lt n$去表⽰它 (相当于对它进⾏降维)，并且让损失的信息量尽可能少。</p><hr><p>解题思路</p><p>我们可以将这个过程看作是一个编码解码的过程。</p><p>编码函数：$f(x) = c$；</p><p>解码函数：$x \approx g(f(x))$；</p><p>则 PCA 由我们选择的解码函数确定，而为了简化解码器，我们使用矩阵乘法将编码映射回$\mathbb{R}^n$，即$g(c) = Dc$，其中$D \in \mathbb{R}^{n \times l}$是定义的解码矩阵，且$D$的列向量相互正交。</p><p>此时，问题可能有多个解。而为了获得唯一解，则可假定$D$的所有列向量均具有单位范数。</p><p>对于给定的$x$，我们需要找到信息损失最小的$c^*$，即</p><script type="math/tex; mode=display">c^* = \arg\min\limits_c ||x - g(c)||_2 = \arg\min\limits_c ||x - g(c)||_2^2</script><p>我们用$L^2$范数来衡量它们的距离，又因$L^2$范数是非负的并且其平方运算在非负值上单调递增，则两者在相同的$c$上取得最小值，则可用平方$L^2$范数代替$L^2$范数。而</p><script type="math/tex; mode=display">||x - g(c)||_2 = (x - g(c))^{\top}(x - g(c)) = x^{\top}x - x^{\top}g(c) - {g(c)}^{\top}x + {g(c)}^{\top}{g(c)}</script><p>则可忽略不依赖$c$的$x^{\top}x$，则有</p><script type="math/tex; mode=display">\begin{align}c^* &= \arg\min\limits_c[x^{\top}x - x^{\top}g(c) - {g(c)}^{\top}x + {g(c)}^{\top}{g(c)}] \newline&= \arg\min\limits_c[-2x^{\top}g(c) + {g(c)}^{\top}{g(c)}] \newline代入g(c)=Dc， &= \arg\min\limits_c[-2x^{\top}Dc + {\left(Dc\right)}^{\top}{Dc}] \newline&= \arg\min\limits_c[-2x^{\top}Dc + c^{\top}D^{\top}{Dc}] \newline由于D具有单位正交性，&= \arg\min\limits_c[-2x^{\top}Dc + c^{\top}I_l c] \newline&= \arg\min\limits_c[-2x^{\top}Dc + c^{\top} c] \newline\end{align}</script><p>则对$c$求梯度，并令其为0有</p><script type="math/tex; mode=display">\begin{align}\nabla_c (-2x^{\top}Dc + c^{\top} c) &= 0 \newline-2D^{\top}x + 2c &= 0 \newline则， c &= D^{\top}x\end{align}</script><p>因此，</p><p>编码函数：$f(x) = c = D^{\top}x$；</p><p>编码解码得到的重构函数：$r(x) = g(f(x)) = g(c) = Dc = DD^{\top}x$；</p><p>接下来，我们就需挑选最优的编码矩阵$D$，所以我们必须最小化所有维度和所有点上的误差矩阵的 Frobenius 范数。</p><script type="math/tex; mode=display">D^* = \arg\min\limits_D \sqrt{\sum\limits_{i,j} \left(x_j^{(i)} - r(x^{(i)})_j \right)^2} \qquad subject\ to\ D^{\top}D = I_l</script><p>而为了方便，我们考虑$l = 1$的情况，此时问题简化为</p><script type="math/tex; mode=display">\begin{align}&d^* = \arg\min\limits_d \sum\limits_i \left( x_j^{(i)} - dd^{\top}x^{(i)} \right)^2 \newline&s.t.\ d^{\top}d = 1\end{align}</script><p>考虑标量$d^{\top}x^{(i)}$的转置与自身相等，则可化为</p><script type="math/tex; mode=display">\begin{align}d^* &= \arg\min\limits_d \sum\limits_i \left( x_j^{(i)} - d^{\top}x^{(i)}d \right)^2 \quad s.t.\ d^{\top}d = 1 \newline&= \arg\min\limits_d \sum\limits_i \left( x_j^{(i)} - {x^{(i)}}^{\top}dd \right)^2 \end{align}</script><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-第一章 引言</title>
    <link href="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/"/>
    <url>/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h1><p class="note note-info">深度学习、花书</p><a id="more"></a><h2 id="1-早期问题"><a href="#1-早期问题" class="headerlink" title="1 早期问题"></a>1 早期问题</h2><p style="text-indent:2em">在人工智能的早期，那些对人类智力来说非常困难、但对计算机来说相对简单的问题得到迅速解决。比如，那些可以通过一系列形式化的数学规则来描述的问题。</p><p style="text-indent:2em">人工智能的真正挑战在于解决那些对人来说很容易执行、但很难形式化描述的任务，如识别人们所说的话或图像中的脸。对于这些问题，我们人类往往可以凭借直觉轻易地解决。</p><p>针对这些比较直观的问题，本书讨论一种解决方案<span style="color:red">(AI 深度学习（deep learning）)</span>。</p><ul><li>该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。</li><li>让计算机从经验获取知识，可以避免由人类来给计算机形式化地指定它需要的所有知识。</li><li>层次化的概念让计算机构建较简单的概念来学习复杂概念。</li></ul><p>一个<span style="color:red">关键挑战</span>：如何将这些非形式化的知识传达给计算机。</p><p><span style="color:red">硬编码 (hard code)</span>：计算机可以使用逻辑推理规则来自动地理解这些形式化语言中的声明。这就是众所周知的人工智能的知识库（knowledge base）方法。</p><p><span style="color:red">机器学习（machine learning）</span>：由于依靠硬编码的知识体系面对的困难表明，AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。</p><p>引入机器学习使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决</p><p>策。如：</p><ul><li>一个被称为 逻辑回归（logistic regression）的简单机器学习算法可以决定是否建议剖腹产 (Mor-Yosef <em>et al.</em>, 1990)。</li><li>简单机器学习算法朴素贝叶斯（naive Bayes）则可以区分垃圾电子邮件和合法电子邮件。</li></ul><p>但是这些简单的机器学习算法的性能在很大程度上依赖于给定数据的<span style="color:red">表示（repre sentation）</span>。，表示的选择对机器学习算法的性能产生巨大的影响。</p><p style="text-indent:2em">许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。例如，对于通过声音鉴别说话者的任务来说，一个有用的特征是对其声道大小的估计。这个特征为判断说话者是男性、女性还是儿童提供了有力线索。</p><p style="text-indent:2em">然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所以我们可能会想用车轮的存在与否作为特征。不幸的是，我们难以准确地根据像素值来描述车轮看上去像什么。虽然车轮具有简单的几何形状，但它的图像可能会因场景而异，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等。</p><p style="text-indent:2em">解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为<span style="color:red">表示学习（representation learning）</span>。</p><p style="text-indent:2em">表示学习算法的典型例子是<span style="color:red">自编码器</span>（autoencoder）。自编码器由一个编码器encoder）函数和一个解码器（decoder）函数组合而成。</p><p><span style="color:red">编码器</span>：编码器函数将输入数据转换为一种不同的表示。</p><p><span style="color:red">解码器</span>：解码器函数则将这个新的表示转换到原来的形式。</p><p style="text-indent:2em">我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。为了实现不同的特性，我们可以设计不同形式的自编码器。</p><p style="text-indent:2em">当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的<span style="color:red">变差因素（factors of variation）</span>。在此背景下，‘‘因素’’ 这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。</p><p style="text-indent:2em">显然，从原始数据中提取如此高层次、抽象的特征是非常困难的。许多诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类水平的理解来辨识。这几乎与获得原问题的表示一样困难，因此，乍一看，表示学习似乎并不能帮助我们。</p><p><span style="color:red">深度学习（deep learning）</span>通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。深度学习让计算机通过较简单概念构建复杂的概念。图 1.2 展示了深度学习系统。</p><img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710112001631.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710112001631.png" srcset="/img/loading.gif" alt></p><p><span style="color:red">多层感知机（multilayer perceptron, MLP）</span>：是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。</p><p>种度量模型深度的方式：</p><ol><li>基于评估架构所需执行的顺序指令的数目。<br><p style="text-indent:2em">假设我们将模型表示为给定输入后，计算对应输出的流程图，则可以将这张流程图中的最长路径视为模型的深度。正如两个使用不同语言编写的等价程序将具有不同的长度；相同的函数可以被绘制为具有不同深度的流程图，其深度取决于我们可以用来作为一个步骤的函数。图 1.3 说明了语言的选择如何给相同的架构两个不同的衡量。</p><img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710113610351.png" srcset="/img/loading.gif" class></li></ol><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710113610351.png" srcset="/img/loading.gif" alt></p><ol><li>将描述概念彼此如何关联的图的深度视为模型深度。<p style="text-indent:2em">在这种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图更深。这是因为系统对较简单概念的理解在给出更复杂概念的信息后可以进一步精细化。例如，一个 AI 系统观察其中一只眼睛在阴影中的脸部图像时，它最初可能只看到一只眼睛。但当检测到脸部的存在后，系统可以推断第二只眼睛也可能是存在的。在这种情况下，概念的图仅包括两层（关于眼睛的层和关于脸的层），但如果我们细化每个概念的估计将需要额外的 n 次计算，即计算的图将包含 2n 层。 </p></li></ol><p style="text-indent:2em">深度学习是一种特定类型的机器学习，具有强大的能力和灵活性，它将大千世界表示为嵌套的层次概念体系（由较简单概念间的联系定义复杂概念、从一般抽象概括到高级抽象表示）。</p><p>图1.4 说明了这些不同的 AI 学科之间的关系。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115035883.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115035883.png" srcset="/img/loading.gif" alt></p><p>图 1.5 展示了每个学科如何工作的高层次原理。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115508658.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115508658.png" srcset="/img/loading.gif" alt></p><p>图 1.6 展示了本书的组织架构。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115751350.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115751350.png" srcset="/img/loading.gif" alt></p><h2 id="2-深度学习的历史趋势"><a href="#2-深度学习的历史趋势" class="headerlink" title="2 深度学习的历史趋势"></a>2 深度学习的历史趋势</h2><h3 id="2-1-神经网络的众多名称和命运变迁"><a href="#2-1-神经网络的众多名称和命运变迁" class="headerlink" title="2.1 神经网络的众多名称和命运变迁"></a>2.1 神经网络的众多名称和命运变迁</h3><p>三次发展浪潮：</p><ol><li>20世纪40年代到60年代深度学习的雏形出现在控制论（cybernetics）中。</li><li>20 世纪 80 年代到 90 年代深度学习表现为联结主义（connectionism）。</li><li>直到 2006 年，才真正以深度学习之名复兴。</li></ol><p>图 1.7 给出了定量的展示。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710121022428.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710121022428.png" srcset="/img/loading.gif" alt></p><p><span style="color:red">控制论</span>：</p><ul><li>$n$个输入：$x_1, x_2, \cdots, x_n$。</li><li>一组权重：$w_1, w_2, \cdots, w_n$。</li><li>输出：$f(x, w)=x_1w_1+\cdots+x_nw_n$。</li></ul><p style="text-indent:2em">McCulloch-Pitts 神经元 (McCulloch and Pitts, 1943) 是脑功能的早期模型。该线性模型通过检验函数 f(x, w) 的正负来识别两种不同类别的输入。显然，模型的权重需要正确设置后才能使模型的输出对应于期望的类别。这些权重可以由操作人员设定。在 20 世纪 50 年代，<span style="color:red">感知机</span> (Rosenblatt, 1956, 1958) 成为第一个能根据每个类别的输入样本来学习权重的模型。约在同一时期，<span style="color:red">自适应线性单元 (adaptive linear element, ADALINE)</span> 简单地返回函数 *f*(**x**) 本身的值来预测一个实数 (Widrow and Hoffff, 1960)，并且它还可以学习从数据预测这些数。</p><p><span style="color:red">随机梯度下降（stochastic gradient descent）</span>的一种特例：调节 ADALINE 权重的训练算法。</p><p><span style="color:red">线性模型（linear model）</span>：基于感知机和 ADALINE 中使用的函数 <em>f</em>(x, w) 的模型。</p><p>局限性：无法学习异或（XOR）函数，即$f([0, 1], w)=1和f([1, 0], w)=1，但f([1, 1], w)=0和f([0, 0], w)=0$。</p><p><span style="color:red">计算神经科学</span>：了解大脑是如何在算法层面上工作的尝试。</p><p><span style="color:red">深度学习领域主要关注任务</span>：如何构建计算机系统，从而成功解决需要智能才能解决的任务。</p><p><span style="color:red">计算神经科学领域主要关注任务</span>：构建大脑如何真实工作的比较精确的模型。</p><p><span style="color:red">联结主义（connectionism）或并行分布处理 ( parallel distributed processing) </span>：当网络将大量简单的计算单元连接在一起时可以实现智能行为。</p><p><span style="color:red"> 分布式表示（distributed representation）</span>：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。</p><p style="text-indent:2em">例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统，表示这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或隐藏单元激活。这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。改善这种情况的方法之一是使用分布式表示，即用三个神经元描述颜色，三个神经元描述对象身份。这仅仅需要 6 个神经元而不是 9 个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。</p><p> 相关研究进展：</p><ul><li>在 20 世纪 90 年代，研究人员在使用神经网络进行序列建模的方面取得了重要进展。Hochreiter (1991b) 和 Bengio <em>et al.</em> (1994a) 指出了对长序列进行建模的一些根本性数学难题。</li><li>Hochreiter and Schmidhuber (1997)引入 长短期记忆（long short-term memory, LSTM）网络来解决这些难题。如今，LSTM 在许多序列建模任务中广泛应用，包括 Google 的许多自然语言处理任务。</li><li>神经网络继续在某些任务上获得令人印象深刻的表现 (LeCun <em>et al.</em>,  1998c; Bengio <em>et al.</em>, 2001a)。加拿大高级研究所（CIFAR）通过其神经计算和自适应感知（NCAP）研究计划帮助维持神经网络研究。该计划联合了分别由 Geoffffrey Hinton、Yoshua Bengio和 Yann LeCun 领导的多伦多大学、蒙特利尔大学和纽约大学的机器学习研究小组。这个多学科的 CIFAR NCAP 研究计划还囊括了神经科学家、人类和计算机视觉专家。</li></ul><p style="text-indent:2em">神经网络研究的第三次浪潮始于 2006 年的突破。Geoffffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton *et al.*, 2006a)，神经网络研究的第三次浪潮始于 2006 年的突破。eoffffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton *et al.*, 2006a)，我们将在第 15.1 节中更详细地描述。其他 CIFAR 附属研究小组很快表明，同样的策略可以被用来训练许多其他类型的深度网络 (Bengio andLeCun, 2007a; Ranzato *et al.*, 2007b)，并能系统地帮助提高在测试样例上的泛化能力。神经网络研究的这一次浪潮普及了 “深度学习’’ 这一术语的使用，强调研究者现在有能力训练以前不可能训练的比较深的神经网络，并着力于深度的理论重要性上 (Bengio and LeCun, 2007b; Delalleau and Bengio, 2011; ascanu *et al.*, 2014a; Montufar *et al.*, 2014)。此时，深度神经网络已经优于与之竞争的基于其他机器学习技术以及手工设计功能的 AI 系统。在写这本书的时候，神经网络的第三次发展浪潮仍在继续，尽管深度学习的研究重点在这一段时间内发生了巨大变化。第三次浪潮已开始着眼于新的无监督学习技术和深度模型在小数据集的泛化能力，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据集的能力。</p><h3 id="2-2-与日俱增的数据量"><a href="#2-2-与日俱增的数据量" class="headerlink" title="2.2 与日俱增的数据量"></a>2.2 与日俱增的数据量</h3><p>图 1.8 展示了基准数据集的大小如何随着时间的推移而显著增加。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154348314.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154348314.png" srcset="/img/loading.gif" alt></p><p>图 1.10 展示了神经元连接数如何随着时间的推移而显著增加。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154829899.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154829899.png" srcset="/img/loading.gif" alt></p><p>图 1.11 展示了神经元网络规模如何随着时间的推移而显著增加。</p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710155243539.png" srcset="/img/loading.gif" alt></p><h3 id="2-3-应用领域"><a href="#2-3-应用领域" class="headerlink" title="2.3 应用领域"></a>2.3 应用领域</h3><p>图像识别：</p><ol><li>最早的深度模型被用来识别裁剪紧凑且非常小的图像中的单个对象(Rumelhart <em>et al.</em>, 1986d)。</li><li>此后，神经网络可以处理的图像尺寸逐渐增加。现代对象识别网络能处理丰富的高分辨率照片，并且不需要在被识别的对象附近进行裁剪(Krizhevsky <em>et al.</em>, 2012b)。</li><li>类似地，最早的网络只能识别两种对象（或在某些情况下，单类对象的存在与否），而这些现代网络通常能够识别至少1000个不同类别的对象。对象识别中最大的比赛是每年举行的 ImageNet 大型视觉识别挑战（ILSVRC）。深度学习迅速崛起的激动人心的一幕是卷积网络第一次大幅赢得这一挑战，它将最高水准的前5 错误率从 26.1% 降到 15.3% (Krizhevsky <em>et al.</em>, 2012b)，这意味着该卷积网络针对每个图像的可能类别生成一个顺序列表，除了 15.3% 的测试样本，其他测试样本的正确类标都出现在此列表中的前 5 项里。此后，深度卷积网络连续地赢得这些比赛，截至写本书时，深度学习的最新结果将这个比赛中的前 5 错误率降到了 3.6%，如图 1.12 所示。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710165940221.png" srcset="/img/loading.gif" class><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710165940221.png" srcset="/img/loading.gif" alt></li></ol><p>语音识别：</p><ul><li>语音识别在 20 世纪 90 年代得到提高后，直到约 2000 年都停滞不前。深度学习的引入 (Dahl <em>et al.</em>, 2010; Deng <em>et al.</em>, 2010b; Seide <em>et al.</em>, 2011; Hinton <em>et al.</em>, 2012a) 使得语音识别错误率陡然下降，有些错误率甚至降低了一半。</li></ul><p>行人检测和图像分割:</p><ul><li>(Sermanet <em>et al.</em>, 2013; Farabet <em>et al.</em>, 2013; Couprie <em>et al.</em>, 2013)，并且在交通标志分类上取得了超越人类的表现 (Ciresan <em>et al.</em>, 2012)</li></ul><p style="text-indent:2em">在深度网络的规模和精度有所提高的同时，它们可以解决的任务也日益复杂。Goodfellow *et al.* (2014d) 表明，神经网络可以学习输出描述图像的整个字符序列，而不是仅仅识别单个对象。此前，人们普遍认为，这种学习需要对序列中的单个元素进行标注 (Gulcehre and Bengio, 2013)。循环神经网络，如之前提到的LSTM 序列模型，现在用于对序列和其他序列之间的关系进行建模，而不是仅仅固定输入之间的关系。这种序列到序列的学习似乎引领着另一个应用的颠覆性发展，即机器翻译 (Sutskever et al., 2014; Bahdanau et al., 2015)。</p><p>这种复杂性日益增加的趋势已将其推向逻辑结论，即神经图灵机 (Graves <em>et al.</em>, 2014) 的引入。</p><p><span style="color:red">神经图灵机 </span>：能学习读取存储单元和向存储单元写入任意内容。这样的神经网络可以从期望行为的样本中学习简单的程序。</p><p style="text-indent:2em">例如，从杂乱和排好序的样本中学习对一系列数进行排序。这种自我编程技术正处于起步阶段，但原则上未来可以适用于几乎所有的任务。</p><p>深度学习的另一个最大的成就是其在 强化学习（reinforcement learning）领域的扩展。</p><p><span style="color:red">强化学习 </span>：在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。</p><p style="text-indent:2em">DeepMind 表明，基于深度学习的强化学习系统能够学会玩Atari 视频游戏，并在多种任务中可与人类匹敌 (Mnih et al., 2015)。深度学习也显著改善了机器人强化学习的性能 (Finn et al., 2015)。</p><p>技术公司：Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflflix、NVIDIA和 NEC 等。</p><p>软件基础架构的进展：软件库如 Theano (Bergstra <em>et al.</em>, 2010a; Bastien <em>et al.</em>, 2012a)、PyLearn2 (Goodfellow <em>et al.</em>, 2013e)、Torch (Col lobert <em>et al.</em>, 2011b)、DistBelief (Dean <em>et al.</em>, 2012)、Caffffe (Jia, 2013)、MXNet (Chen <em>et al.</em>, 2015) 和 TensorFlow (Abadi <em>et al.</em>, 2015) 都能支持重要的研究项目或商业产品。</p><p style="text-indent:2em">深度学习也为其他科学做出了贡献。用于对象识别的现代卷积网络为神经科学家们提供了可以研究的视觉处理模型 (DiCarlo, 2013)。深度学习也为处理海量数据以及在科学领域作出有效的预测提供了非常有用的工具。它已成功地用于预测分子如何相互作用从而帮助制药公司设计新的药物 (Dahl et al., 2014)，搜索亚原子粒子 (Baldi et al., 2014)，以及自动解析用于构建人脑三维图的显微镜图像(Knowles-Barley et al., 2014) 等。我们期待深度学习未来能够出现在越来越多的科学领域中。</p><p style="text-indent:2em">总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的普及性和实用性都有了极大的发展。未来几年充满了进一步提高深度学习并将它带到新领域的挑战和机遇。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">deep learning</a>.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/MingchaoZhu/DeepLearning" target="_blank" rel="noopener">深度学习</a>。<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2014-Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</title>
    <link href="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/"/>
    <url>/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/</url>
    
    <content type="html"><![CDATA[<h1 id="Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises"><a href="#Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises" class="headerlink" title="Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises"></a>Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</h1><p class="note note-info">属性探索、快速算法</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Uwe Ryssel、Felix Distel、Daniel Borchmann。</p><p>​            2. 期刊：Annals of Mathematics and Artificial Intelligence。</p><p>​            3. 时间：2014。</p><p>​            4. DOI：10.1007/s10472-013-9355-9 。 </p><p style="display:none">​    @article{Ryssel2014Fast,​      title={Fast algorithms for implication bases and attribute exploration using proper premises},​      author={Ryssel, Uwe and Distel, Felix and Borchmann, Daniel},​      journal={Annals of Mathematics & Artificial Intelligence},​      volume={70},​      number={1-2},​      pages={25-53},​      year={2014},​    },​    @inproceedings{Ganter2010Two,​      title={Two Basic Algorithms in Concept Analysis},​      author={Ganter, Bernhard},​      booktitle={International Conference on Formal Concept Analysis},​      year={2010},​      },​      @article{Obiedkov2007Attribute,​      title={Attribute-incremental construction of the canonical implication basis},​      author={Obiedkov, S. and Duquenne, V.},​      journal={Annals of Mathematics & Artificial Intelligence},​      volume={49},​      number={1-4},​      pages={p.77-99},​      year={2007},​      }  ​    </p><p></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>形式概念分析的中心任务是枚举形式背景的最小蕴涵基。</li><li>本文提出了一种快速计算适当前提的新算法。<ul><li>减少了多次获得适当前提的数量。</li><li>减少了在适当前提集合内的冗余。</li></ul></li></ul><p>词汇积累：</p><ul><li>minimal hypergraph transversals：最小超图横断面。</li><li>refactoring：重构。</li><li>heuristic：启发式。</li><li>refactoring of model variants：模型变体重构。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>计算主基存在2种方法$^{[17, 26]}$，由于这两种方法均计算了概念内涵，则无法改变其指数级的复杂度$^{[1, 10]}$。</p></li><li><p>形式概念分析早期提出了一种具有适当前提基的算法，该算法避免了计算概念内涵。</p></li><li><p>有一些方法可以在多项式时间内将适当前提基转换为主基$^{[24, 28]}$。</p></li><li><p><span style="color:red">本文提出了一种快速计算适当前提的算法。基于以下三个思想：</span></p><ol><li>在适当前提和最小超图横断面之间使用一个简单的联系。</li><li>对最小超图断面的枚举问题进行了深入的研究。</li><li>可对现有算法可使用与适当前提的联系。</li></ol><p>1.首先，使用原算法遍历所有属性，并使用黑盒超图算法来计算每个属性的适当前提。<br>2.为了避免多次计算相同的适当前提，本文引入了一个候选过滤器：对属性集中的每个属性进行过滤，并且只在候选属性集中搜素合适的前提。本文表明，这种过滤方法在保持完备性的同时，大大减少了多次计算的适当前提的数量。<br>3.通过仅在交不可约属性集中搜索适当的前提来移除适当前提内的冗余。<br>4.本文认为该算法对于并行化来说是微不足道的，从而导致进一步的加速。由于其增量性质，基于主基的算法的并行化版本至今尚不为人所知。<br>5.本文将给出使用适当前提的属性探索的另一种变体。它使用与本文对主基的枚举算法相同的方法。</p></li><li><p>考虑不使用伪意图的属性探索的替代公式。在$^{[27]}$中首次尝试使用适当的前提来制定属性探索。&lt;/div</p></li></ol><p>词汇积累：</p><ul><li>well-researched：深入研究。</li><li>artifacts：史前古器物；人工产品。</li><li>negated：否定的。</li><li>fractions of a second：几分之一秒。</li><li>arguments：论点。</li></ul><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>1.$g\swarrow m$：在不包含$m$的对象内涵中，关于子集的顺序$g’$是最大的。</p><p>2.蕴涵式$\mathscr{L}$的形式化描述：</p><script type="math/tex; mode=display">\begin{align} \mathscr{L}^1(A)&=A\cup \bigcup\{Y|(X\rightarrow Y)\in \mathscr{L}, X\subseteq A\}  \newline\mathscr{L}^i(A)&=\mathscr{L}^1(\mathscr{L}^{i-1}(A) )\quad for\ i>1  \newline\mathscr{L}(A)&=\bigcup\limits_{i\in N>0}\mathscr{L}^i(A)\end{align}</script><p>原文：Then an implication X →Y follows from the set $\mathscr{L}$ of implications if and only if $Y ⊆ \mathscr{L}$(X). We write $\mathscr{L}\models(X →Y)$ if and only if X →Y follows from $\mathscr{L}$.</p><p>解释：一个蕴涵式$X\rightarrow Y$可从蕴涵式集合$\mathscr{L}$中推出来$，当且仅当$$Y\subseteq \mathscr{L}(X)$成立。可写作$\mathscr{L}\models(X →Y)$，当且仅当$X\rightarrow Y$可由$\mathscr{L}$推出来成立。</p><p><span style="color:red">sound(非冗余的、无噪声的)</span>：$\mathscr{L}$中的所有蕴涵式对于形式背景$\mathbb{K}$均成立。</p><p><span style="color:red">complete(完备的)</span>：形式背景$\mathbb{K}$中成立的所有蕴涵式均来自于$\mathscr{L}$。</p><p>如果$\mathscr{L}$对于形式背景$\mathbb{K}$是sound and complete，则$\mathscr{L}$就称为形式背景$\mathbb{K}$的一个基。进一步地，$\mathscr{L}^1(A)=\mathscr{L}(A)\ holds\ for\ all\ A\subseteq M$，则$\mathscr{L}$称为直接基。</p><p><span style="color:red">伪内涵</span>：$P\neq P^{“}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{“}\subseteq P$。则形式背景的主基为$\{P\rightarrow P^{“}|P: pseudo-intent\ of\ \mathbb{K}\}$。</p><p><span style="color:red">前提</span>：B is called a premise for m if $m\in B^{“}\backslash B$，则形式背景的一个基为：$\mathscr{L}=\{B\rightarrow B^{“}|B: B\subseteq M, premise\ for\ some\ m\in M\}$。</p><p><span style="color:red">适当前提</span>：B  is called a proper premise if $B^{\bullet}$ is not empty，其中$B^{\bullet}=B^{“}\backslash \left(B\cup\bigcup\limits_{S\subsetneq B}S^{“}\right)$。如果$m\in B^{\bullet}$，则称$m$在$m\in M$中是一个适当前提。可以得到$B$对于$m$是一个适当前提，当且仅当$B$在$m$的前提中是$\subseteq-minimal$。</p><p><span style="color:red">适当前提基</span>：进一步地有{$B\rightarrow B^{\bullet}|B: proper\ premise$}是形式背景$\mathbb{K}$的一个complete and sound 直接基。则该集合称为形式背景$\mathbb{K}$的适当前提基。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" alt></p><h2 id="3-Proper-Premises-as-Minimal-Hypergraph-Transversals"><a href="#3-Proper-Premises-as-Minimal-Hypergraph-Transversals" class="headerlink" title="3 Proper Premises as Minimal Hypergraph Transversals"></a>3 Proper Premises as Minimal Hypergraph Transversals</h2><h3 id="3-1-基本定义"><a href="#3-1-基本定义" class="headerlink" title="3.1 基本定义"></a>3.1 基本定义</h3><p><span style="color:red">minimal hypergraph transversals</span>：用于从关系型数据库中挖掘函数依赖。</p><p>超图以前已经用于关联规则挖掘的相关任务$^{[33]}$。超图如何应用于数据挖掘的概述可以在$^{[20]}$中找到。 </p><p><span style="color:red">hypergraph</span>：$V$是有限的顶点集，$V$上的一个超图$\mathscr{H}$是幂集$2^V$的一个子集。每一个集合$E\in \mathscr{H}$为超图的一条边，与经典图论不同的是，这条边可以关联到两个以上的顶点，也可以关联到两个以下的顶点。</p><p><span style="color:red">hypergraph transversal</span>：一个集合$S\subseteq V$被称为$\mathscr{H}$的 hypergraph transversal，当它与每条边$E\in \mathscr{H}$都相交。 即$\forall E\in \mathscr{H}, S\cap E\neq \varnothing$。</p><p><span style="color:red">minimal hypergraph transversal</span>：集合$S\subseteq V$被称为最小超图横断面，当S在$\mathscr{H}$的所有超图横断面中的子集序下是最小的。</p><p><span style="color:red">transversal hypergraph</span>：$\mathscr{H}$的所有 minimal hypergraph transversal构成的集合。记为$Tr(\mathscr{H})$。</p><p>TRANSHYP：判定超图$\mathscr{H}$是否是超图$\mathscr{G}$的 transversal hypergraph 的问题。</p><p>TRANSENUM：枚举出超图$\mathscr{G}$的所有minal hypergraph transversal 的问题。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" alt></p><p>解释：$P\subseteq M\backslash\{m\}对于m\in M$是一个前提，当且仅当对于所有的$g\in G\ with\ g\swarrow m$成立。$P$对于$m$是一个适当前提，当且仅当在$m$的所有前提中，按子集序$P$是最小的。则有推论</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li><p>implicitly：含蓄地；暗中地；间接的；不言而喻。</p></li><li><p>in contrast to：与…不同。</p></li><li><p>be incident to：关联到…。</p></li><li><p>quasi-polynomial time：准多项式时间。</p></li></ul><h2 id="4-Improvements-to-the-Algorithm"><a href="#4-Improvements-to-the-Algorithm" class="headerlink" title="4 Improvements to the Algorithm"></a>4 Improvements to the Algorithm</h2><h3 id="4-1-Avoiding-Duplicates-using-Candidate-Sets"><a href="#4-1-Avoiding-Duplicates-using-Candidate-Sets" class="headerlink" title="4.1 Avoiding Duplicates using Candidate Sets"></a>4.1 Avoiding Duplicates using Candidate Sets</h3><p>原因：Algorithm1会多次计算适当前提，因为它们可以是多个属性的适当前提。如{c, e}是属性a, b, d 的适当前提，则其会计算3次。</p><p>第一种思想：根据当前属性，引入相关属性的候选集。则只需在候选集集合$C$中搜索$\mathscr{H}_{\mathbb{K}, m}^{\swarrow}$最小超图横断面即可。</p><p>冗余条件：$\mu w\wedge\mu m\le\mu v\lt\mu m$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" alt></p><p>Algorithm2的正确性证明。</p><h3 id="4-2-Irreducible-Attributes"><a href="#4-2-Irreducible-Attributes" class="headerlink" title="4.2 Irreducible Attributes"></a>4.2 Irreducible Attributes</h3><p>从候选集合C中移除属性m，属性m满足条件：$\mu m=\wedge_{i=1}^n\mu x_i\ for\ i=1,…, n$。这样的属性称为$\wedge$可约属性，其集合为$N$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708174500270.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708174500270.png" srcset="/img/loading.gif" alt></p><p>Algorithm3的正确性证明。</p><p>词汇积累</p><ul><li>intuition：直觉的。</li><li>snippet：片段。</li><li>identify：确定；识别。</li></ul><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h2><p>算法：</p><ul><li>SB：基于Next-Closure算法计算主基的实现算法。</li><li>HT：和Algorithm1一样，在所有适当前提中计算超图横断面的算法。</li><li>PP：Algorithm3的实现算法。 </li></ul><p>数据集：</p><ol><li>SPECT [9], which describes Single Proton Emission Computed Tomography (SPECT) images. This data set is given as a dyadic formal context with 187 objects, 23 attributes, and an approximate density of 0.38.</li><li>Congressional V oting Records of the U.S. House of Representatives from 1984 [31]. It contains 435 objects, 16 attributes and is given as many valued context. It has been nominally scaled, resulting in a context with 50 attributes and an approximate density of 0.34.</li><li>The third structured data set originates from the application described in Section 5.2 and [30]. It has 26 objects, 79 attributes and an approximate density of 0.35.<img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191106203.png" srcset="/img/loading.gif" class><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191106203.png" srcset="/img/loading.gif" alt></li></ol><p>实验结果：<img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191311319.png" srcset="/img/loading.gif" class> <img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191311319.png" srcset="/img/loading.gif" alt></p><p>词汇积累</p><ul><li>contranominal：相反的。</li><li>trade-off：权衡。</li><li>explicit：显式化的。</li></ul><h2 id="6-Attribute-Exploration"><a href="#6-Attribute-Exploration" class="headerlink" title="6 Attribute Exploration"></a>6 Attribute Exploration</h2><p>基本概念</p><p><span style="color:red">属性探索</span>：一种交互式的形式化算法，即使原形式背景是不完整的，也能够获得sound和complete的蕴涵基集合。其特点为：</p><ul><li>一个不完备的形式背景。</li><li>一个拥有全部领域知识的专家。</li><li>每次迭代，将蕴涵式向专家询问。<ul><li>接受，则将蕴涵式加入蕴涵基集合。</li><li>拒绝，专家提供一个反例加入当前工作形式背景。</li></ul></li></ul><p><span style="color:red">原始形式背景（initial context）</span>：初始探索的形式背景。</p><p><span style="color:red">背景知识（background knowledge）</span>：初始蕴涵式集合。</p><p><span style="color:red">当前工作形式背景（current working context）</span>：每次探索时的形式背景。</p><p><span style="color:red">已知蕴涵式集合（the set of known implications）</span>：探索过程中专家接受的蕴涵式构成的集合。</p><p><span style="color:red">最终形式背景（final context）</span>：整个探索过程结束后的形式背景。</p><p><span style="color:red">背后的形式背景（background context）</span>$\mathbb{K}_{BG}$：隐式的已知的形式背景。</p><p>可以将属性探索看作是使$\mathbb{K}_{BG}$中知识显式化的过程。</p><p>主要挑战:避免重新计算已经被专家接受的蕴涵式。</p><p>词汇积累</p><ul><li>interactive formalism：交互式的形式主义。</li><li>facilitate：便于，促进，帮助；使容易。</li><li>implicit：隐式的。</li></ul><h3 id="6-1-Incremental-Computation-of-Proper-Premises-Using-Berge-Multiplication"><a href="#6-1-Incremental-Computation-of-Proper-Premises-Using-Berge-Multiplication" class="headerlink" title="6.1 Incremental Computation of Proper Premises Using Berge Multiplication"></a>6.1 Incremental Computation of Proper Premises Using Berge Multiplication</h3><h4 id="6-1-1-Berge-Multiplication-and-its-Offspring"><a href="#6-1-1-Berge-Multiplication-and-its-Offspring" class="headerlink" title="6.1.1 Berge Multiplication and its Offspring"></a>6.1.1 Berge Multiplication and its Offspring</h4><p>定义两个超图$\mathscr{G}$和$\mathscr{H}$以边进行合并为</p><script type="math/tex; mode=display">\mathscr{G}\vee \mathscr{H}:=\{ g\cup h|g\in \mathscr{G},\ h\in \mathscr{H} \}</script><p>集合$S$中$\subseteq-minimal$集（子集序）为</p><script type="math/tex; mode=display">min(S):=\{ X\in S |not\ \exists Y\in S:Y\subsetneq X \}</script><p>则对于两个有限的超图$\mathscr{G}$和$\mathscr{H}$有</p><script type="math/tex; mode=display">Tr(\mathscr{G} \cup \mathscr{H})=\min(Tr(\mathscr{G}) \vee Tr(\mathscr{H}))</script><p>Berge Multiplication算法的相关研究：</p><ul><li>Takata$^{[32]}$给出了一个超图族的例子，使得Berge Multiplication算法的最小运行时间为$n^{\Omega(\log\log n)}$，$n$为相应的输出规模。</li><li>Boros$^{[8]}$证明了Berge Multiplication算法在$n^{\sqrt{n} }$的时间内能够获得超图$\mathscr{H}$的所有最小超图横截面，$n$为相应的输出规模。</li><li>所有的Berge Multiplication的变体算法最坏情况的运行时间至少为$n^{\Omega(\log\log n)}$$^{[21]}$，$n$为相应的输出规模。</li></ul><p>词汇积累</p><ul><li>divide and conquer：分而治之。</li><li>hypergraph transversal algorithms：超图遍历算法。</li><li>open question：悬而未决的问题。</li><li>permutation：排列。</li></ul><h4 id="6-1-2-A-Naive-Exploration-Algorithm-using-Proper-Premises"><a href="#6-1-2-A-Naive-Exploration-Algorithm-using-Proper-Premises" class="headerlink" title="6.1.2 A Naive Exploration Algorithm using Proper Premises"></a>6.1.2 A Naive Exploration Algorithm using Proper Premises</h4><p>第一个形式化的带有前提的属性探索算法，令$\mathbb{K}$为原始形式背景，$\mathscr{L}$为背景知识并且$\mathscr{E}=\mathscr{H}_{\mathbb{K}, m}^{\notin}$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712111542675.png" srcset="/img/loading.gif" class> <p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712111542675.png" srcset="/img/loading.gif" alt></p><p>解释：</p><ol><li>该算法遍历所有的属性$m\in M$，并通过Berge Multiplication不断考虑$\mathscr{E}$中的边来计算属性$m$的适当前提。</li><li>如果$\mathscr{L} \nvDash P \rightarrow P^{“}$，则向专家询问蕴涵式$P \rightarrow P^{“}$。<ol><li>接受，将其$P \rightarrow P^{“}$加入$\mathscr{L}$。</li><li>拒绝，$g$作为$P \rightarrow \{ m \}$的反例。<ol><li>如果$m \notin g’$，则将集合$M \backslash g’$作为$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一条边加入$\mathscr{E}$中。</li></ol></li></ol></li><li>2步骤一直持续到$\mathscr{E}$中不存在边剩余，此时就可得到$\mathbb{K}_{B, G}$中所有属性$m$的适当前提。</li><li>最后考虑$M$中的剩余属性。</li></ol><p>这一过程在算法6中正式给出。请注意，在此算法中，我们仅非正式地将专家交互描述为“expert confirms $Q→Q^{“}$”或“ask expert for valid counterexample ”，这在文献中是常见的。但也有可能更正式地描述这种相互作用，就像在$^{[6]}$中所做的那样。然而，我们不会在这里这样做，因为我们没有必要进行进一步的考虑。</p><p>在算法6终止时的形式背景为$\mathbb{K}$，$\mathscr{L}$是<script type="math/tex">\mathbb{K}_{B, G}</script>的一个基，并且$\mathscr{L}$仅包含形式$P \rightarrow P^{“}$的蕴涵式，其中$P$是$\mathbb{K}_{B, G}$的适当前提。</p><p>证明$\mathbb{K}$的适当前提就是$\mathbb{K}_{B, G}$的适当前提。$\Leftarrow$Lemma 3。</p><p>假设对于某些属性$m$，$P$是$\mathbb{K}_{BG}$中的一个真前提。由Lemma 2 知这等价于$P$是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的极小超图横截面。由于所有的反例均取自于$\mathbb{K}_{BG}$，则有$\mathscr{H}_{\mathbb{K}, m}^{\notin} \subseteq  \mathscr{H}_{BG, m}^{\notin}$,因此$P$也是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一个超图横截面。这证明了在$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的超图横截面中$P$的极小性。</p><p>现在假设$Q \subseteq P$是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一个极小超图，那么$Q$是$\mathbb{K}$的一个适当前提，并且有$\mathscr{L} \vDash Q \rightarrow Q^{“}$。而由Lemma 3 可知，$Q$也是$\mathbb{K}_{BG}$的一个适当前提，是$\mathscr{H}_{BG, m}^{\notin}$的一个极小超图横截面。则$P=Q$。这证明了$P$也是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的极小超图横截面即$P$是$\mathbb{K}$的一个适当前提。</p><p>$\Rightarrow$ 算法6终止的形式背景$\mathbb{K}$的前提基与$\mathbb{K}_{BG}$的前提基相同。</p><p>词汇积累</p><ul><li>whereupon：因此。</li></ul><h3 id="6-2-Using-the-improvements"><a href="#6-2-Using-the-improvements" class="headerlink" title="6.2 Using the improvements"></a>6.2 Using the improvements</h3><h4 id="6-2-1-Querying-the-hierarchy-of-attribute-concepts"><a href="#6-2-1-Querying-the-hierarchy-of-attribute-concepts" class="headerlink" title="6.2.1 Querying the hierarchy of attribute concepts"></a>6.2.1 Querying the hierarchy of attribute concepts</h4><p><span style="color:red">find single proper premises</span>:</p><p>算法2和算法3都要求在第一步中计算所有的单一适当前提。在属性探索中，形式背景$\mathbb{K}$最初是不完整的。因此，简单地计算$\mathbb{K}$的所有单个适当前提是不够的，因为我们不知道当形式背景$\mathbb{K}$扩张时它们是否仍然是适当前提。</p><p>然而，如果属性$\{ m \}$是$\mathbb{K}$的一个适当前提，并且专家确认蕴涵式$\{ m \} \rightarrow \{ m \}^{“}$成立，则 Lemma 3 确保了当形式背景$\mathbb{K}$扩张时$\{ m \}$仍然是一个前提。</p><p><span style="color:red">find irreducibles</span>:</p><p>在算法3中，需要知道哪些属性概念是交不可约的。不幸的是，在工作形式背景$\mathbb{K}$中满足可约的属性概念$\mu m$在$\mathbb{K}$的扩展中可能不再满足可约。</p><p>我们可以向专家提出蕴涵式$\{ n \in \{ m \}^{“} | \mu m &lt; \mu n \} \rightarrow \{ m \}$来确定是否可以移除可约属性$\{ m \}$。如果专家接受该蕴涵式，则属性$m$在形式背景$\mathbb{K}$扩张时仍然是交可约的。</p><p>算法7显示了这两个初始查询是如何执行的。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712154529970.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712154529970.png" srcset="/img/loading.gif" alt></p><h4 id="6-2-2-Candidate-Sets"><a href="#6-2-2-Candidate-Sets" class="headerlink" title="6.2.2 Candidate Sets"></a>6.2.2 Candidate Sets</h4><p>将对候选集的限制调整到属性探索设置并非易事。</p><p>问题是，一个属性$u$不是$\mathbb{K}_{init}$中的属性m的候选的属性，但$u$仍然可以是$\mathbb{K}_{B,G}$中的候选属性。</p><p>例子：考虑 Table 7 中的形式背景$\mathbb{K}_{B,G}$，在经过算法7的预处理后，我们得到 Table 8 中的形式背景$\mathbb{K}_{init}$以及蕴涵集$\mathscr{L}_{init}=\{ \{v\} \rightarrow \{m\} \}$。假设我们想获得属性$m$的适当前提，则可计算候选集为：$C_{init}=\{ u \in M \backslash \{m\} |not\ \exists v\in M: \mu u \wedge \mu m \le _{init} \mu v \lt _{init} \mu m \}$。</p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712225021028.png" srcset="/img/loading.gif" alt></p><p>很容易从 Fig. 7 的概念格中看出，它是空集，因此无法计算出属性$m$的前提$\{ u, w \}$。然而从 Fig. 6 的概念格中可以看出，$\{ u, w \}$确实是属性$m$的前提。</p><p>但是请注意，如果我们先计算属性$v$的适当前提 ，再计算属性$m$的适当前提，则不会出现上述问题。然后向专家询问蕴涵式$\{m,u\} \rightarrow \{v\}$是否成立，使得专家添加$E$作为反例。</p><p>显然，这个问题可以通过固定处理属性的顺序来避免，我们可以使$M$中的&lt;序满足$\mu m \lt _{init} \mu n \Rightarrow m \lt n$。算法8中使用了候选集和这种顺序。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712231347704.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712231347704.png" srcset="/img/loading.gif" alt></p><p>如果属性$m$是这中顺序的第一个属性，则$\mu m$是最小的属性概念，因此属性$m$的候选集为$M \backslash \{m\}$。这意味着在第一次迭代后可以获得属性$m$的所有适当前提。而在算法2中，属性$m$的所有适当前提在属性$m$的迭代或属性$v (\mu v \lt \mu m)$的迭代后获得。因此，由归纳法可知，对于所有的属性$n$，以下条件成立：</p><p>在属性$n$处理后，</p><ul><li>对于属性$n$的每一个适当前提$P$，$\mathscr{L}$包含有蕴涵式$P \rightarrow P^{‘’}$。</li><li>对于在$\mathbb{K}_{B,G}$不成立的每个蕴涵式$S \rightarrow \{n\}$，均向$\mathbb{K}$中添加了相应的反例。</li></ul><p>这保证了在$\mathbb{K}_{B,G}$计算的候选集恰好是$\mathbb{K}$中的候选集。下面的 Lemma 7 可以说明这一点。</p><hr><blockquote><p>Lemma 7 假设我们允许算法8运行到属性$m$的迭代，记$\mathbb{K}_m$为此次迭代中获得的形式背景，$\mathbb{K}_{B,G}$为背后的形式背景，$\le_{m}$和$\le_{B,G}$分别为相应概念格的序。</p></blockquote><p>那么</p><script type="math/tex; mode=display">\begin{align}C_{B,G}=& \{ u\in M \backslash \{m\} | not\ \exists v\in M: \mu u \wedge \mu m \le _{BG}{\mu v} \lt _{BG}{\mu m} \} \newline =& \{ u\in M \backslash \{m\} | not\ \exists v\in M: \mu u \wedge \mu m \le _m{\mu v} \lt _m{\mu m} \} \end{align}</script><p>Proof 从 Lemma 6 可知</p><script type="math/tex; mode=display">\mu v \lt _{BG}\mu m \Leftrightarrow \mu v \lt _{init}\mu m \Leftrightarrow \mu v \lt _m \mu m</script><p>而且，由FCA可知一个事实——$\mu u \wedge \mu m \le _{BG}\mu v\ iff\ \{u,m\}^{‘BG} \subseteq \{v\}^{‘BG}$即有$\{u,m\} \rightarrow \{v\}$在$\mathbb{K}_{BG}$中成立。如果有$\mu v \lt _m{\mu m}$，则可知属性$v$在之前的迭代过程中已处理，因此所有在$\mathbb{K}_m$中成立的蕴涵式$S \rightarrow \{v\}\ iff\ they\ hold\ in\ \mathbb{K}_{BG}$。所以可以得到$all\ v\in M\ with\ \mu v \lt _m{\mu m}$。</p><script type="math/tex; mode=display">\begin{align}\mu u \wedge \mu m \le _{BG}\mu v &\Leftrightarrow \{u,m\} \rightarrow \{v\}\ holds\ in\ \mathbb{K}_{BG} \newline&\Leftrightarrow \{u,m\} \rightarrow \{v\}\ holds\ in\ \mathbb{K}_m\ \Leftrightarrow \mu u \wedge \mu m \le _m{\mu v} \end{align}</script><p>从上式三式（9-11）可得以下结论等价：</p><script type="math/tex; mode=display">\begin{align}u\notin C_m &\Leftrightarrow \exists v\in M: \mu u \wedge \mu m \le _m{\mu v} \lt _m{\mu m} \newline&\Leftrightarrow \exists v\in M: \mu u \wedge \mu m \le _{BG}{\mu v} \lt _{BG}{\mu m} \newline&\Leftrightarrow u\notin C_{BG}\end{align}</script><p>这证明了$C_m = C_{BG}$。</p><hr><p>Lemma 7 说明了：如果我们在 background context 中运行算法2， 获得的候选集与以 subcontext 作为算法8的输入获得的候选集相同。而算法8的正确性可由算法2的正确性直接获得（Lemma 1）。</p><h4 id="6-2-3-Removing-Reducibles"><a href="#6-2-3-Removing-Reducibles" class="headerlink" title="6.2.3 Removing Reducibles"></a>6.2.3 Removing Reducibles</h4><p>从 Lemma 6 中，我们可以知道，一旦$\mathbb{K}_{init}$被计算，则在探索过程中，不可约属性集将不会改变，并且实际上与$\mathbb{K}_{BG}$中的不可约属性集相同。因此，可直接将搜索空间限制为不可约属性集来直接扩展算法8获得算法9，而算法9的正确性是算法3和算法9的直接结果。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200713231141823.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200713231141823.png" srcset="/img/loading.gif" alt></p><p>词汇积累</p><ul><li>induction：归纳法。</li></ul><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h2><p>本文</p><ol><li>证明了，当主基具有最小基数时，适当前提的基通常可以更有效地计算。</li><li>在基于超图的算法中进行非常简单的优化会产生很大的性能提升。</li><li>我们已经在随机的、人工生成的形式背景以及来自实际应用的形式背景上对它进行了评估。我们的评估表明，在这些特定的数据集上，我们的算法比现有算法更快。这表明我们的方法可能也更适用于其他数据集，特别是当这些数据集包含大量内涵时。</li><li>至于最小基数，则还需要额外的最小化步骤。在这种情况下，性能收益可能较小。然而，在像模型重构这样的应用程序中，最小基数只是次要的。</li><li>展示了如何执行基于适当前提的探索。并提供了详细的算法描述，并展示了如何将本文第一部分中的改进应用于属性探索设置。</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>快速算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW2</title>
    <link href="/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/"/>
    <url>/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）"><a href="#李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）" class="headerlink" title="李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）"></a>李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）</h1><p>本文主要参考了博文<a href="https://www.cnblogs.com/HL-space/p/10785225.html" target="_blank" rel="noopener">https://www.cnblogs.com/HL-space/p/10785225.html</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是根据每个ID的各种属性值去判断该ID对应角色是Winner还是Losser（收入是否大于50K）。显然这是一个二分类问题，可以运用Logistic Regression来实现。</p><ul><li>spam_train.csv：形状为4000×59，4000行数据对应4000个角色，ID编号从1到4001，59列属性中，第一列为角色ID，最后一列为分类结果（label为0/1），中间57列为角色对应的57种属性值。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><ol><li>本次任务先对数据做线性回归，得出每个样本的回归值，计算公式为：</li></ol><script type="math/tex; mode=display">y^n=\sum\limits_{i=1}^{57}w_ix_i^n+b</script><p>其中，$x_i^n$为第$n$个样本值，$y^n$为对应的回归结果。</p><ol><li><p>将回归结果送入$sigmod$函数，得到对应的概率值。计算公式为：</p><script type="math/tex; mode=display"> p^n=\frac{1}{1+e^{-y^n} }</script></li><li><p>众所周知，不管线性回归还是Logistic回归，其关键和核心就在于通过误差的反向传播来更新参数，进而使模型不断优化。因此，损失函数的确定及对各参数的求导就成了重中之重。在分类问题中，模型一般针对各类别输出一个概率分布，因此常用交叉熵作为损失函数。交叉熵可用于衡量两个概率分布之间的相似、统一程度，两个概率分布越相似、越统一，则交叉熵越小；反之，两概率分布之间差异越大、越混乱，则交叉熵越大。</p></li></ol><p>下式表示k分类问题的交叉熵，P为label，是一个概率分布，常用one_hot编码。例如针对3分类问题而言，若样本属于第一类，则P为(1,0,0)，若属于第二类，则P为(0,1,0)，若属于第三类，则为(0,0,1)。即所属的类概率值为1，其他类概率值为0。Q为模型得出的概率分布，可以是(0.1,0.8,0.1)等。具体计算公式为：</p><script type="math/tex; mode=display">Loss^n=-\sum\limits_1^kp^n\log Q^n</script><p>在实际应用中，为求导方便，常使用以e为底的对数。则上式可化为：</p><script type="math/tex; mode=display">Loss^n=-\sum\limits_1^kp^n\ln Q^n</script><p>针对本次作业而言，虽然模型只输出了一个概率值p，但由于处理的是二分类问题，因此可以很快求出另一概率值为1-p，即可视为模型输出的概率分布为Q(p，1-p)。将本次的label视为概率分布P(y,1-y)，即Winner(label为1)的概率分布为(1,0)，分类为Losser(label为0)的概率分布为(0,1)。则其==损失函数==为：</p><script type="math/tex; mode=display">Loss^n=-[\hat{y}^n\ln p^n+(1-\hat{y}^n)\ln(1-p^n)]</script><p>Loss对权重$w$求偏导，有：</p><script type="math/tex; mode=display">\begin{align}因为&\frac{\partial\ln p^n}{\partial p^n}=\frac{1}{p^n}=1+e^{-y^n} \newline同理&\frac{\partial\ln (1-p^n)}{\partial p^n}=\frac{-1}{1-p^n}=\frac{-e^{-y^n} }{1+e^{-y^n} } \newline而且&\frac{\partial p^n}{\partial y^n}=-\frac{1}{(1+e^{-y^n})^2}(e^{-y^n})(-1)=\frac{-e^{-y^n} }{(1+e^{-y^n})^2} \newline和&\frac{\partial y^n}{\partial w_i}=x_i \newline\newline\end{align}</script><script type="math/tex; mode=display">\begin{align}所以有\frac{\partial Loss^n}{\partial w_i}&=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}]  \newline&=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i]  \newline&=-x_i(\hat{y}^n-p^n)  \end{align}</script><p>同理Loss对偏置$b$求偏导，有：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial Loss^n}{\partial b}&=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}]  \newline&=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})]  \newline&=-(\hat{y}^n-p^n)  \end{align}</script><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>首先将所有空值以0填充，然后对数据进行标准化处理。</p><p>上述操作完成后，将表格的第2列至58列取出为x(shape为4000×57)，将最后一列取出做label y(shape为4000×1)。</p><p>进一步划分训练集和验证集，分别取x、y中前3000个样本为训练集x_train(shape为3000×57)，y_train(shape为3000×1)，后1000个样本为验证集x_val(shape为1000×57)，y_val(shape为1000×1)。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"spam_train.csv"</span>)<span class="hljs-comment"># 空值填0</span>data = data.fillna(<span class="hljs-number">0</span>)<span class="hljs-comment"># 转换为array类型</span>data = np.array(data)<span class="hljs-comment"># 取样本x(4000*57)</span>x = data[:, <span class="hljs-number">1</span>:<span class="hljs-number">-1</span>]<span class="hljs-comment"># 标准化</span>x[<span class="hljs-number">-1</span>] /= np.mean(x[<span class="hljs-number">-1</span>])x[<span class="hljs-number">-2</span>] /= np.mean(x[<span class="hljs-number">-2</span>])<span class="hljs-comment"># 取目标值y</span>y = data[:, <span class="hljs-number">-1</span>]<span class="hljs-comment"># 划分训练集与验证集</span>x_train, x_val = x[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>, :], x[<span class="hljs-number">3000</span>:, :]y_train, y_val = y[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y[<span class="hljs-number">3000</span>:]</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练模型，更新参数。</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(x_train, y_train, epoch)</span>:</span>    num = x_train.shape[<span class="hljs-number">0</span>]    dim = x_train.shape[<span class="hljs-number">1</span>]    bias = <span class="hljs-number">0</span>    weights = np.ones(dim)    learning_rate = <span class="hljs-number">1</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 偏置梯度平方和</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 权重梯度平方和</span>    weight_sum = np.zeros(dim)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        b_g = <span class="hljs-number">0</span>        w_g = np.zeros(dim)                <span class="hljs-comment"># 在所有数据上计算梯度，梯度计算时针对损失函数求导</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):            y_pre = weights.dot(x_train[j, :]) + bias            sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))            b_g += (<span class="hljs-number">-1</span>) * (y_train[j] - sig)            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(dim):                w_g[k] += (<span class="hljs-number">-1</span>) * (y_train[j] - sig) * x_train[j, k] + <span class="hljs-number">2</span> * reg_rate * weights[k]        b_g /= num        w_g /= num        <span class="hljs-comment"># adagrad</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>         <span class="hljs-comment"># 更新权重和偏置</span>        bias -= learning_rate / bias_sum ** <span class="hljs-number">0.5</span> * b_g        weights -= learning_rate / weight_sum ** <span class="hljs-number">0.5</span> * w_g        <span class="hljs-comment"># 每训练100轮，输出一次在训练集上的正确率。</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            acc = <span class="hljs-number">0</span>            result = np.zeros(num)            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):                y_pre = weights.dot(x_train[j, :]) + bias                sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))                <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:                    result[j] = <span class="hljs-number">1</span>                <span class="hljs-keyword">else</span>:                    result[j] = <span class="hljs-number">0</span>                <span class="hljs-keyword">if</span> result[j] == y_train[j]:                    acc += <span class="hljs-number">1.0</span>                loss += (<span class="hljs-number">-1</span>) * (y_train * np.log(sig) + (<span class="hljs-number">1</span> - y_train[j]) * np.log(<span class="hljs-number">1</span> - sig))            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为'</span>.format(i), loss / num)            print(<span class="hljs-string">'&#123;&#125; 次训练后, 训练集的精度为&#123;:.2f&#125;'</span>.format(i, acc / num))    <span class="hljs-keyword">return</span> weights, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 验证模型效果</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate</span><span class="hljs-params">(x_val, y_val, weights, bias)</span>:</span>    num = <span class="hljs-number">1000</span>    loss = <span class="hljs-number">0</span>    acc = <span class="hljs-number">0</span>    result = np.zeros(num)    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):        y_pre = weights.dot(x_val[j, :]) + bias        sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))        <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:            result[j] = <span class="hljs-number">1</span>        <span class="hljs-keyword">else</span>:            result[j] = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> result[j] == y_val[j]:            acc += <span class="hljs-number">1.0</span>        loss += (<span class="hljs-number">-1</span>) * (y_val * np.log(sig) + (<span class="hljs-number">1</span> - y_val[j]) * np.log(<span class="hljs-number">1</span> - sig))    <span class="hljs-keyword">return</span> loss / num, acc / num</code></pre><h3 id="3-4-测试模型"><a href="#3-4-测试模型" class="headerlink" title="3.4 测试模型"></a>3.4 测试模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练轮数</span>epoch = <span class="hljs-number">4000</span><span class="hljs-comment"># 开始训练</span>w, b = train(x_train, y_train, epoch)<span class="hljs-comment"># 在验证集上看效果</span>loss, acc = validate(x_val, y_val, w, b)print(<span class="hljs-string">'验证集的损失值为:'</span>, loss)print(<span class="hljs-string">'验证集的精度值为:'</span>, acc)</code></pre><h3 id="3-5-实验结果"><a href="#3-5-实验结果" class="headerlink" title="3.5 实验结果"></a>3.5 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为 [<span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span>  <span class="hljs-number">1.50623089</span> ... <span class="hljs-number">1.50623089</span> <span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span> ]<span class="hljs-number">0</span> 次训练后, 训练集的精度为<span class="hljs-number">0.62</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.90</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">2000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.86</span> <span class="hljs-number">2400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span> <span class="hljs-number">2800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span>验证集的损失值为: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]验证集的精度值为: <span class="hljs-number">0.864</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率基本能达到90%，验证集分类的正确率能达到86.4%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW1</title>
    <link href="/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/"/>
    <url>/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）"><a href="#李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）" class="headerlink" title="李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）"></a>李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）</h1><p>本文主要参考了博文<a href="https://blog.csdn.net/m123_45n/article/details/106560274" target="_blank" rel="noopener">https://blog.csdn.net/m123_45n/article/details/106560274</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是利用Liner Regression去预测丰原市PM2.5的数值。对丰原市一年的观测记录分为了train set和test set，其中train set 是丰原市每个月前20天的所有记录。test set则是从丰原市剩下的记录取样出来。train set和test set都是以csv格式的文件进行保存的。</p><ul><li>train.csv：每个月前20天的完整数据。</li><li>test.csv：从剩下的数据中取样出连续的10小时为一组，前九个小时的所有观测数据当作==特征值==，第十个小时的PM2.5当作==目标值==。共取出240组不重复的test data，然后根据feature去预测出第十小时的PM2.5的值。</li><li>Data（中含有18项观测数据）：AMB_TEMP，CH4，CO，NHMC，NO，NO2，NOx，O3，PM10，PM2.5，RAINFALL，RH，SO2，THC，WD_HR，WIND_DIREC，WIND_SPEED，WS_HR。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><p>本次任务选择的模型为线性回归模型：</p><script type="math/tex; mode=display">y=\sum\limits_{i=0}^8w_ix_i+b。</script><ul><li><p>$i$从0到8是因为选取前9个小时作为==特征值==输入，每个输入都有一个==权重值==$w$与之相乘，再加上一个偏置$b$，即为线性回归模型。通过这个模型去预测第十个小时的PM2.5的值。</p></li><li><p>此外，可将该模型的运算转换为向量运算：</p><script type="math/tex; mode=display">  \begin{align}  y=  \begin{bmatrix}  w_0\ \dots\ w_8   \end{bmatrix}  \begin{bmatrix}  x_0\newline \vdots\newline x_8   \end{bmatrix}  +b\end{align}</script></li><li><p>选择的<strong>Loss Function</strong>为：</p></li></ul><script type="math/tex; mode=display">Loss=\frac{1}{2m}\sum\limits_{i=0}^{m-1}(\hat{y}^i-y^i)^2+\frac12\lambda\sum\limits_{j=0}^8w_j^2</script><ul><li><p>Loss对$w_j$求导为：</p><script type="math/tex; mode=display">  \frac{\partial L}{\partial w_j}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-x_j)+\lambda w_j</script></li><li></li></ul><script type="math/tex; mode=display">\frac{\partial L}{\partial b}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-1)</script><ul><li><p>则参数更新为：</p><script type="math/tex; mode=display">  \begin{align}  w_j:=w_j-\eta \frac{\partial L}{\partial w_j} \newline  b:=b-\eta \frac{\partial L}{\partial b}  \end{align}</script><p>  其中将$w_j$的更新转换为向量运算：</p><script type="math/tex; mode=display">  \begin{bmatrix}  w_0 \newline \vdots \newline w_8   \end{bmatrix}  :=  \begin{bmatrix}  w_0 \newline \vdots \newline w_8   \end{bmatrix}  -\eta  \begin{bmatrix}  \frac{\partial L}{\partial w_0} \newline \vdots \newline \frac{\partial L}{\partial w_8}   \end{bmatrix}</script></li><li><p>Optimizer的选择：Adagrad。更新方式为：</p></li></ul><script type="math/tex; mode=display">\begin{align}w^1=&w^0-\frac{\eta^0}{\sigma^0}g^0 \newline \vdots \newlinew^{t+1}=&w^t-\frac{\eta^t}{\sigma^t}g^t\end{align}</script><p>​                其中$g^t$为$w^t$的梯度值，而且有：</p><script type="math/tex; mode=display">\begin{align}\sigma^t=&\sqrt{\frac{1}{t+1}\sum\limits_{i=0}^t(g^i)^2} \newline\eta^t=&\frac{\eta}{t+1}\end{align}</script><p>​                带入后，则有：</p><script type="math/tex; mode=display">w^{t+1}=w^t-\frac{\eta}{\sqrt{\sum\limits_{i=0}^t(g^i)^2} }</script><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率能达到90%，验证集分类的正确率能达到87.2%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, header=<span class="hljs-number">0</span>)<span class="hljs-comment"># 删除无关特征</span>data.drop([<span class="hljs-string">"日期"</span>,<span class="hljs-string">"測站"</span>,<span class="hljs-string">"測項"</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataProcess</span><span class="hljs-params">(data)</span>:</span>    <span class="hljs-string">''' 数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数：</span><span class="hljs-string">        data: 原始训练集数据</span><span class="hljs-string">         </span><span class="hljs-string">    返回：</span><span class="hljs-string">        x: 一天中PM2.5前9项特征构成的训练集。</span><span class="hljs-string">        y: 一天中PM2.5第10项特征构成的目标集。</span><span class="hljs-string">        data: 将原始数据集的每个数据值均转换为float类型后的数据集。</span><span class="hljs-string">    '''</span>    x_list, y_list = [], []    data = data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    <span class="hljs-comment"># 将所有数据转换成float类型</span>    data = np.array(data).astype(float)    <span class="hljs-comment"># 将数据集拆分为多个数据帧</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>): <span class="hljs-comment"># 18 * 240 = 4320</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">15</span>): <span class="hljs-comment"># 24-9=15(组)</span>            <span class="hljs-comment"># 截取PM2.5前9项作为训练数据</span>            sample = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j:j+<span class="hljs-number">9</span>]            <span class="hljs-comment"># 截取PM2.5第10项作为目标数据</span>            label = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j+<span class="hljs-number">9</span>]            x_list.append(sample)            y_list.append(label)    x = np.array(x_list)    y = np.array(y_list)    <span class="hljs-keyword">return</span> x, y, data</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainModel</span><span class="hljs-params">(x_data, y_data, epoch)</span>:</span>    <span class="hljs-string">''' 训练模型，从训练集中拿出3000个数据用来训练，剩余600个数据用于验证。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_data: </span><span class="hljs-string">        y_data:</span><span class="hljs-string">        epoch:</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        weight: 权重</span><span class="hljs-string">        bias：偏置</span><span class="hljs-string">        </span><span class="hljs-string">    '''</span>    <span class="hljs-comment"># 初始化偏置</span>    bias = <span class="hljs-number">0</span>    <span class="hljs-comment"># 初始化权重,生成一个9列的行向量，并全部初始化为1。</span>    weight = np.ones(<span class="hljs-number">9</span>)    <span class="hljs-comment"># 初始化学习率为1。</span>    learning_rate = <span class="hljs-number">1</span>    <span class="hljs-comment"># 初始化正则项系数为0.001。</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 用于存放偏置的梯度平方和。</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 用于存放权重的梯度平方和。</span>    weight_sum = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        <span class="hljs-comment"># 偏置梯度平均值。</span>        b_g = <span class="hljs-number">0</span>        <span class="hljs-comment"># 权重梯度平均值</span>        w_g = np.zeros(<span class="hljs-number">9</span>)        <span class="hljs-comment"># 在所有数据上计算w和b的梯度。</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):            b_g += (y_data[j] - weight.dot(x_data[j]) - bias) * (<span class="hljs-number">-1</span>) <span class="hljs-comment"># 如果2个一维向量dot，则结果为它们的内积。</span>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                w_g[k] += (y_data[j] - weight.dot(x_data[j]) - bias) * (-x_data[j, k])        <span class="hljs-comment"># 求平均值</span>        b_g /= <span class="hljs-number">3000</span>        w_g /= <span class="hljs-number">3000</span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):            w_g[k] += reg_rate * weight[k]                <span class="hljs-comment"># adagrad优化</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>        bias -= learning_rate / (bias_sum ** <span class="hljs-number">0.5</span>) * b_g        weight -= learning_rate / (weight_sum ** <span class="hljs-number">0.5</span>) * w_g        <span class="hljs-comment"># 每训练200次输出一次误差</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):                loss += (y_data[j] - weight.dot(x_data[j]) - bias) ** <span class="hljs-number">2</span>            loss /= <span class="hljs-number">3000</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                loss += reg_rate * (weight[j] ** <span class="hljs-number">2</span>)            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为&#123;:.2f&#125;'</span>.format(i, loss / <span class="hljs-number">2</span>))        <span class="hljs-keyword">return</span> weight, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validateModel</span><span class="hljs-params">(x_val, y_val, weight, bias)</span>:</span>    <span class="hljs-string">''' 验证模型，返回损失值。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_val: </span><span class="hljs-string">        y_val: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        loss: 平均损失值。</span><span class="hljs-string">    '''</span>    loss = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">600</span>):        loss += (y_val[i] - weight.dot(x_val[i]) - bias) ** <span class="hljs-number">2</span>        <span class="hljs-keyword">return</span> loss / <span class="hljs-number">600</span></code></pre><h3 id="3-4-测试数据预处理"><a href="#3-4-测试数据预处理" class="headerlink" title="3.4 测试数据预处理"></a>3.4 测试数据预处理</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testDataProcess</span><span class="hljs-params">(test_data)</span>:</span>    <span class="hljs-string">''' 测试数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        test_data: 测试数据集。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        testList: 测试数据集。</span><span class="hljs-string">    '''</span>    testList = []    test_data = test_data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    test_data = np.array(test_data).astype(float)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>):        testList.append(test_data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>])    <span class="hljs-keyword">return</span> np.array(testList)</code></pre><h3 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 训练 '''</span>    <span class="hljs-comment"># 读取训练数据，并将其转换为列表形式。</span>    data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, usecols=np.arange(<span class="hljs-number">3</span>, <span class="hljs-number">27</span>).tolist())    x_data, y_data, data = dataProcess(data)        x_train, y_train = x_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>]    x_val, y_val = x_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>], y_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>]    weight, bias = trainModel(x_train, y_train, <span class="hljs-number">2000</span>)    savePre(weight, bias)    print(<span class="hljs-string">"训练得到的模型的weight为&#123;&#125;"</span>.format(weight))    print(<span class="hljs-string">"训练得到的模型的bias为&#123;&#125;"</span>.format(bias))    loss = validateModel(x_val, y_val, weight, bias)    print(<span class="hljs-string">"模型模型在验证集的loss为&#123;:.2f&#125;"</span>.format(loss))</code></pre><h3 id="3-6-测试模型"><a href="#3-6-测试模型" class="headerlink" title="3.6 测试模型"></a>3.6 测试模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testModel</span><span class="hljs-params">(x_test, weight, bias)</span>:</span>    <span class="hljs-string">''' 用测试数据集测试模型，并将结果保存到output.csv中</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        x_test: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    '''</span>    f = open(<span class="hljs-string">"output.csv"</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>, newline=<span class="hljs-string">""</span>)    csv_write = csv.writer(f)    csv_write.writerow([<span class="hljs-string">"id"</span>, <span class="hljs-string">"value"</span>])    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x_test)):        output = weight.dot(x_test[i]) + bias        csv_write.writerow([<span class="hljs-string">"id_"</span> + str(i), str(output)])    f.close()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 测试 '''</span>    <span class="hljs-comment"># 读取测试数据集</span>    pre = pd.read_csv(<span class="hljs-string">"pre.csv"</span>)    preList = list(pre.replace(<span class="hljs-string">","</span>, <span class="hljs-string">" "</span>))    weight = []    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list(preList[<span class="hljs-number">0</span>].split(<span class="hljs-string">","</span>)):        weight.append(float(i))    bias = float(preList[<span class="hljs-number">1</span>])    test_data = pd.read_csv(<span class="hljs-string">"test.csv"</span>,header=<span class="hljs-literal">None</span>,usecols=np.arange(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>).tolist())    x_test = testDataProcess(test_data)    print(x_test.shape)    print(x_test)    testModel(x_test, np.array(weight), bias)</code></pre><h3 id="3-7-实验结果"><a href="#3-7-实验结果" class="headerlink" title="3.7 实验结果"></a>3.7 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为<span class="hljs-number">477.36</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为<span class="hljs-number">25.08</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为<span class="hljs-number">23.30</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.67</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.37</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为<span class="hljs-number">22.22</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为<span class="hljs-number">22.14</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为<span class="hljs-number">22.10</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.08</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.07</span>训练得到的模型的weight为[ <span class="hljs-number">0.00194352</span> <span class="hljs-number">-0.02562139</span>  <span class="hljs-number">0.18138721</span> <span class="hljs-number">-0.19572767</span> <span class="hljs-number">-0.0230436</span>   <span class="hljs-number">0.40959797</span> <span class="hljs-number">-0.51985702</span>  <span class="hljs-number">0.06835844</span>  <span class="hljs-number">1.03413381</span>]训练得到的模型的bias为<span class="hljs-number">1.948721373892966</span>模型模型在验证集的loss为<span class="hljs-number">39.17</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">在这个模型当中，可以看到在经过2000次训练后，模型在验证集上的损失已达到了39.17。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后600项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2015-CLA-NextCloures Parallel Computation of the Canonical Base</title>
    <link href="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/"/>
    <url>/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/</url>
    
    <content type="html"><![CDATA[<h1 id="NextClosures-Parallel-Computation-of-the-Canonical-Base"><a href="#NextClosures-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="NextClosures: Parallel Computation of the Canonical Base"></a>NextClosures: Parallel Computation of the Canonical Base</h1><div class="note note-primary">            <p>属性探索、并行算法、NextCloures</p>          </div> <a id="more"></a><!-- <p class='note note-info'>论文、属性探索</p> --><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel 、Daniel Borchmann。</p><p>​            2. 期刊：International Journal of General Systems。</p><p>​            3. 会议：CLA。</p><p>​            4. 时间：2015。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>形式背景的规范基在形式概念分析中起着举足轻重的作用。</li><li>目前所提的计算主基的算法都是线性序的，即一次只计算一个伪内涵。</li><li>本文介绍一种允许以并行方式计算主基的方法。</li><li>实验表明，对于足够大的数据集，加速比与CPU的数量成正比。</li></ul><p>词汇积累：</p><ul><li>canonical base: 主基，正则基，规范基。</li><li>remedies: 救济方法，弥补手段。</li><li>deficit: 缺陷。</li><li>is proportional to: 与之成正比。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>有两种已知的线性序的算法可以计算形式背景的主基$^{[6, 12]}$，即一次只计算一个蕴涵式。这两种算法还计算了给定形式背景的所有形式概念。</p></li><li><p>目前为止，还不知道能否在多项式时间内计算主基。</p></li><li><p>实际上，对于以字典序来计算伪内涵的算法，都表明了它无法在多项式时间内计算出主基。</p></li><li><p>已有并行式计算给定形式背景的概念格的方法$^{[5, 13]}$。</p></li><li><p>本文想探索出一种并行式计算主基的算法，该思想已经被Lindg[11]用来顺序计算给定形式背景的概念格。</p><p> 思想：为了计算主基，则需要计算出形式背景的所有内涵和伪内涵的概念格。这个格需要按级别顺序自下而上的计算，并且基于该格特定层上有一定的“宽度”在该计算可以并行完成。</p><p> 现在有一个事实：对于计算内涵/伪内涵$B$的上邻域，则只需要迭代所有$m\notin B$的属性并计算出$B\cup \{m\}$的闭包。</p><p> 而在lindg中计算该闭包的方法为：$B\rightarrow B^{II}$。</p><p> 而且实验结果表明，对于合适的大的形式背景，主基的计算速度与CPU的数量成正比。</p></li></ol><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>1.A model of $X\rightarrow Y$ is a set $T\subseteq M\Leftrightarrow X\subseteq T\Rightarrow Y\subseteq T$.</p><p>2.A model of $L$ is a model of all implications in $L$, and $X^L$ is the superset of $X$ that is a model of $L$.</p><p>3.$X^L$的计算方式如下：</p><script type="math/tex; mode=display">\begin{align}X^L:=\bigcup_{n\geq 1}X^{L_n}\ where\ X^{L_1}:&=X\cup \bigcup \{B\mid A\rightarrow B\in L\ and\ A\subseteq X\} \newlineand\ X^{L_{n+1} }:&=(X^{L_n})^{L_1} for\ all\ n\in N\end{align}</script><p>当且仅当$X，X\subseteq M$是$\mathbb{K}^<em>$闭包时，很容易判定$X$是形式背景$K$的内涵/伪内涵。$\mathbb{K}^</em>$闭包定义如下：</p><script type="math/tex; mode=display">\begin{align}X^{\mathbb{K}^*}:=\bigcup_{n\geq 1}X^{\mathbb{K}_n^*}\ where\ X^{\mathbb{K}_1^*}:&=X\cup\bigcup \{P^{II}\mid P\in PsInt(\mathbb{K})\ and\ P\subsetneq X\} \newlineand\ X^{\mathbb{K}_{n+1}^*}:&=(X^{\mathbb{K}_n^*})^{\mathbb{K}_1^*} for\ all\ n\in N\end{align}</script><p>如果$L$是形式背景$\mathbb{K}$的主基，则$L^<em>$与$\mathbb{K}^</em>$重合。$L^*$闭包定义如下：</p><script type="math/tex; mode=display">\begin{align}X^{L^*}:=\bigcup_{n\geq 1}X^{L_n^*}\ where\ X^{L_1^*}:&=X\cup \bigcup \{B\mid A\rightarrow B\in L\ and\ A\subsetneq X\} \newlineand\ X^{L_{n+1}^*}:&=(X^{L_n^*})^{L_1^*} for\ all\ n\in N\end{align}</script><p>4.内涵：$B=B^{II}$。</p><p>伪内涵：$P\neq P^{II}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{II}\subseteq P$。将形式背景$K$上的所有伪内涵集合记为$PsInt(K)$。</p><p>蕴涵式：$\{P\rightarrow P^{II}\mid P\in PsInt(K)\}$。</p><h2 id="3-Parallel-Computation-of-the-Canonical-Base"><a href="#3-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="3 Parallel Computation of the Canonical Base"></a>3 Parallel Computation of the Canonical Base</h2><p>1.$NextCloures$算法由Ganter$^{[6]}$提出，用于枚举主基。</p><ul><li>算法思想：以一定的线性顺序(即字典序)计算形式背景K的所有内涵和伪内涵。</li><li>优势：下一个(伪)意图是唯一确定的，但我们可能需要回溯才能找到它。</li><li>该算法本质上是顺序的，即不可能将其并行化。</li></ul><p>2.我们的方法按照子集序枚举所有的内涵和伪内涵。</p><ul><li>易于并行化枚举。</li><li>多线程实现中，不同线程之间不需要通信。</li></ul><p>原因：检测$P$是否为伪内涵时，只需判断$P$的伪内涵子集$Q\subsetneq P$，是否满足满足$Q^{II}\subseteq P$。即可按基数递增来判断伪内涵。</p><p>算法思想的基本工作流程：</p><p>​            1. 判断$\varnothing=\varnothing^{II}\Rightarrow \varnothing$是内涵/伪内涵。</p><ol><li>假设基数$&lt;k$的所有伪内涵均已确定，然后就可正确判断出属性集$P，P\subseteq M，|P|=k$是内涵/伪内涵。</li><li>设定一个候选集合储存当前层次的内涵/伪内涵，而无论何时才找到伪内涵$P$，它的$\subseteq-next\ cloure$都是由$P^{II}$唯一确定的。</li><li>如果$B$是一个内涵则它的$\subseteq-next\ cloure$为$(B\cup \{m\})^{\mathbb{K}^*},\ m\notin B$。</li></ol><p>具体的工作流程：</p><p>基本定义：</p><ul><li>$K$：有限的形式背景。</li><li>$k$：当前候选集的基数。候选集为储存当前层级的内涵/伪内涵。</li><li>$C$：候选集合。</li><li>$\mathfrak{B}$：形式概念集合。</li><li>$L$：蕴涵式集合。</li></ul><p>具体流程：</p><p>1.$k:=0，C:=\{\varnothing\}，\mathfrak{B}:=\varnothing，L:=\varnothing$.</p><p>2.并行化：对于每个基数$|C|=k$的候选集合C $\in C$，确定它是否是$L^<em>-closed$。如果不是，则将它的$L^</em>-closure$加入候选集合$C$，跳到5。如果是$L^*-closed$，则跳到3。</p><p>3.如果C是形式背景$K$的内涵，那么将形式概念$(C^I，C)$加入集合$\mathfrak{B}$中。否则C必为伪内涵，则将蕴涵式$C\rightarrow C^{II}$加入集合$L$，并且将形式概念$(C^I，C^{II})$加入集合$\mathfrak{B}$中。</p><p>4.对得到的每一个内涵$C^{II}$，将它的上层邻居$C^{II}\bigcup \{m\}且m\notin C^{II}$加入候选集合$C$中。</p><p>5.等待基数为$k$的所有候选集合都处理完毕，如果$k&lt;M$则递增$k$，并且跳到2；否则算法结束，返回形式概念集合$\mathfrak{B}$和蕴涵式集合$L$。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335.jpg" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335-1593786388801.jpg" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li>backtracking：回溯。</li><li>level-wise：逐级。</li><li>coincide：一致。</li><li>inductive：归纳的；感应的，诱导的。</li><li>suffices：满足。</li></ul><h2 id="4-Benchmarks"><a href="#4-Benchmarks" class="headerlink" title="4 Benchmarks"></a>4 Benchmarks</h2><p style="text-indent:2em">本节主要目的在于将我们并行式的算法与线性序的NextCloures算法在计算主基时进行定性与定量的比较分析。 </p><p style="text-indent:2em">结果：并行式算法在一定的极限下，算法的运行时间与可用CPU的数量成正比的减少。 </p><p>​        特点：同一层的候选集不能彼此影响，即线程之间不需要通信。</p><p>​        异常情况：在某些情况下，当使用所有可用的CPU时，计算时间会增加。（原因未知）—-可能是由于平台或操作系统的一些技术细节，比如基准测试过程中执行的一些后台任务，或者线程维护带来的开销。</p><p>数据集：FCA Data Repository$^{[4]}$。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003519115.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003519115-1595918426558.png" srcset="/img/loading.gif" alt></p><p>基准测试结果：计算主基所用的时间基本与CPU数量成线性负相关。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003803863.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003803863-1595918434190.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003944602.png" srcset="/img/loading.gif" class><p>仅一个CPU的时候，本文方法与原方法所用时间基本相同，但当增加CPU的数量时，本文方法明显优于原方法，大致减少了$\frac13$到3比例的时间。</p><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003944602-1595918438926.png" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li>benchmarks：基准测试。</li><li>overhead caused：导致的开销。</li></ul><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p style="text-indent:2em">本文介绍了计算正则基的并行式算法NextCloures。该算法以层次化的顺序（即增加基数），自下而上的构造给定形式背景的所有内涵和伪内涵的概念格。</p><p style="text-indent:2em">由于概念格中某一层的元素可以独立计算，也可以并行枚举，从而产生了计算正则基的并行算法。</p><p>可能的扩展：</p><ul><li>处理一组蕴涵式或约束闭包算子给出的具有背景知识的形式背景。</li><li>属性探索：包含专家交互，以探索部分已知形式背景的规范基。<ul><li>可以让几位专家同时回答问题。</li><li>由于前提基数的增加即问题的难度与经典属性探索线性序提出的问题相比不断增加。</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>并行算法</tag>
      
      <tag>NextCloures</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学-并行计算学科发展历程</title>
    <link href="/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/"/>
    <url>/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="并行计算学科发展历程"><a href="#并行计算学科发展历程" class="headerlink" title="并行计算学科发展历程"></a>并行计算学科发展历程</h1><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：陈国良、张玉杰</p><p>​            2.期刊：计算机科学</p><p>​            3.时间：2020/06/11</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>回顾在并行计算学科发展所做的工作。</li><li>对非数值计算的计算方法进行介绍。</li><li>新型非冯诺依曼结构计算机体系结构的介绍。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>并行计算课程发展的5个阶段：</p><p>非数值计算的并行算法$\rightarrow$新型非冯诺依曼计算机结构$\rightarrow$改革计算机基础课程的计算思维$\rightarrow$数据科学$\rightarrow$大数据计算理论研究。</p><p>完整的并行算法学科体系：算法理论-算法设计-算法实现-算法应用。</p><p>一体化的并行计算研究方法：并行机结构-并行算法-并行编程。</p><h2 id="2-非数值计算中的计算方法"><a href="#2-非数值计算中的计算方法" class="headerlink" title="2 非数值计算中的计算方法"></a>2 非数值计算中的计算方法</h2><p>从计算科学角度数值计算内容主要有：</p><ul><li>矩阵运算。</li><li>线性方程组的求解。</li><li>快速傅里叶变换等。</li></ul><p>非数值计算中的并行算法基本设计策略包括：</p><ul><li>串行算法的直接并行化。</li><li>从问题描述开始设计全新的并行算法。</li><li>借用已有的算法。</li><li>利用已求解问题与待求解问题两者之间的内在相似性来求解新问题。</li></ul><h2 id="3-新型非冯诺依曼计算机体系结构"><a href="#3-新型非冯诺依曼计算机体系结构" class="headerlink" title="3 新型非冯诺依曼计算机体系结构"></a>3 新型非冯诺依曼计算机体系结构</h2><p style="text-indent:2em">传统的冯诺依曼体系结构：第一代计算机（电子管计算机）、第二代计算机（晶体管计算机）、第三代计算机（集成电路计算机）、第四代计算机（大规模超大规模集成电路）</p><p>一些先进新型计算机系统结构：</p><ul><li>微程序控制器设计：<ul><li>设计了“八位运控模型”，并采用了自行提出的“寄存器传输操作语言”进行形式化描述。</li></ul></li><li>直接执行高级语言的计算机：<ul><li>研究了“直接执行的高级语言FORTRAN”机      器，介绍了对标准FORTRAN语言所作的一些限制和补充，简述了该计算机体系结构，列举典型的FORTEAN语言的直接执行过程，并自行提出了可重组结构与之配合。</li></ul></li><li>数据库计算机：<ul><li>研究了数据库计算机，实现RDF查询语言和SQL语言的转换并在此基础上实现一个对用户透明的、建立在关系数据库之上的RDF搜索引擎，以提高其海量存储和查找效率。</li></ul></li><li>光计算机：<ul><li>通过垂直偏振光、水平偏振光和无强光3个稳定的光状态表示信息的三值光计算机原理$^{[7]}$，提出基于光原理三值逻辑计算机。</li></ul></li><li>生物计算机：<ul><li>研究了基于字符串匹配原理的生物序列比对的生物计算机，在纳米计算模型上实现了DNA序列模体发现算法$^{[8]}$。</li></ul></li><li>可重构可变计算机：<ul><li>研究了“可变结构计算机系统”及其结构中的资源间相互通信问题$^{[9]}$。</li></ul></li><li>数据流计算机：<ul><li>根据MIT提出的数据流计算机概念，分析了曼彻斯特大学的数据流计算机，在国内分布式计算计算会议上发表了“数据流计算机体系结构解析”$^{[10]}$一文。</li></ul></li><li>神经计算机：<ul><li>研究神经网络在组合优化中的应用的同时，自行构建了基于Transputer阵列的“通用并行神经网络模拟系统（GP2N2S2）”$^{[10]}$，提供了高级神经网络描述语言及其编辑和编译器的执行环境，实现了程序的自动化执行。</li></ul></li><li>Transpute阵列机：<ul><li>搭建了Transputer阵列机，在中科院计算所进行了组装、调试和运行，并在其上实现了通用的Rohoman应用平台。</li></ul></li><li>量子计算机：<ul><li>在中国科大开展了量子计算研究，讨论量子计算机模型及其物理实现方案、量子计算过程、量子计算模型和量子并行算法，分析量子指数级存储容量和指数加速特征等，并在保密通信、密码安全等领域对量子信息技术进行研究$^{[12, 13]}$。</li></ul></li></ul><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p style="text-indent:2em">大数据、物联网、云计算和区块链是新一代信息技术发展中的华彩乐章。物联网使成千上万的网络传感器嵌入到现实世界中，云计算为物联网产生的海量数据提供了存储空间和在线处理模式，而大数据则让海量数据产生了价值，区块链促进海量信息可靠交互保障生产要素在区域内有序高效地流通。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2018-CS-A General Form of Attribute Exploration </title>
    <link href="/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/"/>
    <url>/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/</url>
    
    <content type="html"><![CDATA[<!-- * @Author: peerless * @Date: 2020-06-28 18:25:20 * @LastEditTime: 2020-06-30 21:27:52 * @LastEditors: Please set LastEditors * @Description: A General Form of Attribute Exploration * @FilePath: \peer-less.github.io\source\_posts\2012CS-A General Form of Attribute Exploration .md   title:         A General Form of Attribute Exploration # 标题   subtitle:                                              # 副标题   date:          2020-06-30                              # 时间   author         peerless                                # 作者   heaeder-img:   img/post-bg-.jpg                        # 这篇文章标题背景图片   catalog:       true                                    # 是否归档   tags:                                                  # 标签      - 学术      - 属性探索         --> <h1 id="A-General-Form-of-Attribute-Exploration"><a href="#A-General-Form-of-Attribute-Exploration" class="headerlink" title="A General Form of Attribute Exploration"></a>A General Form of Attribute Exploration</h1><h2 id="行文思路简要总结"><a href="#行文思路简要总结" class="headerlink" title="行文思路简要总结"></a>行文思路简要总结</h2><p>问题：如何从经典属性探索出发获得通用的属性探索？</p><p>​            1.回顾经典属性探索算法。</p><p>​            2.通过引入3个条件扩展属性探索算法：</p><ul><li>引入两个闭包算子$c_{cert}(A)$与$c_{univ}(A)$。</li><li>不明确指定提供的反例。</li><li>只向专家询问满足$c_{cert}(A)\subsetneq B\subset c_{univ}(A)$的蕴涵式$A\rightarrow B$。</li></ul><p>​            3.对通用属性探索算法进行非冗余性与完备性验证。</p><p>​            4.对通用算法询问蕴涵式时进行改进，使算法向专家询问的蕴涵式的数量是最小的。</p><ul><li>改进方法：若属性集$A=c_{cert}(A)\subsetneq c_{univ}(A)$，则向专家询问蕴涵式$A\rightarrow c_{univ}(A)$。</li></ul><p>总结：</p><p>​            1. 该通用属性算法首先保留了经典属性探索算法的大多性质：完备性、非冗余性和向专家询问蕴涵式的数量最少等。</p><p>​            2. 该通用属性探索算法能够处理抽象给定的闭包算子和部分形式背景下给出的反例。</p><p>问题：</p><ol><li>理论层面：该通用属性探索算法理论上的时间复杂度仍为指数级，无法在多项式时间内计算主基。</li><li>应用层面：该通用属性探索算法并未说明在实际的应用领域中效果如何。</li></ol><h2 id="论文基本信息："><a href="#论文基本信息：" class="headerlink" title="论文基本信息："></a>论文基本信息：</h2><p>​            1.作者：Daniel Borchmann（德累斯顿州立大学数学与科学学院代数研究所丹尼尔·博尔赫曼）</p><p>​            2.期刊：Computer Science</p><p>​            3.时间：2018/11/15</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>提出一种属性探索的一般形式。</li><li>扩展属性探索的适用性。</li><li>将属性探索的现有变种转换为一般形式，简化理论。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>1.属性探索的变种：</p><ul><li>部分形式背景的属性探索$^{[3]}$。</li><li>描述逻辑模型的探索$^{[1, 2]}$。</li></ul><font color="red">研究问题：寻找一种将所有变种都包含在内的一般属性探索。</font><p>可行原因：</p><pre><code>     1.属性探索算法的整体结构均保持不变。     2.属性探索的所有重要属性均保留了下来。</code></pre><p>单词积累:</p><pre><code>     discourse: 论述、谈话、演讲。</code></pre><p>​         have a close look: 仔细研究。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>蕴涵式</p><script type="math/tex; mode=display">A\rightarrow B \Leftrightarrow A^{'}\subseteq B^{'} \Leftrightarrow B\subseteq A^{''}</script><p>一些声明：</p><ul><li><p>$Imp(M)$：$M$上的所有蕴涵式集合。</p></li><li><p>$Imp(K)$：形式背景$K$上的所有蕴涵式集合。</p></li><li><p>$Th(K)$：形式背景$K$上的所有成立的蕴涵式集合。</p></li><li><p>$L\subseteq Imp(K)$，$A\subseteq M$。如果对于所有的蕴涵式$(X\rightarrow Y)\in L$，都有$X\subsetneq A或Y\subseteq A$成立，则集合$A$在$L$下为封闭集合。则可做如下定义：</p><script type="math/tex; mode=display">  \begin{align}  L^0(A):&=A \newline  L^1(A):&=\bigcup{ \{Y\mid (X\rightarrow Y)\in L, X\subseteq A\} } \newline  L^i(A):&=L^1(L^{i-1}(A))\ for\ i > 1 \newline  L(A):&=\bigcup\limits_{i\in N}{L^i(A)}  \end{align}</script><p>  $L(A)$是基于$L$下比集合A大的最小集合。</p><p>  由上可知$L^N(A)$表示集合$A$的子集所能推出来的属性集合的子集所能推出来的属性集合。</p><p>  $Cn(L)$：在$L$下成立的所有蕴涵式集合。</p><ul><li>$B\subseteq Imp(K)$在$L$下是非冗余的。$\Leftrightarrow B\subseteq Cn(L)$。</li><li>$B\subseteq Imp(K)$在$L$下是完备的。$\Leftrightarrow Cn(B)\supseteq L$。</li><li><p>$B$是主基。$\Leftrightarrow Cn(B)=Cn(L)$。</p><p>$P$在$L$下是伪闭集，如果以下条件成立：</p></li><li><p>$P\neq L(P)$。</p></li><li><p>对于所有的伪闭集$Q\subsetneq P$有$L(Q)\subseteq P$成立。</p><p>特别的，若$L=Th(K)$，则$P$称为形式背景$K$的伪内涵。则主基可定义为：$Can(L):=\{P\rightarrow L(P)\mid P 在L下是伪闭集\}$。</p><p>由于不知道对象$g$是否具有属性$m$，因此需要引入部分形式背景：存在一个属性集的有序对$(A, B), A, B\in M且A\bigcap B=\varnothing$所构成的集合即为部分形式背景。</p></li><li><p>若$A\bigcup B=M$，则集合称为全局对象描述，即相应对象明确具有的属性集。</p></li><li>若$A\bigcup B\neq M$，则集合称为部分对象描述，即相应对象明确不具有的属性集。</li></ul></li></ul><h2 id="3-Classical-Attribute-Exploration"><a href="#3-Classical-Attribute-Exploration" class="headerlink" title="3 Classical Attribute Exploration"></a>3 Classical Attribute Exploration</h2><p>$M$是一个有限的属性集合，$K$是基于$M$的形式背景，$p$是基于$M$的领域专家。</p><p>1.对属性集通过字典序进行初始化，并获得第一个属性$\varnothing / P$.</p><p>2.如果$P^{‘}=P$，跳到第5步；否则，令$r:=(P\rightarrow P^{“})$</p><p>3.如果专家认为$r$成立，把$r$加入蕴涵集$Imp(K)$中。</p><p>4.如果专家认为$r$不成立，给出相应的一个反例$C$，将$C$（及其相应的属性）作为新对象加入当前工作形式背景$K$中。</p><p>5.找到字典序$P$的下一个属性集$Q$，若不存在下一个，则算法终止，否则，将$P$设置为$Q$。</p><h2 id="4-Generalizing-Attribute-Exploration"><a href="#4-Generalizing-Attribute-Exploration" class="headerlink" title="4 Generalizing Attribute Exploration"></a>4 Generalizing Attribute Exploration</h2><font color="red">推广目的：用更抽象的术语来描述属性探索，以允许该算法在经典属性探索算法之外的应用。</font><p>3个扩展：</p><p>1.提出两个闭包操作符：$c_{univ}、c_{cert}$。</p><ul><li><p>$c_{univ}$：我们已知的全部领域知识。$c_{univ}(A)$可以从$A$推出的属性集。</p></li><li><p>$c_{cert}$：我们已知的某些知识。$c_{cert}(A)$确定可以从$A$推出的属性集。</p></li></ul><p>2.采用如下方法对算法进行扩展：</p><ul><li><p>提供反例时，不需要完全指定。</p></li><li><p>只需要所提反例所拥有的信息与所给的蕴涵式相矛盾即可。</p></li><li><p>提供关于该对象具有哪些属性以及不具有的属性信息即可。</p></li></ul><p>3.我们向专家提出的蕴涵式是一种特殊的形式：</p><ul><li>搜索关于$c_{cert}$和$c_{univ}$未确定的蕴涵式$A\rightarrow B$，即$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$。对于这样的蕴涵式，我们不能从$c_{cert}$和$c_{univ}$推断出属性集$c_{univ}(A) \backslash B$是否能从$A$推出或不能推出，因此，我们需要向专家询问。</li></ul><p>Algorithm(General Attribute Exploration)</p><p>   $q$为部分领域专家。  </p><p>   ​    1. $K = \varnothing$<br>   ​    2. 对于有限属性集$A\subseteq M$，若存在有限属性集$B$，有$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$成立，则考虑蕴涵式$A\rightarrow B$；如果不存在，则算法终止，输出$K$与$c_{cert}$。</p><p>   ​    3. 若$q$认为$A\rightarrow B$成立，那么更新$c_{cert}^{‘}=X\longmapsto c_{cert}(L(c_{cert}(X) ) )$。</p><p>​        4. 否则，$(C, D)=q(A\rightarrow B)$作为反例，加入形式背景$K$。</p><p>​        5. 对所有的反例$(C, D)$有：</p><script type="math/tex; mode=display">\begin{align}C':&=c_{cert}(C) \newlineD':&=D\bigcup \{ {m\in M\backslash D\mid c_{cert}(C\bigcup \{m\})\bigcap D\neq \varnothing}\}\end{align}</script><p>​        6. 对所有的$X\subseteq M，X\longmapsto c_{univ}(X)\bigcap K(X)$，即将$c_{univ}$更新为$c_{univ}^{‘}(X)=c_{univ}(X)\bigcap K(X)$。</p><p>​        7. 跳转到2。</p><p>两条性质：非冗余性与完备性。</p><h2 id="5-Computing-Undecided-Implications"><a href="#5-Computing-Undecided-Implications" class="headerlink" title="5 Computing Undecided Implications"></a>5 Computing Undecided Implications</h2><p>目的：使向专家询问蕴涵式的数量是最小的。</p><p>原因：该通用属性探索算法对询问蕴涵式的顺序未做约束。（可能询问以确定的蕴涵式）</p><p>经典属性探索下计算未确定的蕴涵式：</p><p>​        在基于已知蕴涵式$k$的条件下，计算属性集$P$字典序之后的最小属性集$Q(Q\subseteq M，且Q不是当前工作形式背景的内涵)$。那么就需要向专家询问蕴涵式$Q\rightarrow Q^{“}$。</p><p>通用属性探索下计算未确定的蕴涵式：</p><p>​        为了保证向专家询问的蕴涵式的数量是最小的，将通用属性探索算法的第二步更改为：$对于有限属性集A\subseteq M$，有$A=c_{cert}(A)\subsetneq c_{univ}(A)$并且$A$是$\subseteq-minimal$成立，则考虑蕴涵式$A\rightarrow c_{univ}(A)$。</p><p>在给出算法总是能得到最小数量的询问蕴涵式前，先给出如下定义：</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h2><p style="text-indent:2em">从使用领域专家的经典的属性探索中推导出一种更加通用的属性探索算法。它能够处理抽象给定的闭包算子，并且可以处理部分给定的反例。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
