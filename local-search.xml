<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW1</title>
    <link href="/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/"/>
    <url>/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）"><a href="#李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）" class="headerlink" title="李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）"></a>李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）</h1><p>本文主要参考了博文<a href="https://blog.csdn.net/m123_45n/article/details/106560274" target="_blank" rel="noopener">https://blog.csdn.net/m123_45n/article/details/106560274</a>，本人对其进行了实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是利用Liner Regression去预测丰原市PM2.5的数值。对丰原市一年的观测记录分为了train set和test set，其中train set 是丰原市每个月前20天的所有记录。test set则是从丰原市剩下的记录取样出来。train set和test set都是以csv格式的文件进行保存的。</p><ul><li>train.csv：每个月前20天的完整数据。</li><li>test.csv：从剩下的数据中取样出连续的10小时为一组，前九个小时的所有观测数据当作==特征值==，第十个小时的PM2.5当作==目标值==。共取出240组不重复的test data，然后根据feature去预测出第十小时的PM2.5的值。</li><li>Data（中含有18项观测数据）：AMB_TEMP，CH4，CO，NHMC，NO，NO2，NOx，O3，PM10，PM2.5，RAINFALL，RH，SO2，THC，WD_HR，WIND_DIREC，WIND_SPEED，WS_HR。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><p>本次任务选择的模型为线性回归模型：<br>$$<br>y=\sum\limits_{i=0}^8w_ix_i+b。<br>$$</p><ul><li><p>$i$从0到8是因为选取前9个小时作为==特征值==输入，每个输入都有一个==权重值==$w$与之相乘，再加上一个偏置$b$，即为线性回归模型。通过这个模型去预测第十个小时的PM2.5的值。</p></li><li><p>此外，可将该模型的运算转换为向量运算：<br>  $$<br>  \begin{align}<br>  y=<br>  \begin{bmatrix}<br>  w_0\ \dots\ w_8<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  x_0\newline \vdots\newline x_8<br>  \end{bmatrix}<br>  +b<br>\end{align}<br>  $$</p></li><li><p>选择的<strong>Loss Function</strong>为：</p></li></ul><p>$$<br>Loss=\frac{1}{2m}\sum\limits_{i=0}^{m-1}(\hat{y}^i-y^i)^2+\frac12\lambda\sum\limits_{j=0}^8w_j^2<br>$$</p><ul><li><p>Loss对$w_j$求导为：<br>  $$<br>  \frac{\partial L}{\partial w_j}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-x_j)+\lambda w_j<br>  $$</p></li><li></li></ul><p>$$<br>\frac{\partial L}{\partial b}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-1)<br>$$</p><ul><li><p>则参数更新为：<br>  $$<br>  \begin{align}<br>  w_j:=w_j-\eta \frac{\partial L}{\partial w_j} \newline<br>  b:=b-\eta \frac{\partial L}{\partial b}<br>  \end{align}<br>  $$<br>  其中将$w_j$的更新转换为向量运算：<br>  $$<br>  \begin{bmatrix}<br>  w_0 \newline \vdots \newline w_8<br>  \end{bmatrix}<br>  :=<br>  \begin{bmatrix}<br>  w_0 \newline \vdots \newline w_8<br>  \end{bmatrix}<br>  -\eta<br>  \begin{bmatrix}<br>  \frac{\partial L}{\partial w_0} \newline \vdots \newline \frac{\partial L}{\partial w_8}<br>  \end{bmatrix}<br>  $$</p></li><li></li></ul><p>$$<br>w^1=w^0-\frac{\eta^0}{\sigma^0}g^0 \<br>\vdots \<br>w^{t+1}=w^t-\frac{\eta^t}{\sigma^t}g^t<br>$$</p><p>​                其中$g^t$为$w^t$的梯度值，而且有：<br>$$<br>\begin{align}<br>\sigma^t=&amp;\sqrt{\frac{1}{t+1}\sum\limits_{i=0}^t(g^i)^2} \newline<br>\eta^t=&amp;\frac{\eta}{t+1}<br>\end{align}<br>$$<br>​                带入后，则有：<br>$$<br>w^{t+1}=w^t-\frac{\eta}{\sqrt{\sum\limits_{i=0}^t(g^i)^2} }<br>$$</p><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>train.csv当中保存的是丰原市一年当中每个月前20天的观测数据，由于每次==输入==的数据为前9个小时的PM2.5的数值，第10个小时的数值用来做==label==，所以一天当中的PM2.5的观测数据可以分为15组（15=24-10+1）。因此，train.csv当中的数据可以分为3600组（3600=12×20×15）。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, header=<span class="hljs-number">0</span>)<span class="hljs-comment"># 删除无关特征</span>data.drop([<span class="hljs-string">"日期"</span>,<span class="hljs-string">"測站"</span>,<span class="hljs-string">"測項"</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataProcess</span><span class="hljs-params">(data)</span>:</span>    <span class="hljs-string">''' 数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数：</span><span class="hljs-string">        data: 原始训练集数据</span><span class="hljs-string">         </span><span class="hljs-string">    返回：</span><span class="hljs-string">        x: 一天中PM2.5前9项特征构成的训练集。</span><span class="hljs-string">        y: 一天中PM2.5第10项特征构成的目标集。</span><span class="hljs-string">        data: 将原始数据集的每个数据值均转换为float类型后的数据集。</span><span class="hljs-string">    '''</span>    x_list, y_list = [], []    data = data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    <span class="hljs-comment"># 将所有数据转换成float类型</span>    data = np.array(data).astype(float)    <span class="hljs-comment"># 将数据集拆分为多个数据帧</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>): <span class="hljs-comment"># 18 * 240 = 4320</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">15</span>): <span class="hljs-comment"># 24-9=15(组)</span>            <span class="hljs-comment"># 截取PM2.5前9项作为训练数据</span>            sample = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j:j+<span class="hljs-number">9</span>]            <span class="hljs-comment"># 截取PM2.5第10项作为目标数据</span>            label = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j+<span class="hljs-number">9</span>]            x_list.append(sample)            y_list.append(label)    x = np.array(x_list)    y = np.array(y_list)    <span class="hljs-keyword">return</span> x, y, data</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainModel</span><span class="hljs-params">(x_data, y_data, epoch)</span>:</span>    <span class="hljs-string">''' 训练模型，从训练集中拿出3200个数据用来训练，剩余400个数据用于验证。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_data: </span><span class="hljs-string">        y_data:</span><span class="hljs-string">        epoch:</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        weight: 权重</span><span class="hljs-string">        bias：偏置</span><span class="hljs-string">        </span><span class="hljs-string">    '''</span>    <span class="hljs-comment"># 初始化偏置</span>    bias = <span class="hljs-number">0</span>    <span class="hljs-comment"># 初始化权重,生成一个9列的行向量，并全部初始化为1。</span>    weight = np.ones(<span class="hljs-number">9</span>)    <span class="hljs-comment"># 初始化学习率为1。</span>    learning_rate = <span class="hljs-number">1</span>    <span class="hljs-comment"># 初始化正则项系数为0.001。</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 用于存放偏置的梯度平方和。</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 用于存放权重的梯度平方和。</span>    weight_sum = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        <span class="hljs-comment"># 偏置梯度平均值。</span>        b_g = <span class="hljs-number">0</span>        <span class="hljs-comment"># 权重梯度平均值</span>        w_g = np.zeros(<span class="hljs-number">9</span>)        <span class="hljs-comment"># 在所有数据上计算w和b的梯度。</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3200</span>):            b_g += (y_data[j] - weight.dot(x_data[j]) - bias) * (<span class="hljs-number">-1</span>) <span class="hljs-comment"># 如果2个一维向量dot，则结果为它们的内积。</span>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                w_g[k] += (y_data[j] - weight.dot(x_data[j]) - bias) * (-x_data[j, k])        <span class="hljs-comment"># 求平均值</span>        b_g /= <span class="hljs-number">3200</span>        w_g /= <span class="hljs-number">3200</span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):            w_g[k] += reg_rate * weight[k]                <span class="hljs-comment"># adagrad优化</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>        bias -= learning_rate / (bias_sum ** <span class="hljs-number">0.5</span>) * b_g        weight -= learning_rate / (weight_sum ** <span class="hljs-number">0.5</span>) * w_g        <span class="hljs-comment"># 每训练200次输出一次误差</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3200</span>):                loss += (y_data[j] - weight.dot(x_data[j]) - bias) ** <span class="hljs-number">2</span>            loss /= <span class="hljs-number">3200</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                loss += reg_rate * (weight[j] ** <span class="hljs-number">2</span>)            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为&#123;:.2f&#125;'</span>.format(i, loss / <span class="hljs-number">2</span>))        <span class="hljs-keyword">return</span> weight, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validateModel</span><span class="hljs-params">(x_val, y_val, weight, bias)</span>:</span>    <span class="hljs-string">''' 验证模型，返回损失值。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_val: </span><span class="hljs-string">        y_val: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        loss: 平均损失值。</span><span class="hljs-string">    '''</span>    loss = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">400</span>):        loss += (y_val[i] - weight.dot(x_val[i]) - bias) ** <span class="hljs-number">2</span>        <span class="hljs-keyword">return</span> loss / <span class="hljs-number">400</span></code></pre><h3 id="3-4-测试数据预处理"><a href="#3-4-测试数据预处理" class="headerlink" title="3.4 测试数据预处理"></a>3.4 测试数据预处理</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testDataProcess</span><span class="hljs-params">(test_data)</span>:</span>    <span class="hljs-string">''' 测试数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        test_data: 测试数据集。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        testList: 测试数据集。</span><span class="hljs-string">    '''</span>    testList = []    test_data = test_data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    test_data = np.array(test_data).astype(float)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>):        testList.append(test_data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>])    <span class="hljs-keyword">return</span> np.array(testList)</code></pre><h3 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 训练 '''</span>    <span class="hljs-comment"># 读取训练数据，并将其转换为列表形式。</span>    data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, usecols=np.arange(<span class="hljs-number">3</span>, <span class="hljs-number">27</span>).tolist())    x_data, y_data, data = dataProcess(data)        x_train, y_train = x_data[<span class="hljs-number">0</span>:<span class="hljs-number">3200</span>], y_data[<span class="hljs-number">0</span>:<span class="hljs-number">3200</span>]    x_val, y_val = x_data[<span class="hljs-number">3200</span>:<span class="hljs-number">3600</span>], y_data[<span class="hljs-number">3200</span>:<span class="hljs-number">3600</span>]    weight, bias = trainModel(x_train, y_train, <span class="hljs-number">2000</span>)    savePre(weight, bias)    print(<span class="hljs-string">"训练得到的模型的weight为&#123;&#125;"</span>.format(weight))    print(<span class="hljs-string">"训练得到的模型的bias为&#123;&#125;"</span>.format(bias))    loss = validateModel(x_val, y_val, weight, bias)    print(<span class="hljs-string">"模型模型在验证集的loss为&#123;:.2f&#125;"</span>.format(loss))</code></pre><h3 id="3-6-测试模型"><a href="#3-6-测试模型" class="headerlink" title="3.6 测试模型"></a>3.6 测试模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testModel</span><span class="hljs-params">(x_test, weight, bias)</span>:</span>    <span class="hljs-string">''' 用测试数据集测试模型，并将结果保存到output.csv中</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        x_test: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    '''</span>    f = open(<span class="hljs-string">"output.csv"</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>, newline=<span class="hljs-string">""</span>)    csv_write = csv.writer(f)    csv_write.writerow([<span class="hljs-string">"id"</span>, <span class="hljs-string">"value"</span>])    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x_test)):        output = weight.dot(x_test[i]) + bias        csv_write.writerow([<span class="hljs-string">"id_"</span> + str(i), str(output)])    f.close()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 测试 '''</span>    <span class="hljs-comment"># 读取测试数据集</span>    pre = pd.read_csv(<span class="hljs-string">"pre.csv"</span>)    preList = list(pre.replace(<span class="hljs-string">","</span>, <span class="hljs-string">" "</span>))    weight = []    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list(preList[<span class="hljs-number">0</span>].split(<span class="hljs-string">","</span>)):        weight.append(float(i))    bias = float(preList[<span class="hljs-number">1</span>])    test_data = pd.read_csv(<span class="hljs-string">"test.csv"</span>,header=<span class="hljs-literal">None</span>,usecols=np.arange(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>).tolist())    x_test = testDataProcess(test_data)    print(x_test.shape)    print(x_test)    testModel(x_test, np.array(weight), bias)</code></pre><h3 id="3-7-实验结果"><a href="#3-7-实验结果" class="headerlink" title="3.7 实验结果"></a>3.7 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为<span class="hljs-number">477.65</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为<span class="hljs-number">24.93</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为<span class="hljs-number">23.10</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.45</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.14</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为<span class="hljs-number">21.98</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为<span class="hljs-number">21.89</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为<span class="hljs-number">21.85</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为<span class="hljs-number">21.82</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为<span class="hljs-number">21.81</span>训练得到的模型的weight为[ <span class="hljs-number">0.0041924</span>  <span class="hljs-number">-0.03002267</span>  <span class="hljs-number">0.18515256</span> <span class="hljs-number">-0.19707856</span> <span class="hljs-number">-0.03064956</span>  <span class="hljs-number">0.4192367</span> <span class="hljs-number">-0.52283658</span>  <span class="hljs-number">0.05472427</span>  <span class="hljs-number">1.04587455</span>]训练得到的模型的bias为<span class="hljs-number">1.9972854726781095</span>模型模型在验证集的loss为<span class="hljs-number">40.35</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3200项作为training data，后400项作为validation data；此外，本次训练只考虑了PM2.5作为输入，其他观测信息并未使用，所以可能并没有达到很好的效果。在这个模型当中，感觉增加Regularization并未起多大的作用。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2015-CLA-NextCloures Parallel Computation of the Canonical Base</title>
    <link href="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/"/>
    <url>/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/</url>
    
    <content type="html"><![CDATA[<h1 id="NextClosures-Parallel-Computation-of-the-Canonical-Base"><a href="#NextClosures-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="NextClosures:Parallel Computation of the Canonical Base"></a>NextClosures:Parallel Computation of the Canonical Base</h1><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel 、Daniel Borchmann</p><p>​            2. 会议：CLA</p><p>​            3. 时间：2015</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>前人所提的计算主基的算法大多均为串行，一次只计算一个伪内涵。</li><li>本文引入一种并行方式计算主基。</li></ul><p>词汇积累：</p><ul><li>canonical base: 主基，正则基，规范基。</li><li>remedies: 救济方法，弥补手段。</li><li>deficit: 缺陷。</li><li>is proportional to: 与之成正比。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>有两种串行算法$^{[6, 12]}$，即它们一个接一个的计算蕴涵式。</p></li><li><p>目前为止，还不知道能否在多项式时间内计算主基。</p></li><li><p>已有并行计算形式背景的方法$^{[5, 13]}$。</p></li></ol><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><ol><li>A model of $X\rightarrow Y$ is a set $T\subseteq M\Leftrightarrow X\subseteq T\Rightarrow Y\subseteq T$.</li></ol><p>2.A model of $L$ is a model of all implications in $L$, and $X^L$ is the superset of $X$ that is a model of $L$.</p><p>3.$X^L$的计算方式如下：<br>$$<br>\begin{align}<br>X^L:=\bigcup_{n\geq 1}X^{L_n}\ where\ X^{L_1}:&amp;=X\bigcup {B\mid A\rightarrow B\in L\ and\ A\subseteq X} \newline<br>and\ X^{L_{n+1} }:&amp;=(X^{L_n})^{L_1} for\ all\ n\in N<br>\end{align}<br>$$<br>4.内涵：$B=B^{II}$。</p><p>伪内涵：$P\neq P^{II}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{II}\subseteq P$。将形式背景$K$上的所有伪内涵集合记为$PsInt(K)$。</p><p>蕴涵式：${P\rightarrow P^{II}\mid P\in PsInt(K)}$。</p><h2 id="3-Parallel-Computation-of-the-Canonical-Base"><a href="#3-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="3 Parallel Computation of the Canonical Base"></a>3 Parallel Computation of the Canonical Base</h2><p>1.$NextCloures$算法由Ganter$^{[6]}$提出，用于枚举主基。</p><ul><li>算法思想：以一定的线性顺序(即选择顺序)计算形式背景K的所有内涵和伪内涵。</li><li>优势：下一个(伪)意图是唯一确定的，但我们可能需要回溯才能找到它。</li><li>该算法本质上是顺序的，即不可能将其并行化。</li></ul><p>2.我们的方法按照子集序枚举所有的内涵和伪内涵。</p><ul><li>易于并行化枚举。</li><li>多线程实现中，不同线程之间不需要通信。</li></ul><p>原因：检测$P$是否为伪内涵时，只需判断$P$的伪内涵子集$Q\subsetneq P$，是否满足满足$Q^{II}\subseteq P$。即可按基数递增来判断伪内涵。</p><p>算法思想的基本工作流程：</p><p>​            1. 判断$\varnothing=\varnothing^{II}\Rightarrow \varnothing$是内涵/伪内涵。</p><ol start="2"><li>假设基数$&lt;k$的所有伪内涵均已确定，然后就可正确判断出属性集$P，P\subseteq M，|P|=k$是内涵/伪内涵。</li></ol><p>具体的工作流程：</p><p>基本定义：</p><ol><li><p>$K$：有限的形式背景。</p></li><li><p>$k$：当前候选集的基数。候选集为储存当前层级的内涵/伪内涵。</p></li><li><p>$C$：候选集合。</p></li><li><p>$\mathcal{B}$：形式概念集合。</p></li><li><p>$L$：蕴涵式集合。</p></li></ol><p>1.$k:=0，C:={\varnothing}，\mathcal{B}:=\varnothing，L:=\varnothing$.</p><p>2.并行化：对于每个基数$|C|=k$的候选集合C $\in C$，确定它是否是$L^<em>-closed$。如果不是，则将它的$L^</em>-closure$加入候选集合$C$，跳到5。</p><p>3.如果C是形式背景$K$的内涵，那么将形式概念$(C^I，C)$加入集合$\mathcal{B}$中。否则C必为伪内涵，则将蕴涵式$C\rightarrow C^{II}$加入集合$L$，并且将形式概念$(C^I，C^{II})$加入集合$\mathcal{B}$中。</p><p>4.对得到的每一个内涵$C^{II}$，将它的上层邻居$C^{II}\bigcup {m}且m\notin C^{II}$加入候选集合$C$中。</p><p>5.等待基数为$k$的所有候选集合都处理完毕，如果$k&lt;M$则递增$k$，并且跳到2；否则算法结束，返回形式概念集合$\mathcal{B}$和蕴涵式集合$L$。</p><p><img src="../images/2015-CLA-NextClouresParallel%20Computation%20of%20the%20Canonical%20Base/1593623335.jpg" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github/FCA\github\peer-less.github.io\source\_posts\2015-CLA-NextCloures Parallel Computation of the Canonical Base"></p><p>词汇积累：</p><ul><li>backtracking：回溯。</li><li>level-wise：逐级。</li><li>coincide：一致。</li><li>inductive：归纳的；感应的，诱导的。</li><li>suffices：满足。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学-并行计算学科发展历程</title>
    <link href="/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/"/>
    <url>/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="并行计算学科发展历程"><a href="#并行计算学科发展历程" class="headerlink" title="并行计算学科发展历程"></a>并行计算学科发展历程</h1><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：陈国良、张玉杰</p><p>​            2.期刊：计算机科学</p><p>​            3.时间：2020/06/11</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>回顾在并行计算学科发展所做的工作。</li><li>对非数值计算的计算方法进行介绍。</li><li>新型非冯诺依曼结构计算机体系结构的介绍。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>并行计算课程发展的5个阶段：</p><p style="text-indent:2em">非数值计算的并行算法$\rightarrow$新型非冯诺依曼计算机结构$\rightarrow$改革计算机基础课程的计算思维$\rightarrow$数据科学$\rightarrow$大数据计算理论研究。</p><p>完整的并行算法学科体系：算法理论-算法设计-算法实现-算法应用。</p><p>一体化的并行计算研究方法：并行机结构-并行算法-并行编程。</p><h2 id="2-非数值计算中的计算方法"><a href="#2-非数值计算中的计算方法" class="headerlink" title="2 非数值计算中的计算方法"></a>2 非数值计算中的计算方法</h2><p>从计算科学角度数值计算内容主要有：</p><ul><li>矩阵运算。</li><li>线性方程组的求解。</li><li>快速傅里叶变换等。</li></ul><p>非数值计算中的并行算法基本设计策略包括：</p><ul><li>串行算法的直接并行化。</li><li>从问题描述开始设计全新的并行算法。</li><li>借用已有的算法。</li><li>利用已求解问题与待求解问题两者之间的内在相似性来求解新问题。</li></ul><h2 id="3-新型非冯诺依曼计算机体系结构"><a href="#3-新型非冯诺依曼计算机体系结构" class="headerlink" title="3 新型非冯诺依曼计算机体系结构"></a>3 新型非冯诺依曼计算机体系结构</h2><p style="text-indent:2em">传统的冯诺依曼体系结构：第一代计算机（电子管计算机）、第二代计算机（晶体管计算机）、第三代计算机（集成电路计算机）、第四代计算机（大规模超大规模集成电路）</p><p>一些先进新型计算机系统结构：</p><ul><li>微程序控制器设计：<ul><li>设计了“八位运控模型”，并采用了自行提出的“寄存器传输操作语言”进行形式化描述。</li></ul></li><li>直接执行高级语言的计算机：<ul><li>研究了“直接执行的高级语言FORTRAN”机      器，介绍了对标准FORTRAN语言所作的一些限制和补充，简述了该计算机体系结构，列举典型的FORTEAN语言的直接执行过程，并自行提出了可重组结构与之配合。</li></ul></li><li>数据库计算机：<ul><li>研究了数据库计算机，实现RDF查询语言和SQL语言的转换并在此基础上实现一个对用户透明的、建立在关系数据库之上的RDF搜索引擎，以提高其海量存储和查找效率。</li></ul></li><li>光计算机：<ul><li>通过垂直偏振光、水平偏振光和无强光3个稳定的光状态表示信息的三值光计算机原理$^{[7]}$，提出基于光原理三值逻辑计算机。</li></ul></li><li>生物计算机：<ul><li>研究了基于字符串匹配原理的生物序列比对的生物计算机，在纳米计算模型上实现了DNA序列模体发现算法$^{[8]}$。</li></ul></li><li>可重构可变计算机：<ul><li>研究了“可变结构计算机系统”及其结构中的资源间相互通信问题$^{[9]}$。</li></ul></li><li>数据流计算机：<ul><li>根据MIT提出的数据流计算机概念，分析了曼彻斯特大学的数据流计算机，在国内分布式计算计算会议上发表了“数据流计算机体系结构解析”$^{[10]}$一文。</li></ul></li><li>神经计算机：<ul><li>研究神经网络在组合优化中的应用的同时，自行构建了基于Transputer阵列的“通用并行神经网络模拟系统（GP2N2S2）”$^{[10]}$，提供了高级神经网络描述语言及其编辑和编译器的执行环境，实现了程序的自动化执行。</li></ul></li><li>Transpute阵列机：<ul><li>搭建了Transputer阵列机，在中科院计算所进行了组装、调试和运行，并在其上实现了通用的Rohoman应用平台。</li></ul></li><li>量子计算机：<ul><li>在中国科大开展了量子计算研究，讨论量子计算机模型及其物理实现方案、量子计算过程、量子计算模型和量子并行算法，分析量子指数级存储容量和指数加速特征等，并在保密通信、密码安全等领域对量子信息技术进行研究$^{[12, 13]}$。</li></ul></li></ul><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p style="text-indent:2em">大数据、物联网、云计算和区块链是新一代信息技术发展中的华彩乐章。物联网使成千上万的网络传感器嵌入到现实世界中，云计算为物联网产生的海量数据提供了存储空间和在线处理模式，而大数据则让海量数据产生了价值，区块链促进海量信息可靠交互保障生产要素在区域内有序高效地流通。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2018-CS-A General Form of Attribute Exploration </title>
    <link href="/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/"/>
    <url>/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/</url>
    
    <content type="html"><![CDATA[<!-- * @Author: peerless * @Date: 2020-06-28 18:25:20 * @LastEditTime: 2020-06-30 21:27:52 * @LastEditors: Please set LastEditors * @Description: A General Form of Attribute Exploration * @FilePath: \peer-less.github.io\source\_posts\2012CS-A General Form of Attribute Exploration .md   title:         A General Form of Attribute Exploration # 标题   subtitle:                                              # 副标题   date:          2020-06-30                              # 时间   author         peerless                                # 作者   heaeder-img:   img/post-bg-.jpg                        # 这篇文章标题背景图片   catalog:       true                                    # 是否归档   tags:                                                  # 标签      - 学术      - 属性探索         --> <h1 id="A-General-Form-of-Attribute-Exploration"><a href="#A-General-Form-of-Attribute-Exploration" class="headerlink" title="A General Form of Attribute Exploration"></a>A General Form of Attribute Exploration</h1><h2 id="行文思路简要总结"><a href="#行文思路简要总结" class="headerlink" title="行文思路简要总结"></a>行文思路简要总结</h2><p>问题：如何从经典属性探索出发获得通用的属性探索？</p><p>​            1.回顾经典属性探索算法。</p><p>​            2.通过引入3个条件扩展属性探索算法：</p><ul><li>引入两个闭包算子$c_{cert}(A)$与$c_{univ}(A)$。</li><li>不明确指定提供的反例。</li><li>只向专家询问满足$c_{cert}(A)\subsetneq B\subset c_{univ}(A)$的蕴涵式$A\rightarrow B$。</li></ul><p>​            3.对通用属性探索算法进行非冗余性与完备性验证。</p><p>​            4.对通用算法询问蕴涵式时进行改进，使算法向专家询问的蕴涵式的数量是最小的。</p><ul><li>改进方法：若属性集$A=c_{cert}(A)\subsetneq c_{univ}(A)$，则向专家询问蕴涵式$A\rightarrow c_{univ}(A)$。</li></ul><p>总结：</p><p>​            1. 该通用属性算法首先保留了经典属性探索算法的大多性质：完备性、非冗余性和向专家询问蕴涵式的数量最少等。</p><p>​            2. 该通用属性探索算法能够处理抽象给定的闭包算子和部分形式背景下给出的反例。</p><p>问题：</p><ol><li>理论层面：该通用属性探索算法理论上的时间复杂度仍为指数级，无法在多项式时间内计算主基。</li><li>应用层面：该通用属性探索算法并未说明在实际的应用领域中效果如何。</li></ol><h2 id="论文基本信息："><a href="#论文基本信息：" class="headerlink" title="论文基本信息："></a>论文基本信息：</h2><p>​            1.作者：Daniel Borchmann（德累斯顿州立大学数学与科学学院代数研究所丹尼尔·博尔赫曼）</p><p>​            2.期刊：Computer Science</p><p>​            3.时间：2018/11/15</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>提出一种属性探索的一般形式。</li><li>扩展属性探索的适用性。</li><li>将属性探索的现有变种转换为一般形式，简化理论。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>1.属性探索的变种：</p><ul><li>部分形式背景的属性探索$^{[3]}$。</li><li>描述逻辑模型的探索$^{[1, 2]}$。</li></ul><p><font color="red">研究问题：寻找一种将所有变种都包含在内的一般属性探索。</font></p><p>可行原因：</p><pre><code>1.属性探索算法的整体结构均保持不变。2.属性探索的所有重要属性均保留了下来。</code></pre><p>单词积累:</p><pre><code>discourse: 论述、谈话、演讲。</code></pre><p>​         have a close look: 仔细研究。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>蕴涵式<br>$$<br>A\rightarrow B \Leftrightarrow A^{‘}\subseteq B^{‘} \Leftrightarrow B\subseteq A^{‘’}<br>$$</p><p>一些声明：</p><ul><li><p>$Imp(M)$：$M$上的所有蕴涵式集合。</p></li><li><p>$Imp(K)$：形式背景$K$上的所有蕴涵式集合。</p></li><li><p>$Th(K)$：形式背景$K$上的所有成立的蕴涵式集合。</p></li><li><p>$L\subseteq Imp(K)$，$A\subseteq M$。如果对于所有的蕴涵式$(X\rightarrow Y)\in L$，都有$X\subsetneq A或Y\subseteq A$成立，则集合$A$在$L$下为封闭集合。则可做如下定义：<br>  $$<br>  \begin{align}<br>  L^0(A):&amp;=A \newline<br>  L^1(A):&amp;=\bigcup{ {Y\mid (X\rightarrow Y)\in L, X\subseteq A} } \newline<br>  L^i(A):&amp;=L^1(L^{i-1}(A))\ for\ i &gt; 1 \newline<br>  L(A):&amp;=\bigcup\limits_{i\in N}{L^i(A)}<br>  \end{align}<br>  $$<br>  $L(A)$是基于$L$下比集合A大的最小集合。</p><p>  由上可知$L^N(A)$表示集合$A$的子集所能推出来的属性集合的子集所能推出来的属性集合。</p><p>  $Cn(L)$：在$L$下成立的所有蕴涵式集合。</p><ul><li><p>$B\subseteq Imp(K)$在$L$下是非冗余的。$\Leftrightarrow B\subseteq Cn(L)$。</p></li><li><p>$B\subseteq Imp(K)$在$L$下是完备的。$\Leftrightarrow Cn(B)\supseteq L$。</p></li><li><p>$B$是主基。$\Leftrightarrow Cn(B)=Cn(L)$。</p><p>$P$在$L$下是伪闭集，如果以下条件成立：</p></li><li><p>$P\neq L(P)$。</p></li><li><p>对于所有的伪闭集$Q\subsetneq P$有$L(Q)\subseteq P$成立。</p><p>特别的，若$L=Th(K)$，则$P$称为形式背景$K$的伪内涵。则主基可定义为：$Can(L):={P\rightarrow L(P)\mid P 在L下是伪闭集}$。</p><p>由于不知道对象$g$是否具有属性$m$，因此需要引入部分形式背景：存在一个属性集的有序对$(A, B), A, B\in M且A\bigcap B=\varnothing$所构成的集合即为部分形式背景。</p></li><li><p>若$A\bigcup B=M$，则集合称为全局对象描述，即相应对象明确具有的属性集。</p></li><li><p>若$A\bigcup B\neq M$，则集合称为部分对象描述，即相应对象明确不具有的属性集。</p></li></ul></li></ul><h2 id="3-Classical-Attribute-Exploration"><a href="#3-Classical-Attribute-Exploration" class="headerlink" title="3 Classical Attribute Exploration"></a>3 Classical Attribute Exploration</h2><p>$M$是一个有限的属性集合，$K$是基于$M$的形式背景，$p$是基于$M$的领域专家。</p><p>1.对属性集通过字典序进行初始化，并获得第一个属性$\varnothing / P$.</p><p>2.如果$P^{‘}=P$，跳到第5步；否则，令$r:=(P\rightarrow P^{“})$</p><p>3.如果专家认为$r$成立，把$r$加入蕴涵集$Imp(K)$中。</p><p>4.如果专家认为$r$不成立，给出相应的一个反例$C$，将$C$（及其相应的属性）作为新对象加入当前工作形式背景$K$中。</p><p>5.找到字典序$P$的下一个属性集$Q$，若不存在下一个，则算法终止，否则，将$P$设置为$Q$。</p><h2 id="4-Generalizing-Attribute-Exploration"><a href="#4-Generalizing-Attribute-Exploration" class="headerlink" title="4 Generalizing Attribute Exploration"></a>4 Generalizing Attribute Exploration</h2><p><font color="red">推广目的：用更抽象的术语来描述属性探索，以允许该算法在经典属性探索算法之外的应用。</font></p><p>3个扩展：</p><p>1.提出两个闭包操作符：$c_{univ}、c_{cert}$。</p><ul><li><p>$c_{univ}$：我们已知的全部领域知识。$c_{univ}(A)$可以从$A$推出的属性集。</p></li><li><p>$c_{cert}$：我们已知的某些知识。$c_{cert}(A)$确定可以从$A$推出的属性集。</p></li></ul><p>2.采用如下方法对算法进行扩展：</p><ul><li><p>提供反例时，不需要完全指定。</p></li><li><p>只需要所提反例所拥有的信息与所给的蕴涵式相矛盾即可。</p></li><li><p>提供关于该对象具有哪些属性以及不具有的属性信息即可。</p></li></ul><p>3.我们向专家提出的蕴涵式是一种特殊的形式：</p><ul><li>搜索关于$c_{cert}$和$c_{univ}$未确定的蕴涵式$A\rightarrow B$，即$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$。对于这样的蕴涵式，我们不能从$c_{cert}$和$c_{univ}$推断出属性集$c_{univ}(A) \backslash B$是否能从$A$推出或不能推出，因此，我们需要向专家询问。</li></ul><p>Algorithm(General Attribute Exploration)</p><p>   $q$为部分领域专家。  </p><p>   ​    1. $K = \varnothing$<br>   ​    2. 对于有限属性集$A\subseteq M$，若存在有限属性集$B$，有$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$成立，则考虑蕴涵式$A\rightarrow B$；如果不存在，则算法终止，输出$K$与$c_{cert}$。</p><p>   ​    3. 若$q$认为$A\rightarrow B$成立，那么更新$c_{cert}^{‘}=X\longmapsto c_{cert}(L(c_{cert}(X) ) )$。</p><p>​        4. 否则，$(C, D)=q(A\rightarrow B)$作为反例，加入形式背景$K$。</p><p>​        5. 对所有的反例$(C, D)$有：<br>$$<br>\begin{align}<br>C’:&amp;=c_{cert}(C) \newline<br>D’:&amp;=D\bigcup { {m\in M\backslash D\mid c_{cert}(C\bigcup {m})\bigcap D\neq \varnothing}}<br>\end{align}<br>$$<br>​        6. 对所有的$X\subseteq M，X\longmapsto c_{univ}(X)\bigcap K(X)$，即将$c_{univ}$更新为$c_{univ}^{‘}(X)=c_{univ}(X)\bigcap K(X)$。</p><p>​        7. 跳转到2。</p><p>两条性质：非冗余性与完备性。</p><h2 id="5-Computing-Undecided-Implications"><a href="#5-Computing-Undecided-Implications" class="headerlink" title="5 Computing Undecided Implications"></a>5 Computing Undecided Implications</h2><p>目的：使向专家询问蕴涵式的数量是最小的。</p><p>原因：该通用属性探索算法对询问蕴涵式的顺序未做约束。（可能询问以确定的蕴涵式）</p><p>经典属性探索下计算未确定的蕴涵式：</p><p>​        在基于已知蕴涵式$k$的条件下，计算属性集$P$字典序之后的最小属性集$Q(Q\subseteq M，且Q不是当前工作形式背景的内涵)$。那么就需要向专家询问蕴涵式$Q\rightarrow Q^{“}$。</p><p>通用属性探索下计算未确定的蕴涵式：</p><p>​        为了保证向专家询问的蕴涵式的数量是最小的，将通用属性探索算法的第二步更改为：$对于有限属性集A\subseteq M$，有$A=c_{cert}(A)\subsetneq c_{univ}(A)$并且$A$是$\subseteq-minimal$成立，则考虑蕴涵式$A\rightarrow c_{univ}(A)$。</p><p>在给出算法总是能得到最小数量的询问蕴涵式前，先给出如下定义：</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h2><p style="text-indent:2em">从使用领域专家的经典的属性探索中推导出一种更加通用的属性探索算法。它能够处理抽象给定的闭包算子，并且可以处理部分给定的反例。</p><h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
