<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2014-Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</title>
    <link href="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/"/>
    <url>/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/</url>
    
    <content type="html"><![CDATA[<h1 id="Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises"><a href="#Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises" class="headerlink" title="Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises"></a>Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</h1><p class="note note-info">属性探索、快速算法</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Uwe Ryssel、Felix Distel、Daniel Borchmann。</p><p>​            2. 会议：Computer Science。</p><p>​            3. 时间：2014。</p><p>​            4. DOI：10.1007/s10472-013-9355-9 。 </p><p style="display:none"><br>​    @article{Ryssel2014Fast,<br>​      title={Fast algorithms for implication bases and attribute exploration using proper premises},<br>​      author={Ryssel, Uwe and Distel, Felix and Borchmann, Daniel},<br>​      journal={Annals of Mathematics &amp; Artificial Intelligence},<br>​      volume={70},<br>​      number={1-2},<br>​      pages={25-53},<br>​      year={2014},<br>​    }<br>​    </p><p></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>形式概念分析的中心任务是枚举形式背景的最小蕴涵基。</li><li>本文提出了一种快速计算适当前提的新算法。<ul><li>减少了多次获得适当前提的数量。</li><li>减少了在适当前提集合内的冗余。</li></ul></li></ul><p>词汇积累：</p><ul><li>minimal hypergraph transversals：最小超图横断面。</li><li>refactoring：重构。</li><li>heuristic：启发式。</li><li>refactoring of model variants：模型变体重构。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>计算主基存在2种方法$^{[17, 26]}$，由于这两种方法均计算了概念内涵，则无法改变其指数级的复杂度$^{[1, 10]}$。</p></li><li><p>形式概念分析早期提出了一种具有适当前提基的算法，该算法避免了计算概念内涵。</p></li><li><p>有一些方法可以在多项式时间内将适当前提基转换为主基$^{[24, 28]}$。</p></li><li><p>本文提出了一种快速计算适当前提的算法。基于以下三个思想：</p><pre><code>1. 在适当前提和最小超图横断面之间使用一个简单的联系。2. 对最小超图断面的枚举问题进行了深入的研究。3. 可对现有算法可使用与适当前提的联系。</code></pre><p> 1.首先，使用原算法遍历所有属性，并使用黑盒超图算法来计算每个属性的适当前提。<br> 2.为了避免多次计算相同的适当前提，本文引入了一个候选过滤器：对属性集中的每个属性进行过滤，并且只在候选属性集中搜素合适的前提。本文表明，这种过滤方法在保持完备性的同时，大大减少了多次计算的适当前提的数量。<br> 3.通过仅在交不可约属性集中搜索适当的前提来移除适当前提内的冗余。<br> 4.本文认为该算法对于并行化来说是微不足道的，从而导致进一步的加速。由于其增量性质，基于主基的算法的并行化版本至今尚不为人所知。<br> 5.本文将给出使用适当前提的属性探索的另一种变体。它使用与本文对主基的枚举算法相同的方法。</p></li><li><p>考虑不使用伪意图的属性探索的替代公式。在$^{[27]}$中首次尝试使用适当的前提来制定属性探索。</p></li></ol><p>词汇积累：</p><ul><li>well-researched：深入研究。</li><li>artifacts：史前古器物；人工产品。</li><li>negated：否定的。</li><li>fractions of a second：几分之一秒。</li><li>arguments：论点。</li></ul><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>1.$g\swarrow m$：在不包含$m$的对象内涵中，关于子集的顺序$g’$是最大的。</p><p>2.蕴涵式$\mathscr{L}$的形式化描述：<br>$$<br>\begin{align}<br>\mathscr{L}^1(A)&amp;=A\cup \bigcup{Y|(X\rightarrow Y)\in \mathscr{L}, X\subseteq A}  \newline<br>\mathscr{L}^i(A)&amp;=\mathscr{L}^1(\mathscr{L}^{i-1}(A) )\quad for\ i&gt;1  \newline<br>\mathscr{L}(A)&amp;=\bigcup\limits_{i\in N&gt;0}\mathscr{L}^i(A)<br>\end{align}<br>$$<br>原文：Then an implication X →Y follows from the set $\mathscr{L}$ of implications if and only if $Y ⊆ \mathscr{L}$(X). We write $\mathscr{L}\models(X →Y)$ if and only if X →Y follows from $\mathscr{L}$.</p><p>解释：一个蕴涵式$X\rightarrow Y$可从蕴涵式集合$\mathscr{L}$中推出来$，当且仅当$$Y\subseteq \mathscr{L}(X)$成立。可写作$\mathscr{L}\models(X →Y)$，当且仅当$X\rightarrow Y$可由$\mathscr{L}$推出来成立。</p><p>sound(非冗余的、无噪声的)：$\mathscr{L}$中的所有蕴涵式对于形式背景$\mathbb{K}$均成立。</p><p>complete(完备的)：形式背景$\mathbb{K}$中成立的所有蕴涵式均来自于$\mathscr{L}$。</p><p>如果$\mathscr{L}$对于形式背景$\mathbb{K}$是sound and complete，则$\mathscr{L}$就称为形式背景$\mathbb{K}$的一个基。进一步地，$\mathscr{L}^1(A)=\mathscr{L}(A)\ holds\ for\ all\ A\subseteq M$，则$\mathscr{L}$称为直接基。</p><p>伪内涵：$P\neq P^{“}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{“}\subseteq P$。则形式背景的主基为${P\rightarrow P^{“}|P: pseudo-intent\ of\ \mathbb{K}}$。</p><p>前提：B is called a premise for m if $m\in B^{“}\backslash B$，则形式背景的一个基为：$\mathscr{L}={B\rightarrow B^{“}|B: B\subseteq M, premise\ for\ some\ m\in M}$。</p><p>适当前提：B  is called a proper premise if $B^{\bullet}$ is not empty，其中$B^{\bullet}=B^{“}\backslash \left(B\cup\bigcup\limits_{S\subsetneq B}S^{“}\right)$。如果$m\in B^{\bullet}$，则称$m$在$m\in M$中是一个适当前提。可以得到$B$对于$m$是一个适当前提，当且仅当$B$在$m$的前提中是$\subseteq-minimal$。</p><p>进一步地有{$B\rightarrow B^{\bullet}|B: proper\ premise$}是形式背景$\mathbb{K}$的一个complete and sound 直接基。则该集合称为形式背景$\mathbb{K}$的适当前提基。</p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" alt></p><h2 id="3-Proper-Premises-as-Minimal-Hypergraph-Transversals"><a href="#3-Proper-Premises-as-Minimal-Hypergraph-Transversals" class="headerlink" title="3 Proper Premises as Minimal Hypergraph Transversals"></a>3 Proper Premises as Minimal Hypergraph Transversals</h2><h3 id="3-1-基本定义"><a href="#3-1-基本定义" class="headerlink" title="3.1 基本定义"></a>3.1 基本定义</h3><p>minimal hypergraph transversals：用于从关系型数据库中挖掘函数依赖。</p><p>超图以前已经用于关联规则挖掘的相关任务$^{[33]}$。超图如何应用于数据挖掘的概述可以在$^{[20]}$中找到。 </p><p>hypergraph：$V$是有限的顶点集，$V$上的一个超图$\mathscr{H}$是幂集$2^V$的一个子集。每一个集合$E\in \mathscr{H}$为超图的一条边，与经典图论不同的是，这条边可以关联到两个以上的顶点，也可以关联到两个以下的顶点。</p><p>hypergraph transversal：一个集合$S\subseteq V$被称为$\mathscr{H}$的 hypergraph transversal，当它与每条边$E\in \mathscr{H}$都相交。 即$\forall E\in \mathscr{H}, S\cap E\neq \varnothing$。</p><p>minimal hypergraph transversal：集合$S\subseteq V$被称为最小超图横断面，当S在$\mathscr{H}$的所有超图横断面中的子集序下是最小的。</p><p>transversal hypergraph：$\mathscr{H}$的所有 minimal hypergraph transversal构成的集合。记为$Tr(\mathscr{H})$。</p><p>TRANSHYP：判定超图$\mathscr{H}$是否是超图$\mathscr{G}$的 transversal hypergraph 的问题。</p><p>TRANSENUM：枚举出超图$\mathscr{G}$的所有minal hypergraph transversal 的问题。</p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" alt></p><p>解释：$P\subseteq M\backslash{m}对于m\in M$是一个前提，当且仅当对于所有的$g\in G\ with\ g\swarrow m$成立。$P$对于$m$是一个适当前提，当且仅当在$m$的所有前提中，按子集序$P$是最小的。则有推论</p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><p>implicitly：含蓄地；暗中地；间接的；不言而喻。</p><p>in contrast to：与…不同。</p><p>be incident to：关联到…。</p><p>quasi-polynomial time：准多项式时间。</p><h2 id="4-Improvements-to-the-Algorithm"><a href="#4-Improvements-to-the-Algorithm" class="headerlink" title="4 Improvements to the Algorithm"></a>4 Improvements to the Algorithm</h2><h3 id="4-1-Avoiding-Duplicates-using-Candidate-Sets"><a href="#4-1-Avoiding-Duplicates-using-Candidate-Sets" class="headerlink" title="4.1 Avoiding Duplicates using Candidate Sets"></a>4.1 Avoiding Duplicates using Candidate Sets</h3><p>原因：Algorithm1会多次计算适当前提，因为它们可以是多个属性的适当前提。如{c, e}是属性a, b, d 的适当前提，则其会计算3次。</p><p>第一种思想：根据当前属性，引入相关属性的候选集。则只需在候选集集合$C$中搜索$\mathscr{H}_{\mathbb{K}, m}^{\swarrow}$最小超图横断面即可。</p><p>冗余条件：$\mu w\wedge\mu m\leq\mu v\lt\mu m$。</p><p>候选集合$C$：$C={u\in M\backslash{m}|\nexists v\in M: \mu u\wedge\mu m\leq\mu v\lt\mu m}$。</p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" alt></p><p>词汇积累</p><ul><li>intuition：直觉的。</li><li>snippet：片段。</li><li>identify：确定；识别。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>快速算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW2</title>
    <link href="/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/"/>
    <url>/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）"><a href="#李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）" class="headerlink" title="李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）"></a>李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）</h1><p>本文主要参考了博文<a href="https://www.cnblogs.com/HL-space/p/10785225.html" target="_blank" rel="noopener">https://www.cnblogs.com/HL-space/p/10785225.html</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是根据每个ID的各种属性值去判断该ID对应角色是Winner还是Losser（收入是否大于50K）。显然这是一个二分类问题，可以运用Logistic Regression来实现。</p><ul><li>spam_train.csv：形状为4000×59，4000行数据对应4000个角色，ID编号从1到4001，59列属性中，第一列为角色ID，最后一列为分类结果（label为0/1），中间57列为角色对应的57种属性值。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><ol><li>本次任务先对数据做线性回归，得出每个样本的回归值，计算公式为：</li></ol><p>$$<br>y^n=\sum\limits_{i=1}^{57}w_ix_i^n+b<br>$$</p><p>其中，$x_i^n$为第$n$个样本值，$y^n$为对应的回归结果。</p><ol start="2"><li><p>将回归结果送入$sigmod$函数，得到对应的概率值。计算公式为：<br> $$<br> p^n=\frac{1}{1+e^{-y^n} }<br> $$</p></li><li><p>众所周知，不管线性回归还是Logistic回归，其关键和核心就在于通过误差的反向传播来更新参数，进而使模型不断优化。因此，损失函数的确定及对各参数的求导就成了重中之重。在分类问题中，模型一般针对各类别输出一个概率分布，因此常用交叉熵作为损失函数。交叉熵可用于衡量两个概率分布之间的相似、统一程度，两个概率分布越相似、越统一，则交叉熵越小；反之，两概率分布之间差异越大、越混乱，则交叉熵越大。</p></li></ol><p>下式表示k分类问题的交叉熵，P为label，是一个概率分布，常用one_hot编码。例如针对3分类问题而言，若样本属于第一类，则P为(1,0,0)，若属于第二类，则P为(0,1,0)，若属于第三类，则为(0,0,1)。即所属的类概率值为1，其他类概率值为0。Q为模型得出的概率分布，可以是(0.1,0.8,0.1)等。具体计算公式为：<br>$$<br>Loss^n=-\sum\limits_1^kp^n\log Q^n<br>$$<br>在实际应用中，为求导方便，常使用以e为底的对数。则上式可化为：<br>$$<br>Loss^n=-\sum\limits_1^kp^n\ln Q^n<br>$$<br>针对本次作业而言，虽然模型只输出了一个概率值p，但由于处理的是二分类问题，因此可以很快求出另一概率值为1-p，即可视为模型输出的概率分布为Q(p，1-p)。将本次的label视为概率分布P(y,1-y)，即Winner(label为1)的概率分布为(1,0)，分类为Losser(label为0)的概率分布为(0,1)。则其==损失函数==为：<br>$$<br>Loss^n=-[\hat{y}^n\ln p^n+(1-\hat{y}^n)\ln(1-p^n)]<br>$$<br>Loss对权重$w$求偏导，有：<br>$$<br>\begin{align}<br>因为&amp;\frac{\partial\ln p^n}{\partial p^n}=\frac{1}{p^n}=1+e^{-y^n} \newline<br>同理&amp;\frac{\partial\ln (1-p^n)}{\partial p^n}=\frac{-1}{1-p^n}=\frac{-e^{-y^n} }{1+e^{-y^n} } \newline<br>而且&amp;\frac{\partial p^n}{\partial y^n}=-\frac{1}{(1+e^{-y^n})^2}(e^{-y^n})(-1)=\frac{-e^{-y^n} }{(1+e^{-y^n})^2} \newline<br>和&amp;\frac{\partial y^n}{\partial w_i}=x_i \newline\newline<br>\end{align}<br>$$</p><p>$$<br>\begin{align}<br>所以有\frac{\partial Loss^n}{\partial w_i}&amp;=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}]  \newline<br>&amp;=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i]  \newline<br>&amp;=-x_i(\hat{y}^n-p^n)<br>\end{align}<br>$$</p><p>同理Loss对偏置$b$求偏导，有：<br>$$<br>\begin{align}<br>\frac{\partial Loss^n}{\partial b}&amp;=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}]  \newline<br>&amp;=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})]  \newline<br>&amp;=-(\hat{y}^n-p^n)<br>\end{align}<br>$$</p><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>首先将所有空值以0填充，然后对数据进行标准化处理。</p><p>上述操作完成后，将表格的第2列至58列取出为x(shape为4000×57)，将最后一列取出做label y(shape为4000×1)。</p><p>进一步划分训练集和验证集，分别取x、y中前3000个样本为训练集x_train(shape为3000×57)，y_train(shape为3000×1)，后1000个样本为验证集x_val(shape为1000×57)，y_val(shape为1000×1)。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"spam_train.csv"</span>)<span class="hljs-comment"># 空值填0</span>data = data.fillna(<span class="hljs-number">0</span>)<span class="hljs-comment"># 转换为array类型</span>data = np.array(data)<span class="hljs-comment"># 取样本x(4000*57)</span>x = data[:, <span class="hljs-number">1</span>:<span class="hljs-number">-1</span>]<span class="hljs-comment"># 标准化</span>x[<span class="hljs-number">-1</span>] /= np.mean(x[<span class="hljs-number">-1</span>])x[<span class="hljs-number">-2</span>] /= np.mean(x[<span class="hljs-number">-2</span>])<span class="hljs-comment"># 取目标值y</span>y = data[:, <span class="hljs-number">-1</span>]<span class="hljs-comment"># 划分训练集与验证集</span>x_train, x_val = x[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>, :], x[<span class="hljs-number">3000</span>:, :]y_train, y_val = y[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y[<span class="hljs-number">3000</span>:]</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练模型，更新参数。</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(x_train, y_train, epoch)</span>:</span>    num = x_train.shape[<span class="hljs-number">0</span>]    dim = x_train.shape[<span class="hljs-number">1</span>]    bias = <span class="hljs-number">0</span>    weights = np.ones(dim)    learning_rate = <span class="hljs-number">1</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 偏置梯度平方和</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 权重梯度平方和</span>    weight_sum = np.zeros(dim)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        b_g = <span class="hljs-number">0</span>        w_g = np.zeros(dim)                <span class="hljs-comment"># 在所有数据上计算梯度，梯度计算时针对损失函数求导</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):            y_pre = weights.dot(x_train[j, :]) + bias            sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))            b_g += (<span class="hljs-number">-1</span>) * (y_train[j] - sig)            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(dim):                w_g[k] += (<span class="hljs-number">-1</span>) * (y_train[j] - sig) * x_train[j, k] + <span class="hljs-number">2</span> * reg_rate * weights[k]        b_g /= num        w_g /= num        <span class="hljs-comment"># adagrad</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>         <span class="hljs-comment"># 更新权重和偏置</span>        bias -= learning_rate / bias_sum ** <span class="hljs-number">0.5</span> * b_g        weights -= learning_rate / weight_sum ** <span class="hljs-number">0.5</span> * w_g        <span class="hljs-comment"># 每训练100轮，输出一次在训练集上的正确率。</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            acc = <span class="hljs-number">0</span>            result = np.zeros(num)            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):                y_pre = weights.dot(x_train[j, :]) + bias                sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))                <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:                    result[j] = <span class="hljs-number">1</span>                <span class="hljs-keyword">else</span>:                    result[j] = <span class="hljs-number">0</span>                <span class="hljs-keyword">if</span> result[j] == y_train[j]:                    acc += <span class="hljs-number">1.0</span>                loss += (<span class="hljs-number">-1</span>) * (y_train * np.log(sig) + (<span class="hljs-number">1</span> - y_train[j]) * np.log(<span class="hljs-number">1</span> - sig))            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为'</span>.format(i), loss / num)            print(<span class="hljs-string">'&#123;&#125; 次训练后, 训练集的精度为&#123;:.2f&#125;'</span>.format(i, acc / num))    <span class="hljs-keyword">return</span> weights, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 验证模型效果</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate</span><span class="hljs-params">(x_val, y_val, weights, bias)</span>:</span>    num = <span class="hljs-number">1000</span>    loss = <span class="hljs-number">0</span>    acc = <span class="hljs-number">0</span>    result = np.zeros(num)    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):        y_pre = weights.dot(x_val[j, :]) + bias        sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))        <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:            result[j] = <span class="hljs-number">1</span>        <span class="hljs-keyword">else</span>:            result[j] = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> result[j] == y_val[j]:            acc += <span class="hljs-number">1.0</span>        loss += (<span class="hljs-number">-1</span>) * (y_val * np.log(sig) + (<span class="hljs-number">1</span> - y_val[j]) * np.log(<span class="hljs-number">1</span> - sig))    <span class="hljs-keyword">return</span> loss / num, acc / num</code></pre><h3 id="3-4-测试模型"><a href="#3-4-测试模型" class="headerlink" title="3.4 测试模型"></a>3.4 测试模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练轮数</span>epoch = <span class="hljs-number">4000</span><span class="hljs-comment"># 开始训练</span>w, b = train(x_train, y_train, epoch)<span class="hljs-comment"># 在验证集上看效果</span>loss, acc = validate(x_val, y_val, w, b)print(<span class="hljs-string">'验证集的损失值为:'</span>, loss)print(<span class="hljs-string">'验证集的精度值为:'</span>, acc)</code></pre><h3 id="3-5-实验结果"><a href="#3-5-实验结果" class="headerlink" title="3.5 实验结果"></a>3.5 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为 [<span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span>  <span class="hljs-number">1.50623089</span> ... <span class="hljs-number">1.50623089</span> <span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span> ]<span class="hljs-number">0</span> 次训练后, 训练集的精度为<span class="hljs-number">0.62</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.90</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">2000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.86</span> <span class="hljs-number">2400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span> <span class="hljs-number">2800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span>验证集的损失值为: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]验证集的精度值为: <span class="hljs-number">0.864</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率基本能达到90%，验证集分类的正确率能达到86.4%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW1</title>
    <link href="/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/"/>
    <url>/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）"><a href="#李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）" class="headerlink" title="李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）"></a>李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）</h1><p>本文主要参考了博文<a href="https://blog.csdn.net/m123_45n/article/details/106560274" target="_blank" rel="noopener">https://blog.csdn.net/m123_45n/article/details/106560274</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是利用Liner Regression去预测丰原市PM2.5的数值。对丰原市一年的观测记录分为了train set和test set，其中train set 是丰原市每个月前20天的所有记录。test set则是从丰原市剩下的记录取样出来。train set和test set都是以csv格式的文件进行保存的。</p><ul><li>train.csv：每个月前20天的完整数据。</li><li>test.csv：从剩下的数据中取样出连续的10小时为一组，前九个小时的所有观测数据当作==特征值==，第十个小时的PM2.5当作==目标值==。共取出240组不重复的test data，然后根据feature去预测出第十小时的PM2.5的值。</li><li>Data（中含有18项观测数据）：AMB_TEMP，CH4，CO，NHMC，NO，NO2，NOx，O3，PM10，PM2.5，RAINFALL，RH，SO2，THC，WD_HR，WIND_DIREC，WIND_SPEED，WS_HR。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><p>本次任务选择的模型为线性回归模型：<br>$$<br>y=\sum\limits_{i=0}^8w_ix_i+b。<br>$$</p><ul><li><p>$i$从0到8是因为选取前9个小时作为==特征值==输入，每个输入都有一个==权重值==$w$与之相乘，再加上一个偏置$b$，即为线性回归模型。通过这个模型去预测第十个小时的PM2.5的值。</p></li><li><p>此外，可将该模型的运算转换为向量运算：<br>  $$<br>  \begin{align}<br>  y=<br>  \begin{bmatrix}<br>  w_0\ \dots\ w_8<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  x_0\newline \vdots\newline x_8<br>  \end{bmatrix}<br>  +b<br>\end{align}<br>  $$</p></li><li><p>选择的<strong>Loss Function</strong>为：</p></li></ul><p>$$<br>Loss=\frac{1}{2m}\sum\limits_{i=0}^{m-1}(\hat{y}^i-y^i)^2+\frac12\lambda\sum\limits_{j=0}^8w_j^2<br>$$</p><ul><li><p>Loss对$w_j$求导为：<br>  $$<br>  \frac{\partial L}{\partial w_j}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-x_j)+\lambda w_j<br>  $$</p></li><li></li></ul><p>$$<br>\frac{\partial L}{\partial b}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-1)<br>$$</p><ul><li><p>则参数更新为：<br>  $$<br>  \begin{align}<br>  w_j:=w_j-\eta \frac{\partial L}{\partial w_j} \newline<br>  b:=b-\eta \frac{\partial L}{\partial b}<br>  \end{align}<br>  $$<br>  其中将$w_j$的更新转换为向量运算：<br>  $$<br>  \begin{bmatrix}<br>  w_0 \newline \vdots \newline w_8<br>  \end{bmatrix}<br>  :=<br>  \begin{bmatrix}<br>  w_0 \newline \vdots \newline w_8<br>  \end{bmatrix}<br>  -\eta<br>  \begin{bmatrix}<br>  \frac{\partial L}{\partial w_0} \newline \vdots \newline \frac{\partial L}{\partial w_8}<br>  \end{bmatrix}<br>  $$</p></li><li><p>Optimizer的选择：Adagrad。更新方式为：</p></li></ul><p>$$<br>\begin{align}<br>w^1=&amp;w^0-\frac{\eta^0}{\sigma^0}g^0 \newline<br>\vdots \newline<br>w^{t+1}=&amp;w^t-\frac{\eta^t}{\sigma^t}g^t<br>\end{align}<br>$$</p><p>​                其中$g^t$为$w^t$的梯度值，而且有：<br>$$<br>\begin{align}<br>\sigma^t=&amp;\sqrt{\frac{1}{t+1}\sum\limits_{i=0}^t(g^i)^2} \newline<br>\eta^t=&amp;\frac{\eta}{t+1}<br>\end{align}<br>$$<br>​                带入后，则有：<br>$$<br>w^{t+1}=w^t-\frac{\eta}{\sqrt{\sum\limits_{i=0}^t(g^i)^2} }<br>$$</p><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率能达到90%，验证集分类的正确率能达到87.2%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, header=<span class="hljs-number">0</span>)<span class="hljs-comment"># 删除无关特征</span>data.drop([<span class="hljs-string">"日期"</span>,<span class="hljs-string">"測站"</span>,<span class="hljs-string">"測項"</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataProcess</span><span class="hljs-params">(data)</span>:</span>    <span class="hljs-string">''' 数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数：</span><span class="hljs-string">        data: 原始训练集数据</span><span class="hljs-string">         </span><span class="hljs-string">    返回：</span><span class="hljs-string">        x: 一天中PM2.5前9项特征构成的训练集。</span><span class="hljs-string">        y: 一天中PM2.5第10项特征构成的目标集。</span><span class="hljs-string">        data: 将原始数据集的每个数据值均转换为float类型后的数据集。</span><span class="hljs-string">    '''</span>    x_list, y_list = [], []    data = data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    <span class="hljs-comment"># 将所有数据转换成float类型</span>    data = np.array(data).astype(float)    <span class="hljs-comment"># 将数据集拆分为多个数据帧</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>): <span class="hljs-comment"># 18 * 240 = 4320</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">15</span>): <span class="hljs-comment"># 24-9=15(组)</span>            <span class="hljs-comment"># 截取PM2.5前9项作为训练数据</span>            sample = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j:j+<span class="hljs-number">9</span>]            <span class="hljs-comment"># 截取PM2.5第10项作为目标数据</span>            label = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j+<span class="hljs-number">9</span>]            x_list.append(sample)            y_list.append(label)    x = np.array(x_list)    y = np.array(y_list)    <span class="hljs-keyword">return</span> x, y, data</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainModel</span><span class="hljs-params">(x_data, y_data, epoch)</span>:</span>    <span class="hljs-string">''' 训练模型，从训练集中拿出3000个数据用来训练，剩余600个数据用于验证。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_data: </span><span class="hljs-string">        y_data:</span><span class="hljs-string">        epoch:</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        weight: 权重</span><span class="hljs-string">        bias：偏置</span><span class="hljs-string">        </span><span class="hljs-string">    '''</span>    <span class="hljs-comment"># 初始化偏置</span>    bias = <span class="hljs-number">0</span>    <span class="hljs-comment"># 初始化权重,生成一个9列的行向量，并全部初始化为1。</span>    weight = np.ones(<span class="hljs-number">9</span>)    <span class="hljs-comment"># 初始化学习率为1。</span>    learning_rate = <span class="hljs-number">1</span>    <span class="hljs-comment"># 初始化正则项系数为0.001。</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 用于存放偏置的梯度平方和。</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 用于存放权重的梯度平方和。</span>    weight_sum = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        <span class="hljs-comment"># 偏置梯度平均值。</span>        b_g = <span class="hljs-number">0</span>        <span class="hljs-comment"># 权重梯度平均值</span>        w_g = np.zeros(<span class="hljs-number">9</span>)        <span class="hljs-comment"># 在所有数据上计算w和b的梯度。</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):            b_g += (y_data[j] - weight.dot(x_data[j]) - bias) * (<span class="hljs-number">-1</span>) <span class="hljs-comment"># 如果2个一维向量dot，则结果为它们的内积。</span>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                w_g[k] += (y_data[j] - weight.dot(x_data[j]) - bias) * (-x_data[j, k])        <span class="hljs-comment"># 求平均值</span>        b_g /= <span class="hljs-number">3000</span>        w_g /= <span class="hljs-number">3000</span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):            w_g[k] += reg_rate * weight[k]                <span class="hljs-comment"># adagrad优化</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>        bias -= learning_rate / (bias_sum ** <span class="hljs-number">0.5</span>) * b_g        weight -= learning_rate / (weight_sum ** <span class="hljs-number">0.5</span>) * w_g        <span class="hljs-comment"># 每训练200次输出一次误差</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):                loss += (y_data[j] - weight.dot(x_data[j]) - bias) ** <span class="hljs-number">2</span>            loss /= <span class="hljs-number">3000</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                loss += reg_rate * (weight[j] ** <span class="hljs-number">2</span>)            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为&#123;:.2f&#125;'</span>.format(i, loss / <span class="hljs-number">2</span>))        <span class="hljs-keyword">return</span> weight, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validateModel</span><span class="hljs-params">(x_val, y_val, weight, bias)</span>:</span>    <span class="hljs-string">''' 验证模型，返回损失值。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_val: </span><span class="hljs-string">        y_val: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        loss: 平均损失值。</span><span class="hljs-string">    '''</span>    loss = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">600</span>):        loss += (y_val[i] - weight.dot(x_val[i]) - bias) ** <span class="hljs-number">2</span>        <span class="hljs-keyword">return</span> loss / <span class="hljs-number">600</span></code></pre><h3 id="3-4-测试数据预处理"><a href="#3-4-测试数据预处理" class="headerlink" title="3.4 测试数据预处理"></a>3.4 测试数据预处理</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testDataProcess</span><span class="hljs-params">(test_data)</span>:</span>    <span class="hljs-string">''' 测试数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        test_data: 测试数据集。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        testList: 测试数据集。</span><span class="hljs-string">    '''</span>    testList = []    test_data = test_data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    test_data = np.array(test_data).astype(float)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>):        testList.append(test_data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>])    <span class="hljs-keyword">return</span> np.array(testList)</code></pre><h3 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 训练 '''</span>    <span class="hljs-comment"># 读取训练数据，并将其转换为列表形式。</span>    data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, usecols=np.arange(<span class="hljs-number">3</span>, <span class="hljs-number">27</span>).tolist())    x_data, y_data, data = dataProcess(data)        x_train, y_train = x_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>]    x_val, y_val = x_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>], y_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>]    weight, bias = trainModel(x_train, y_train, <span class="hljs-number">2000</span>)    savePre(weight, bias)    print(<span class="hljs-string">"训练得到的模型的weight为&#123;&#125;"</span>.format(weight))    print(<span class="hljs-string">"训练得到的模型的bias为&#123;&#125;"</span>.format(bias))    loss = validateModel(x_val, y_val, weight, bias)    print(<span class="hljs-string">"模型模型在验证集的loss为&#123;:.2f&#125;"</span>.format(loss))</code></pre><h3 id="3-6-测试模型"><a href="#3-6-测试模型" class="headerlink" title="3.6 测试模型"></a>3.6 测试模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testModel</span><span class="hljs-params">(x_test, weight, bias)</span>:</span>    <span class="hljs-string">''' 用测试数据集测试模型，并将结果保存到output.csv中</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        x_test: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    '''</span>    f = open(<span class="hljs-string">"output.csv"</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>, newline=<span class="hljs-string">""</span>)    csv_write = csv.writer(f)    csv_write.writerow([<span class="hljs-string">"id"</span>, <span class="hljs-string">"value"</span>])    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x_test)):        output = weight.dot(x_test[i]) + bias        csv_write.writerow([<span class="hljs-string">"id_"</span> + str(i), str(output)])    f.close()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 测试 '''</span>    <span class="hljs-comment"># 读取测试数据集</span>    pre = pd.read_csv(<span class="hljs-string">"pre.csv"</span>)    preList = list(pre.replace(<span class="hljs-string">","</span>, <span class="hljs-string">" "</span>))    weight = []    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list(preList[<span class="hljs-number">0</span>].split(<span class="hljs-string">","</span>)):        weight.append(float(i))    bias = float(preList[<span class="hljs-number">1</span>])    test_data = pd.read_csv(<span class="hljs-string">"test.csv"</span>,header=<span class="hljs-literal">None</span>,usecols=np.arange(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>).tolist())    x_test = testDataProcess(test_data)    print(x_test.shape)    print(x_test)    testModel(x_test, np.array(weight), bias)</code></pre><h3 id="3-7-实验结果"><a href="#3-7-实验结果" class="headerlink" title="3.7 实验结果"></a>3.7 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为<span class="hljs-number">477.36</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为<span class="hljs-number">25.08</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为<span class="hljs-number">23.30</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.67</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.37</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为<span class="hljs-number">22.22</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为<span class="hljs-number">22.14</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为<span class="hljs-number">22.10</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.08</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.07</span>训练得到的模型的weight为[ <span class="hljs-number">0.00194352</span> <span class="hljs-number">-0.02562139</span>  <span class="hljs-number">0.18138721</span> <span class="hljs-number">-0.19572767</span> <span class="hljs-number">-0.0230436</span>   <span class="hljs-number">0.40959797</span> <span class="hljs-number">-0.51985702</span>  <span class="hljs-number">0.06835844</span>  <span class="hljs-number">1.03413381</span>]训练得到的模型的bias为<span class="hljs-number">1.948721373892966</span>模型模型在验证集的loss为<span class="hljs-number">39.17</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">在这个模型当中，可以看到在经过2000次训练后，模型在验证集上的损失已达到了39.17。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后600项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2015-CLA-NextCloures Parallel Computation of the Canonical Base</title>
    <link href="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/"/>
    <url>/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/</url>
    
    <content type="html"><![CDATA[<h1 id="NextClosures-Parallel-Computation-of-the-Canonical-Base"><a href="#NextClosures-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="NextClosures: Parallel Computation of the Canonical Base"></a>NextClosures: Parallel Computation of the Canonical Base</h1><div class="note note-primary">            <p>属性探索、并行算法、NextCloures</p>          </div> <a id="more"></a><!-- <p class='note note-info'>论文、属性探索</p> --><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel 、Daniel Borchmann</p><p>​            2. 会议：CLA</p><p>​            3. 时间：2015</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>前人所提的计算主基的算法大多均为串行，一次只计算一个伪内涵。</li><li>本文引入一种并行方式计算主基。</li></ul><p>词汇积累：</p><ul><li>canonical base: 主基，正则基，规范基。</li><li>remedies: 救济方法，弥补手段。</li><li>deficit: 缺陷。</li><li>is proportional to: 与之成正比。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>有两种串行算法$^{[6, 12]}$，即它们一个接一个的计算蕴涵式。</p></li><li><p>目前为止，还不知道能否在多项式时间内计算主基。</p></li><li><p>已有并行计算形式背景的方法$^{[5, 13]}$。</p></li></ol><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><ol><li>A model of $X\rightarrow Y$ is a set $T\subseteq M\Leftrightarrow X\subseteq T\Rightarrow Y\subseteq T$.</li></ol><p>2.A model of $L$ is a model of all implications in $L$, and $X^L$ is the superset of $X$ that is a model of $L$.</p><p>3.$X^L$的计算方式如下：<br>$$<br>\begin{align}<br>X^L:=\bigcup_{n\geq 1}X^{L_n}\ where\ X^{L_1}:&amp;=X\bigcup {B\mid A\rightarrow B\in L\ and\ A\subseteq X} \newline<br>and\ X^{L_{n+1} }:&amp;=(X^{L_n})^{L_1} for\ all\ n\in N<br>\end{align}<br>$$<br>4.内涵：$B=B^{II}$。</p><p>伪内涵：$P\neq P^{II}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{II}\subseteq P$。将形式背景$K$上的所有伪内涵集合记为$PsInt(K)$。</p><p>蕴涵式：${P\rightarrow P^{II}\mid P\in PsInt(K)}$。</p><h2 id="3-Parallel-Computation-of-the-Canonical-Base"><a href="#3-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="3 Parallel Computation of the Canonical Base"></a>3 Parallel Computation of the Canonical Base</h2><p>1.$NextCloures$算法由Ganter$^{[6]}$提出，用于枚举主基。</p><ul><li>算法思想：以一定的线性顺序(即选择顺序)计算形式背景K的所有内涵和伪内涵。</li><li>优势：下一个(伪)意图是唯一确定的，但我们可能需要回溯才能找到它。</li><li>该算法本质上是顺序的，即不可能将其并行化。</li></ul><p>2.我们的方法按照子集序枚举所有的内涵和伪内涵。</p><ul><li>易于并行化枚举。</li><li>多线程实现中，不同线程之间不需要通信。</li></ul><p>原因：检测$P$是否为伪内涵时，只需判断$P$的伪内涵子集$Q\subsetneq P$，是否满足满足$Q^{II}\subseteq P$。即可按基数递增来判断伪内涵。</p><p>算法思想的基本工作流程：</p><p>​            1. 判断$\varnothing=\varnothing^{II}\Rightarrow \varnothing$是内涵/伪内涵。</p><ol start="2"><li>假设基数$&lt;k$的所有伪内涵均已确定，然后就可正确判断出属性集$P，P\subseteq M，|P|=k$是内涵/伪内涵。</li></ol><p>具体的工作流程：</p><p>基本定义：</p><ul><li>$K$：有限的形式背景。</li><li>$k$：当前候选集的基数。候选集为储存当前层级的内涵/伪内涵。</li><li>$C$：候选集合。</li><li>$\mathcal{B}$：形式概念集合。</li><li>$L$：蕴涵式集合。</li></ul><p>具体流程：</p><p>1.$k:=0，C:={\varnothing}，\mathcal{B}:=\varnothing，L:=\varnothing$.</p><p>2.并行化：对于每个基数$|C|=k$的候选集合C $\in C$，确定它是否是$L^<em>-closed$。如果不是，则将它的$L^</em>-closure$加入候选集合$C$，跳到5。如果是$L^*-closed$，则跳到3。</p><p>3.如果C是形式背景$K$的内涵，那么将形式概念$(C^I，C)$加入集合$\mathcal{B}$中。否则C必为伪内涵，则将蕴涵式$C\rightarrow C^{II}$加入集合$L$，并且将形式概念$(C^I，C^{II})$加入集合$\mathcal{B}$中。</p><p>4.对得到的每一个内涵$C^{II}$，将它的上层邻居$C^{II}\bigcup {m}且m\notin C^{II}$加入候选集合$C$中。</p><p>5.等待基数为$k$的所有候选集合都处理完毕，如果$k&lt;M$则递增$k$，并且跳到2；否则算法结束，返回形式概念集合$\mathcal{B}$和蕴涵式集合$L$。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335.jpg" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335-1593786388801.jpg" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li>backtracking：回溯。</li><li>level-wise：逐级。</li><li>coincide：一致。</li><li>inductive：归纳的；感应的，诱导的。</li><li>suffices：满足。</li></ul><h2 id="4-Benchmarks"><a href="#4-Benchmarks" class="headerlink" title="4 Benchmarks"></a>4 Benchmarks</h2><p style="text-indent:2em">本节主要目的在于将我们并行式的算法与线性序的$NextCloures$算法在计算主基时进行定性与定量的比较分析。 </p><p style="text-indent:2em">结果：并行式算法在一定的极限下，算法的运行时间与可用CPU的数量成正比的减少。 </p><p>​        特点：同一层的候选集不能彼此影响，即线程之间不需要通信。</p><p>​        异常情况：在某些情况下，当使用所有可用的CPU时，计算时间会增加。（原因未知）—可能是由于平台或操作系统的一些技术细节，比如基准测试过程中执行的一些后台任务，或者线程维护带来的开销。</p><p>词汇积累：</p><ul><li>benchmarks：基准测试。</li><li>overhead caused：导致的开销。</li></ul><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p style="text-indent:2em">本文介绍了计算正则基的并行式算法NextCloures。该算法以层次化的顺序（即增加基数），自下而上的构造给定形式背景的所有内涵和伪内涵的概念格。</p><p style="text-indent:2em">由于概念格中某一层的元素可以独立计算，也可以并行枚举，从而产生了计算正则基的并行算法。</p><p>可能的扩展：</p><ul><li>处理一组蕴涵或约束闭包算子给出的背景知识。</li><li>属性探索：包含专家交互，以探索部分已知形式背景的规范基。<ul><li>可以让几位专家同时回答问题。</li><li>问题的难度即前提基数与经典属性探索线性序提出的问题相比不断增加。</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>并行算法</tag>
      
      <tag>NextCloures</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学-并行计算学科发展历程</title>
    <link href="/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/"/>
    <url>/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="并行计算学科发展历程"><a href="#并行计算学科发展历程" class="headerlink" title="并行计算学科发展历程"></a>并行计算学科发展历程</h1><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：陈国良、张玉杰</p><p>​            2.期刊：计算机科学</p><p>​            3.时间：2020/06/11</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>回顾在并行计算学科发展所做的工作。</li><li>对非数值计算的计算方法进行介绍。</li><li>新型非冯诺依曼结构计算机体系结构的介绍。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>并行计算课程发展的5个阶段：</p><p style="text-indent:2em">非数值计算的并行算法$\rightarrow$新型非冯诺依曼计算机结构$\rightarrow$改革计算机基础课程的计算思维$\rightarrow$数据科学$\rightarrow$大数据计算理论研究。</p><p>完整的并行算法学科体系：算法理论-算法设计-算法实现-算法应用。</p><p>一体化的并行计算研究方法：并行机结构-并行算法-并行编程。</p><h2 id="2-非数值计算中的计算方法"><a href="#2-非数值计算中的计算方法" class="headerlink" title="2 非数值计算中的计算方法"></a>2 非数值计算中的计算方法</h2><p>从计算科学角度数值计算内容主要有：</p><ul><li>矩阵运算。</li><li>线性方程组的求解。</li><li>快速傅里叶变换等。</li></ul><p>非数值计算中的并行算法基本设计策略包括：</p><ul><li>串行算法的直接并行化。</li><li>从问题描述开始设计全新的并行算法。</li><li>借用已有的算法。</li><li>利用已求解问题与待求解问题两者之间的内在相似性来求解新问题。</li></ul><h2 id="3-新型非冯诺依曼计算机体系结构"><a href="#3-新型非冯诺依曼计算机体系结构" class="headerlink" title="3 新型非冯诺依曼计算机体系结构"></a>3 新型非冯诺依曼计算机体系结构</h2><p style="text-indent:2em">传统的冯诺依曼体系结构：第一代计算机（电子管计算机）、第二代计算机（晶体管计算机）、第三代计算机（集成电路计算机）、第四代计算机（大规模超大规模集成电路）</p><p>一些先进新型计算机系统结构：</p><ul><li>微程序控制器设计：<ul><li>设计了“八位运控模型”，并采用了自行提出的“寄存器传输操作语言”进行形式化描述。</li></ul></li><li>直接执行高级语言的计算机：<ul><li>研究了“直接执行的高级语言FORTRAN”机      器，介绍了对标准FORTRAN语言所作的一些限制和补充，简述了该计算机体系结构，列举典型的FORTEAN语言的直接执行过程，并自行提出了可重组结构与之配合。</li></ul></li><li>数据库计算机：<ul><li>研究了数据库计算机，实现RDF查询语言和SQL语言的转换并在此基础上实现一个对用户透明的、建立在关系数据库之上的RDF搜索引擎，以提高其海量存储和查找效率。</li></ul></li><li>光计算机：<ul><li>通过垂直偏振光、水平偏振光和无强光3个稳定的光状态表示信息的三值光计算机原理$^{[7]}$，提出基于光原理三值逻辑计算机。</li></ul></li><li>生物计算机：<ul><li>研究了基于字符串匹配原理的生物序列比对的生物计算机，在纳米计算模型上实现了DNA序列模体发现算法$^{[8]}$。</li></ul></li><li>可重构可变计算机：<ul><li>研究了“可变结构计算机系统”及其结构中的资源间相互通信问题$^{[9]}$。</li></ul></li><li>数据流计算机：<ul><li>根据MIT提出的数据流计算机概念，分析了曼彻斯特大学的数据流计算机，在国内分布式计算计算会议上发表了“数据流计算机体系结构解析”$^{[10]}$一文。</li></ul></li><li>神经计算机：<ul><li>研究神经网络在组合优化中的应用的同时，自行构建了基于Transputer阵列的“通用并行神经网络模拟系统（GP2N2S2）”$^{[10]}$，提供了高级神经网络描述语言及其编辑和编译器的执行环境，实现了程序的自动化执行。</li></ul></li><li>Transpute阵列机：<ul><li>搭建了Transputer阵列机，在中科院计算所进行了组装、调试和运行，并在其上实现了通用的Rohoman应用平台。</li></ul></li><li>量子计算机：<ul><li>在中国科大开展了量子计算研究，讨论量子计算机模型及其物理实现方案、量子计算过程、量子计算模型和量子并行算法，分析量子指数级存储容量和指数加速特征等，并在保密通信、密码安全等领域对量子信息技术进行研究$^{[12, 13]}$。</li></ul></li></ul><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p style="text-indent:2em">大数据、物联网、云计算和区块链是新一代信息技术发展中的华彩乐章。物联网使成千上万的网络传感器嵌入到现实世界中，云计算为物联网产生的海量数据提供了存储空间和在线处理模式，而大数据则让海量数据产生了价值，区块链促进海量信息可靠交互保障生产要素在区域内有序高效地流通。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2018-CS-A General Form of Attribute Exploration </title>
    <link href="/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/"/>
    <url>/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/</url>
    
    <content type="html"><![CDATA[<!-- * @Author: peerless * @Date: 2020-06-28 18:25:20 * @LastEditTime: 2020-06-30 21:27:52 * @LastEditors: Please set LastEditors * @Description: A General Form of Attribute Exploration * @FilePath: \peer-less.github.io\source\_posts\2012CS-A General Form of Attribute Exploration .md   title:         A General Form of Attribute Exploration # 标题   subtitle:                                              # 副标题   date:          2020-06-30                              # 时间   author         peerless                                # 作者   heaeder-img:   img/post-bg-.jpg                        # 这篇文章标题背景图片   catalog:       true                                    # 是否归档   tags:                                                  # 标签      - 学术      - 属性探索         --> <h1 id="A-General-Form-of-Attribute-Exploration"><a href="#A-General-Form-of-Attribute-Exploration" class="headerlink" title="A General Form of Attribute Exploration"></a>A General Form of Attribute Exploration</h1><h2 id="行文思路简要总结"><a href="#行文思路简要总结" class="headerlink" title="行文思路简要总结"></a>行文思路简要总结</h2><p>问题：如何从经典属性探索出发获得通用的属性探索？</p><p>​            1.回顾经典属性探索算法。</p><p>​            2.通过引入3个条件扩展属性探索算法：</p><ul><li>引入两个闭包算子$c_{cert}(A)$与$c_{univ}(A)$。</li><li>不明确指定提供的反例。</li><li>只向专家询问满足$c_{cert}(A)\subsetneq B\subset c_{univ}(A)$的蕴涵式$A\rightarrow B$。</li></ul><p>​            3.对通用属性探索算法进行非冗余性与完备性验证。</p><p>​            4.对通用算法询问蕴涵式时进行改进，使算法向专家询问的蕴涵式的数量是最小的。</p><ul><li>改进方法：若属性集$A=c_{cert}(A)\subsetneq c_{univ}(A)$，则向专家询问蕴涵式$A\rightarrow c_{univ}(A)$。</li></ul><p>总结：</p><p>​            1. 该通用属性算法首先保留了经典属性探索算法的大多性质：完备性、非冗余性和向专家询问蕴涵式的数量最少等。</p><p>​            2. 该通用属性探索算法能够处理抽象给定的闭包算子和部分形式背景下给出的反例。</p><p>问题：</p><ol><li>理论层面：该通用属性探索算法理论上的时间复杂度仍为指数级，无法在多项式时间内计算主基。</li><li>应用层面：该通用属性探索算法并未说明在实际的应用领域中效果如何。</li></ol><h2 id="论文基本信息："><a href="#论文基本信息：" class="headerlink" title="论文基本信息："></a>论文基本信息：</h2><p>​            1.作者：Daniel Borchmann（德累斯顿州立大学数学与科学学院代数研究所丹尼尔·博尔赫曼）</p><p>​            2.期刊：Computer Science</p><p>​            3.时间：2018/11/15</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>提出一种属性探索的一般形式。</li><li>扩展属性探索的适用性。</li><li>将属性探索的现有变种转换为一般形式，简化理论。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>1.属性探索的变种：</p><ul><li>部分形式背景的属性探索$^{[3]}$。</li><li>描述逻辑模型的探索$^{[1, 2]}$。</li></ul><p><font color="red">研究问题：寻找一种将所有变种都包含在内的一般属性探索。</font></p><p>可行原因：</p><pre><code>1.属性探索算法的整体结构均保持不变。2.属性探索的所有重要属性均保留了下来。</code></pre><p>单词积累:</p><pre><code>discourse: 论述、谈话、演讲。</code></pre><p>​         have a close look: 仔细研究。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>蕴涵式<br>$$<br>A\rightarrow B \Leftrightarrow A^{‘}\subseteq B^{‘} \Leftrightarrow B\subseteq A^{‘’}<br>$$</p><p>一些声明：</p><ul><li><p>$Imp(M)$：$M$上的所有蕴涵式集合。</p></li><li><p>$Imp(K)$：形式背景$K$上的所有蕴涵式集合。</p></li><li><p>$Th(K)$：形式背景$K$上的所有成立的蕴涵式集合。</p></li><li><p>$L\subseteq Imp(K)$，$A\subseteq M$。如果对于所有的蕴涵式$(X\rightarrow Y)\in L$，都有$X\subsetneq A或Y\subseteq A$成立，则集合$A$在$L$下为封闭集合。则可做如下定义：<br>  $$<br>  \begin{align}<br>  L^0(A):&amp;=A \newline<br>  L^1(A):&amp;=\bigcup{ {Y\mid (X\rightarrow Y)\in L, X\subseteq A} } \newline<br>  L^i(A):&amp;=L^1(L^{i-1}(A))\ for\ i &gt; 1 \newline<br>  L(A):&amp;=\bigcup\limits_{i\in N}{L^i(A)}<br>  \end{align}<br>  $$<br>  $L(A)$是基于$L$下比集合A大的最小集合。</p><p>  由上可知$L^N(A)$表示集合$A$的子集所能推出来的属性集合的子集所能推出来的属性集合。</p><p>  $Cn(L)$：在$L$下成立的所有蕴涵式集合。</p><ul><li><p>$B\subseteq Imp(K)$在$L$下是非冗余的。$\Leftrightarrow B\subseteq Cn(L)$。</p></li><li><p>$B\subseteq Imp(K)$在$L$下是完备的。$\Leftrightarrow Cn(B)\supseteq L$。</p></li><li><p>$B$是主基。$\Leftrightarrow Cn(B)=Cn(L)$。</p><p>$P$在$L$下是伪闭集，如果以下条件成立：</p></li><li><p>$P\neq L(P)$。</p></li><li><p>对于所有的伪闭集$Q\subsetneq P$有$L(Q)\subseteq P$成立。</p><p>特别的，若$L=Th(K)$，则$P$称为形式背景$K$的伪内涵。则主基可定义为：$Can(L):={P\rightarrow L(P)\mid P 在L下是伪闭集}$。</p><p>由于不知道对象$g$是否具有属性$m$，因此需要引入部分形式背景：存在一个属性集的有序对$(A, B), A, B\in M且A\bigcap B=\varnothing$所构成的集合即为部分形式背景。</p></li><li><p>若$A\bigcup B=M$，则集合称为全局对象描述，即相应对象明确具有的属性集。</p></li><li><p>若$A\bigcup B\neq M$，则集合称为部分对象描述，即相应对象明确不具有的属性集。</p></li></ul></li></ul><h2 id="3-Classical-Attribute-Exploration"><a href="#3-Classical-Attribute-Exploration" class="headerlink" title="3 Classical Attribute Exploration"></a>3 Classical Attribute Exploration</h2><p>$M$是一个有限的属性集合，$K$是基于$M$的形式背景，$p$是基于$M$的领域专家。</p><p>1.对属性集通过字典序进行初始化，并获得第一个属性$\varnothing / P$.</p><p>2.如果$P^{‘}=P$，跳到第5步；否则，令$r:=(P\rightarrow P^{“})$</p><p>3.如果专家认为$r$成立，把$r$加入蕴涵集$Imp(K)$中。</p><p>4.如果专家认为$r$不成立，给出相应的一个反例$C$，将$C$（及其相应的属性）作为新对象加入当前工作形式背景$K$中。</p><p>5.找到字典序$P$的下一个属性集$Q$，若不存在下一个，则算法终止，否则，将$P$设置为$Q$。</p><h2 id="4-Generalizing-Attribute-Exploration"><a href="#4-Generalizing-Attribute-Exploration" class="headerlink" title="4 Generalizing Attribute Exploration"></a>4 Generalizing Attribute Exploration</h2><p><font color="red">推广目的：用更抽象的术语来描述属性探索，以允许该算法在经典属性探索算法之外的应用。</font></p><p>3个扩展：</p><p>1.提出两个闭包操作符：$c_{univ}、c_{cert}$。</p><ul><li><p>$c_{univ}$：我们已知的全部领域知识。$c_{univ}(A)$可以从$A$推出的属性集。</p></li><li><p>$c_{cert}$：我们已知的某些知识。$c_{cert}(A)$确定可以从$A$推出的属性集。</p></li></ul><p>2.采用如下方法对算法进行扩展：</p><ul><li><p>提供反例时，不需要完全指定。</p></li><li><p>只需要所提反例所拥有的信息与所给的蕴涵式相矛盾即可。</p></li><li><p>提供关于该对象具有哪些属性以及不具有的属性信息即可。</p></li></ul><p>3.我们向专家提出的蕴涵式是一种特殊的形式：</p><ul><li>搜索关于$c_{cert}$和$c_{univ}$未确定的蕴涵式$A\rightarrow B$，即$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$。对于这样的蕴涵式，我们不能从$c_{cert}$和$c_{univ}$推断出属性集$c_{univ}(A) \backslash B$是否能从$A$推出或不能推出，因此，我们需要向专家询问。</li></ul><p>Algorithm(General Attribute Exploration)</p><p>   $q$为部分领域专家。  </p><p>   ​    1. $K = \varnothing$<br>   ​    2. 对于有限属性集$A\subseteq M$，若存在有限属性集$B$，有$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$成立，则考虑蕴涵式$A\rightarrow B$；如果不存在，则算法终止，输出$K$与$c_{cert}$。</p><p>   ​    3. 若$q$认为$A\rightarrow B$成立，那么更新$c_{cert}^{‘}=X\longmapsto c_{cert}(L(c_{cert}(X) ) )$。</p><p>​        4. 否则，$(C, D)=q(A\rightarrow B)$作为反例，加入形式背景$K$。</p><p>​        5. 对所有的反例$(C, D)$有：<br>$$<br>\begin{align}<br>C’:&amp;=c_{cert}(C) \newline<br>D’:&amp;=D\bigcup { {m\in M\backslash D\mid c_{cert}(C\bigcup {m})\bigcap D\neq \varnothing}}<br>\end{align}<br>$$<br>​        6. 对所有的$X\subseteq M，X\longmapsto c_{univ}(X)\bigcap K(X)$，即将$c_{univ}$更新为$c_{univ}^{‘}(X)=c_{univ}(X)\bigcap K(X)$。</p><p>​        7. 跳转到2。</p><p>两条性质：非冗余性与完备性。</p><h2 id="5-Computing-Undecided-Implications"><a href="#5-Computing-Undecided-Implications" class="headerlink" title="5 Computing Undecided Implications"></a>5 Computing Undecided Implications</h2><p>目的：使向专家询问蕴涵式的数量是最小的。</p><p>原因：该通用属性探索算法对询问蕴涵式的顺序未做约束。（可能询问以确定的蕴涵式）</p><p>经典属性探索下计算未确定的蕴涵式：</p><p>​        在基于已知蕴涵式$k$的条件下，计算属性集$P$字典序之后的最小属性集$Q(Q\subseteq M，且Q不是当前工作形式背景的内涵)$。那么就需要向专家询问蕴涵式$Q\rightarrow Q^{“}$。</p><p>通用属性探索下计算未确定的蕴涵式：</p><p>​        为了保证向专家询问的蕴涵式的数量是最小的，将通用属性探索算法的第二步更改为：$对于有限属性集A\subseteq M$，有$A=c_{cert}(A)\subsetneq c_{univ}(A)$并且$A$是$\subseteq-minimal$成立，则考虑蕴涵式$A\rightarrow c_{univ}(A)$。</p><p>在给出算法总是能得到最小数量的询问蕴涵式前，先给出如下定义：</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h2><p style="text-indent:2em">从使用领域专家的经典的属性探索中推导出一种更加通用的属性探索算法。它能够处理抽象给定的闭包算子，并且可以处理部分给定的反例。</p><h2 id><a href="#" class="headerlink" title></a></h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
