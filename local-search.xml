<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2020-IEEE TKDE-Semi-supervised Concept Learning by Concept-cognitive Learning and Concept Space</title>
    <link href="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/"/>
    <url>/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/</url>
    
    <content type="html"><![CDATA[<h1 id="Semi-supervised-Concept-Learning-by-Concept-cognitive-Learning-and-Concept-Space"><a href="#Semi-supervised-Concept-Learning-by-Concept-cognitive-Learning-and-Concept-Space" class="headerlink" title="Semi-supervised Concept Learning by Concept-cognitive Learning and Concept Space"></a>Semi-supervised Concept Learning by Concept-cognitive Learning and Concept Space</h1><div class="note note-primary">            <p>半监督学习、概念认知学习、增量学习</p>          </div> <a id="more"></a><!-- <p class='note note-info'>论文、属性探索</p> --><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><pre><code>        1. 作者：Yunlong Mi, Wenqi Liu, Yong Shi, and Jinhai Li。           2. 期刊：IEEE Transactions on Knowledge and Data Engineering(IEEE TKDE)。              1. SCI： 2区。</code></pre><p>​                  2. CCF： A类。<br>​                  3. IF：4.935。</p><p>​            3. 时间：2020/7/21。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol><li>提出了一种新的基于概念空间的动态半监督学习方法(SSL)—半监督概念学习方法(S2CL)，该方法利用层次概念来表示知识。</li><li>为了充分利用全局和局部的概念信息，进一步提出一种扩展版本的S2CL(即${S2CL}^{\alpha}$)用于概念学习。</li></ol><ul><li>给出了基于正则决策形式背景的$S2CL$与${S2CL}^{\alpha}$一些新的相关理论。</li><li>设计了一个新的SSL框架，并给出了相应的$S2CL$与${S2CL}^{\alpha}$算法。</li><li>在不同的数据集上进行实验(包括概念分类和大量未标注数据下的增量学习)，验证方法的有效性。</li></ul><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><ol><li>由于许多实际任务仍然缺乏足够数量的标注数据，这导致传统的监督学习方法在这些任务中性能较差。</li></ol><p>因为==不能充分利用大量未标注数据背后的信息==，这意味着采用半监督假设与数据的分布一致时，==半监督学习(SSL)可以提高学习精度$^{[1]}$==。</p><p>2.为了充分利用未标注数据背后的信息，人们从不同的角度提出了多种将未标注数据集成到不同的分类器的SSL方法$^{[2-7]}$。</p><p>3.为了实现对少量已标注数据集的动态分类，需要对标准的SSL算法做出重大改变以获得更好的性能。</p><p>然而 ，存在==两个主要问题==：</p><ul><li>重构经典的SSL算法以适应新增的数据并非易事。</li><li>经典的SSL算法对数据敏感，无法在增量过程中利用标记信息，这将会影响SSL算法的性能。</li></ul><p>因此==通过更新概念空间来解决动态过程的概念认知学习(CCL)$^{[8,9]}$==可能是一个不错的选择。一般从概念学习$^{[10]}$、粒计算(GrC)$^{[11-13]}$和动态过程三个方面进行讨论。</p><p>概念学习：人们从不同的角度研究了一些未知概念的学习。</p><ul><li>从不同层次$^{[14]}$解释解释概念学习。</li><li>从认知角度$^{[15,16]}$获取概念。</li><li>通过构建不同的概念体系$^{[17-19]}$获得新概念。</li><li>通过概念格$^{[20,21]}$中的正反例实现基于概念的学习。</li></ul><p>粒计算(GrC)：表示数据集通常可以划分为不同的粒度以满足不同的需求，并已融合入各种领域。如形式概念分析(FCA)$^{[22,23]}$、粗糙集$^{[24]}$、大数据$^{[25,26]}$、机器学习$^{[8,25]}$和概念学习$^{[8,9,15,27,28]}$。</p><p>动态过程：数据挖掘的重要步骤之一$^{[26]}$，已存在各种增量学习方法$^{[7,29-34]}$。CCL就是这样一种方法，它以概念为知识载体，通过模仿人类的学习过程，自然地完成增量学习任务。                                                                                                                                                                                         </p><p>4.到目前为止，已经从认知关系的CCL$^{[37]}$和近似CCL$^{[9,15]}$等不同的角度提出了不同的相关的CCL系统$^{[14,35,36]}$。</p><p>特别地，为了获得动态环境下的泛化能力，提出了一种基于正则决策形式背景$^{[8,27]}$的CCL模型(CCLM)。然而这些相关的CCL系统都==缺乏概念分类能力==。</p><p>例如：如何将GrC与认知过程和MapReduce框架相结合来学习近似概念$^{[9]}$。</p><p>​            提出了一个泛化的CCL框架，还需要进一步的研究$^{[8]}$。</p><p>同时，当前的CCLM及其扩展版本(即C3LM)也无法解决SSL的问题。            </p><p>另外，基于概念的正反例学习方法$^{[20,21]}$由于主要考虑静态学习的情况，还不能实现动态分类任务。</p><p>因此==如何将CCL引入SSL来实现增量学习==是本研究的一个挑战。</p><p>5.结构化知识在人类知识组织中起重要的作用，许多关于知识表示的研究都致力于这一主题。</p><p>如知识层次结构$^{[39]}$、知识粒度结构$^{[14]}$以及基于概念层次的概念聚类$^{[40,41]}$。</p><p>此外，FCA$^{[42]}$通过层次化的概念结构来组织知识，为结构化知识表示和概念学习提供了一种有用的方法。换句话说就是，在FCA$^{[43,44]}$中构建分层概念结构时，允许从数据构建中间知识。</p><p>6.概念聚类试图提供一个或几个概念作为对所获得的聚类的解释$^{[45]}$，它由两个最主要的任务组成，即概念分类和概念发现$^{[46]}$。</p><p>但与[41]类似，本文只关注==层次概念结构在用于概念分类的特殊概念聚类(即FCA)中的作用==。</p><p>7.事实上，为了满足不同的需求，FCA已经被广泛地集成到概念聚类中。</p><p>例如：因子空间的概念形成$^{[47]}$、构建概念层次$^{[48]}$、自动本体生成$^{[48]}$、以及基于案例的分层学习$^{[41]}$。</p><p>8.同时，S2CL是一种借助FCA中知识层次结构的增量式概念学习算法，它不同于侧重于对单链接层次聚类的稳定性和收敛性进行公理化描述的稳定聚类方法$^{[49]}$。</p><p>9.这些关于聚类方法的相关研究，如层次聚类方法$^{[49]}$和基于密度的流聚类算法$^{[50]}$，进一步证明了聚类过程将有利于知识发现，以及将概念聚类引入到我们方法中的可行性。</p><p>然而，就我们所知，基于概念结构表征的半监督概念学习还没有相关的研究。</p><p>因此，如何==通过概念聚类将概念结构信息集成到SSL==中是我们工作中的另一个挑战。</p><p>10.本文提出了一种新的用于SSL的半监督概念学习模型(S2CL)，为实现具有层次化概念知识的复杂分类任务提供参考。此外，为了提高分类性能，我们的方法(包括S2CL及其扩展版本${S2CL}^{\alpha}$)还加入了概念结构信息和Top-K集相似度$^{[51-53]}$。</p><p>本文主要贡献如下：</p><ol><li>首先提出了一种新的基于正则决策形式背景的SSL理论，并将CCL和概念聚类相结合，提出了一种增量SSL的新框架。</li><li>我们的方法不是直接将所有未标记的数据输入到模型中，而是通过模仿人类的认知机制来自然地完成动态过程。这样可以避免了昂贵的计算。换言之，所提出的方法也可以被称为增量学习方法，它可以随着时间的推移进行更新，而不需要重构。</li><li>最后，与一些以特征向量表示为特征的标准SSL方法相比，我们的方法是基于结构化知识表示库(即概念空间)设计的。此外，我们在真实数据上的实验表明，所提出的方法能够在少量标注数据的情况下获得良好的概念分类和增量学习性能。</li></ol><h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2 RELATED WORK"></a>2 RELATED WORK</h2><h3 id="2-1-Related-Notations"><a href="#2-1-Related-Notations" class="headerlink" title="2.1 Related Notations"></a>2.1 Related Notations</h3><p>Instance $x$：一个对象$x$，信息系统的输入项。</p><p>Class $y$：正则决策形式背景的决策属性值。</p><p>Dataset $X = \{ (x_i, y_i) \}_{i=1}^{n}$：非空有限对象集合G。</p><p>Label set $\mathcal{Y} = \{ 1,2, \cdots, l \}$：正则决策形式背景的决策属性集D。</p><p>Labeled dataset $S_L = \{ (x_i, y_i) \}_{i=1}^{l^{\prime}}$：正则决策形式背景中具有决策属性信息的对象集。</p><p>Unlabeled dataset $S_U = \{ x_j \}_{j = l^{\prime} + 1}^{m}$：正则决策形式背景中不具有决策属性信息的对象集。</p><h3 id="2-2-CCL"><a href="#2-2-CCL" class="headerlink" title="2.2 CCL"></a>2.2 CCL</h3><p>G：对象集，M：属性集。</p><p>$2^G$：对象幂集，$2^M$：属性幂集。</p><p>两个值映射：$\mathcal{F}: 2^G \rightarrow 2^M$ 和 $\mathcal{H}: 2^M \rightarrow 2^G$。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027174249832.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027174249832.png" srcset="/img/loading.gif" alt></p><p>考虑新型冠状病毒确诊患者的共性症状，即研究对象是确诊患者，属性为新型冠状病毒的感染症状。</p><p>(i) $X_1 \subseteq X_2 \Rightarrow  \mathcal{F}(X_2) \subseteq \mathcal{F}(X_1)$，即确诊患者越多，其共性症状越少。</p><p>(ii) $\mathcal{F}(X_1) \cap \mathcal{F}(X_2)  \subseteq \mathcal{F}(X_1 \cup X_2)$，即对于寻找患者的共性症状，整体识别比分开依次完成更容易实现。</p><p>可以理解为，将所有患者集中观察其共同症状要比分开观察更容易。</p><p>(iii) $\mathcal{H}(A) = \{ x\in G\ |\ A\subseteq \mathcal{F}({x})  \}$，即对于疑似病例，只有具备关键的共性症状才需要进一步判断其是否为新型冠状病毒患者。</p><p>==粒概念==(简单概念)：对于任意的$x\in G$ 和 $a\in M$，有序对$(\mathcal{HF}(x), \mathcal{F}(x))$和$(\mathcal{H}(a), \mathcal{FH}(a))$被称为认知算子$\mathcal{F}$和$\mathcal{H}$下的粒概念。</p><p>注意：==在CCL系统中使用概念而不是对象作为信息载体==。</p><p>==概念空间==(所有粒概念的集合)：$G_{\mathcal{FH}} = \{ (\mathcal{HF}(x), \mathcal{F}(x))\ |\ x\in G \} \cup \{ (\mathcal{H}(a), \mathcal{FH}(a))\ |\ a\in M \}$。</p><p>由于CCL系统中的学习过程通常是在动态环境下完成的。这意味着在第$i-1$阶段的对象集$G_{i-1}$中输入新的对象可以得到第$i$阶段的对象集$G_i$。其中$\mathcal{F}_{i-1}$和$\mathcal{H}_{i-1}$分别为第$i-1$阶段的认知算子，相应的概念空间为$G_{\mathcal{F_{i-1}H_{i-1}}}$。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027194043193.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027194043193.png" srcset="/img/loading.gif" alt></p><p>由定义1可以获得认知算子$\mathcal{F}$和$\mathcal{H}$下的初始概念空间。</p><p>由定义2可以基于当前概念空间完成学习过程。</p><h3 id="2-3-Regular-Formal-Decision-Context"><a href="#2-3-Regular-Formal-Decision-Context" class="headerlink" title="2.3 Regular Formal Decision Context"></a>2.3 Regular Formal Decision Context</h3><p>G：对象集，D：决策属性集。</p><p>$2^G$：对象幂集，$2^D$：决策属性幂集。</p><p>两个值映射：$\widetilde{\mathcal{F}}: 2^G \rightarrow 2^D$ 和 $\widetilde{\mathcal{H}}: 2^D \rightarrow 2^G$。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027201610790.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027201610790.png" srcset="/img/loading.gif" alt></p><p>实际上，性质1表明，当对象$x$满足$X_1 = \mathcal{HF}(x)$和$K = \mathcal{FH}(k)$时，对象$x$可以由单个标签$k$来标注。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027204535376.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027204535376.png" srcset="/img/loading.gif" alt></p><p>由于决策子属性集合$D_k$在正则决策形式背景下包含单个标记信息。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027204842657.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027204842657.png" srcset="/img/loading.gif" alt></p><p>定义4说明，一个对象集可以根据决策属性信息分解成多个子对象集。</p><p>此外，根据范畴表示公理$^{[54]}$，我们有充分的理由相信</p><ol><li>每个子概念空间由不同的概念组成，</li><li>其中不同的概念可以通过标签集合$\mathcal{Y}$中不同类别的标签来构成。</li></ol><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027210916523.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027210916523.png" srcset="/img/loading.gif" alt></p><p>因此，下文只讨论正则决策形式背景下与单个类标签相关联的子概念空间的情况。</p><p>尽管如此，但是需要指出的是，上述讨论是建立在一个强有力的假设的基础上的：所有样本均有正确的标签信息。</p><p>也就是说，它们不能直接处理没有考虑到标签信息的SSL问题。因此，探索一些新的理论来应对半监督概念学习是非常必要的。</p><h2 id="3-PROPOSED-S2CL"><a href="#3-PROPOSED-S2CL" class="headerlink" title="3 PROPOSED S2CL"></a>3 PROPOSED S2CL</h2><p style="text-indent:2em">我们首先在3.1小节中介绍有标签数据的初始概念空间，然后在3.2小节、3.3小节和3.4小节中分别介绍无标签数据的概念认知过程、S2CL的概念识别和理论分析。最后，为了更清楚地理解建议的S2CL，我们给出了它的整个过程，并在第3.5节讨论了计算成本。</p><h3 id="3-1-Concept-Space-with-Structural-Information"><a href="#3-1-Concept-Space-with-Structural-Information" class="headerlink" title="3.1 Concept Space with Structural Information"></a>3.1 Concept Space with Structural Information</h3><p>与标记$k$相关联的正则子对象决策形式背景</p><p>子对象集$G^k$下的条件子对象认知算子：$\mathcal{F^k}, \mathcal{H^k}$。</p><p>子对象集$G^k$下的决策子对象认知算子：$\mathcal{\widetilde{F}^k}, \mathcal{\widetilde{H}^k}$。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027212152476.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027212152476.png" srcset="/img/loading.gif" alt></p><p> 条件子对象形式背景下的面向对象的条件概念与面向属性的条件概念</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027212857576.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027212857576.png" srcset="/img/loading.gif" alt></p><p>条件子对象形式背景下的面向对象的条决策概念与面向属性的决策概念</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027213053996.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027213053996.png" srcset="/img/loading.gif" alt></p><p>正则子对象决策形式背景下的条件概念空间和决策概念空间</p><script type="math/tex; mode=display">\begin{align*}G_{\mathcal{F^k H^k}} = &\ OG_{\mathcal{F^k H^k}} \cup AG_{\mathcal{F^k H^k}} \newline= &\ \{ (\mathcal{H^k F^k}(x), \mathcal{F^k}(x))\ |\ x\in G^k  \} \cup \{ (\mathcal{H^k}(a), \mathcal{F^k H^k}(a))\ |\ a\in M \} \newline\newlineG_{\mathcal{\widetilde{F}^k \widetilde{H}^k}} = &\ OG_{\mathcal{\widetilde{F}^k \widetilde{H}^k}} \cup AG_{\mathcal{\widetilde{F}^k \widetilde{H}^k}} \newline= &\ \{ (\mathcal{\widetilde{H}^k \widetilde{F}^k}(x), \mathcal{\widetilde{F}^k}(x))\ |\ x\in G^k  \} \cup \{ (\mathcal{\widetilde{H}^k}(k^{\prime}), \mathcal{\widetilde{F}^k \widetilde{H}^k}(k^{\prime}))\ |\ k^{\prime}\in D \}\end{align*}</script><p>这意味着可以利用面向对象的概念和面向属性的概念来构造子对象集$G^k$的概念空间。</p><p>条件概念空间与决策概念空间中概念的构成。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027222113798.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027222113798.png" srcset="/img/loading.gif" alt></p><p>概念空间：$G_{\mathcal{F^k H^k}}^{\diamond}$和$G_{\mathcal{\widetilde{F}^k \widetilde{H}^k}}^{\diamond}$。</p><p>面向属性的条件概念空间：$AG_{\mathcal{F^k H^k}}$</p><p>面向属性的决策概念空间：$AG_{\mathcal{\widetilde{F}^k \widetilde{H}^k}}$。</p><p>同时，初始化：$G_{\mathcal{F^k H^k}}^{\diamond} = AG_{\mathcal{F^k H^k}}$和$G_{\mathcal{\widetilde{F}^k \widetilde{H}^k}}^{\diamond} = AG_{\mathcal{\widetilde{F}^k \widetilde{H}^k}}$。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027234758350.png" srcset="/img/loading.gif" class><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027234820257.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027234758350.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201027234820257.png" srcset="/img/loading.gif" alt></p><p>性质2说明，当$\mathcal{F^k}(x) = \mathcal{F^k H^k}(a)$和$\mathcal{\widetilde{F}^k}(x) = \mathcal{\widetilde{F}^k \widetilde{H}^k}(k^{\prime})$时，我们不需要向文献[27]那样构造概念$(\mathcal{H^k F^k}(x), \mathcal{F^k}(x))$和$(\mathcal{\widetilde{H}^k \widetilde{F}^k}(x), \mathcal{\widetilde{F}^k}(x))$，因此，利用这种方法最终我们可以得到$G_{\mathcal{F^k H^k}} = G_{\mathcal{F^k H^k}}^{\diamond}$和$G_{\mathcal{\widetilde{F}^k \widetilde{H}^k}} = G_{\mathcal{\widetilde{F}^k \widetilde{H}^k}}^{\diamond}$</p><p>为了方便，将带标记的数据集$S_L$以$G_0$指定，则==初始的概念空间为$G_{\mathcal{F_0 H_0}}$和$G_{\mathcal{\widetilde{F}_0 \widetilde{H}_0}}$==。</p><p>初始的概念空间阶段，如果将对象集$G^k$以$G_0^k$代替，则相应的认知算子$\mathcal{F}_0^k,\mathcal{H}_0^k$和$\mathcal{\widetilde{F}_0^k}, \mathcal{\widetilde{H}_0^k}$。</p><h3 id="3-2-Cognitive-Process-with-Unlabeled-Data-in-Concept-Learning"><a href="#3-2-Cognitive-Process-with-Unlabeled-Data-in-Concept-Learning" class="headerlink" title="3.2 Cognitive Process with Unlabeled Data in Concept Learning"></a>3.2 Cognitive Process with Unlabeled Data in Concept Learning</h3><p>在概念认知的过程中，假设通过新增加一个新的对象而不是多个对象来对更新概念空间。</p><p>每一步只增加一个对象$x$：$\triangle G_i = \{ x_i \}$。</p><p>未标记的数据集$S_U$：$\triangle G = \{ \triangle G_0, \triangle G_1, \cdots, \triangle G_{n-1} \} = \{ x_0, x_1, \cdots, x_{n-1} \}$。</p><p>不同于文献[27]，由于没有标记信息，因此我们假定：一个对象$x$与一个虚拟的标记$k^*$相关联。</p><p>然后对新输入的数据$\triangle G_{i-1}^{k^<em>} = G_{i}^{k^</em>} - G_{i-1}^{k^*}$使用条件子对象认知算子和决策子对象认知算子则有：</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028003142792.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028003142792.png" srcset="/img/loading.gif" alt></p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028003548523.png" srcset="/img/loading.gif" class><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028003634392.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028003548523.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028003634392.png" srcset="/img/loading.gif" alt></p><p>定理2说明了在添加实例时如何更新概念空间。</p><p>但是概念识别是非常困难的，因为我们不能直接识别每个实例$x$的真实类别标签。也就是说，与最初的概念空间生成不同的是，当输入没有标签信息的新对象时，哪个子概念空间会被更新仍然是一个谜。</p><h3 id="3-3-Concept-Cognition"><a href="#3-3-Concept-Cognition" class="headerlink" title="3.3 Concept Cognition"></a>3.3 Concept Cognition</h3><p>对于新输入的对象$x$，由于$|\triangle G_i^{k^<em>}| = 1$，所以概念$(\mathcal{H}_{\triangle G_i^{k^</em>}}^{k^<em>} \mathcal{F}_{\triangle G_i^{k^</em>}}^{k^<em>}(x), \mathcal{F}_{\triangle G_i^{k^</em>}}^{k^<em>}(x)) = (\{ x \}, \mathcal{F}_{\triangle G_i^{k^</em>}}^{k^*}(x))$</p><p>同时，为了满足大量的未标记数据的需求，提出了一种新的概念学习相似度度量方法。事实上，良好的概念评估相似性是S2CL成功的关键。</p><p>状态(i-1)下的全局信息与局部信息。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028005546979.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028005546979.png" srcset="/img/loading.gif" alt></p><p>更一般地，考虑处于(i-1)状态下的整个概念空间。</p><p>状态(i-1)下的全局信息和局部信息为</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028005958929.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028005958929.png" srcset="/img/loading.gif" alt></p><p>新输入的概念$C = (\{ x \}, \mathcal{F}_{\triangle G_i^{k^<em>}}^{k^</em>}(x))$与任意已知概念间得到概念相似度(concept similarity(CS))为</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028010738594.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028010738594.png" srcset="/img/loading.gif" alt></p><p>事实上，当α=0.5时，等式(15)退化为Jaccard相似度$^{[39]、[55]}$。</p><p>根据样本分离公理$^{[54]}$，对于任何实例，总是存在与其最相似的唯一类。</p><p>因此，对于实例$x$，类向量可以通过如下步骤获得：</p><ol><li><p>先计算出给定概念与一个子概念空间任意概念的相似度，并从每个子概念空间中选取该概念空间最大的概念相似度。</p></li><li><p>构成最大的类向量来评估类分布。平均类向量类似。</p></li></ol><p>注意：通过将概念认知过程与结构化概念相似度$θ_j$相结合而设计的SSL方法被称为半监督概念学习方法，为了方便起见，将其缩写为S2CL。同时，通过充分利用概念空间中的全局和局部概念信息(即结构概念相似度$θ_j^I$)，进一步提出了S2CL的扩展版本。为简洁起见，在不存在混淆的情况下，我们也将其写为${S2CL}^{\alpha}$。</p><h3 id="3-4-Theoretical-Analysis"><a href="#3-4-Theoretical-Analysis" class="headerlink" title="3.4  Theoretical Analysis"></a>3.4  Theoretical Analysis</h3><p>从本质上讲，α主要反映了集合$A^∗−B_j$和集合$B_j−A^∗$中不同特征对整体概念相似度的影响。因此，讨论如何在每个数据集上选择合适的α是非常重要的。</p><p>状态(i-1)下不同$\alpha_r$的概念空间为</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028012736354.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028012736354.png" srcset="/img/loading.gif" alt></p><p>状态(i-1)下概念相似度为</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028013322891.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028013322891.png" srcset="/img/loading.gif" alt="image-20201028013322891"></p><p>此外，受文献[54]的启发，给定概念$C_i$与类空间$G_{\mathcal{F_{i-1},H_{i-1}}}^{\alpha_r,k}$之间的相似度为</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028013810551.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028013810551.png" srcset="/img/loading.gif" alt></p><p>根据Top-K$^{[51]}$集合相似度得到的目标函数为</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028014057792.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028014057792.png" srcset="/img/loading.gif" alt></p><p>我们的目标是利用概念结构信息获取最优的概念空间。</p><h3 id="3-5-Framework-and-Computational-Complexity-Analysis"><a href="#3-5-Framework-and-Computational-Complexity-Analysis" class="headerlink" title="3.5 Framework and Computational Complexity Analysis"></a>3.5 Framework and Computational Complexity Analysis</h3><p>为简明起见，我们可以认为有三类需要预测，S2CL的整个学习过程</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028081317553.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028081317553.png" srcset="/img/loading.gif" alt></p><p>1.1我们首先获得相应的正规形式决策形式背景。然后，基于认知算子构造具有概念结构信息的初始概念空间(包括条件概念空间及其对应的决策概念空间)。</p><p>具体地说，条件概念空间包含三个子概念空间，每个子概念空间由不同的概念组成。在条件概念空间中存在三个子概念空间，对应于三个类别，每个子概念空间包含两种不同类型的概念，即面向对象的条件概念(红色)和面向属性的条件概念(黑色)。</p><p>1.2如图1的第一阶段所示，每个子概念空间还与相应的决策概念空间中的一个决策概念相关联。</p><p>1.3对于任何新输入的未标注数据，首先利用它们形成概念，然后通过概念识别来完成概念认知过程。</p><p>1.4S2CL(或$S2CL^α$)试图根据不同参数$α_r(r=1，2，\cdots, n)$下的概念识别和概念认知过程来学习最优概念空间。换言之，S2CL(或$S2CL^α$)的目标是通过概念认知过程寻找一个合适的概念空间来表示潜在的数据分布。</p><p>2.在预测阶段，给定一个实例，最终的概念空间可以通过使用CS度$θ_j$(或$θ_j^I$)产生两个类分布估计(包括最大类向量和平均类向量)。然后将两个三维类向量相加得到最终的CS度向量，输出数值最大的类，如图1所示。</p><h3 id="3-6-Algorithm-Of-S2CL-And-S2CL-alpha"><a href="#3-6-Algorithm-Of-S2CL-And-S2CL-alpha" class="headerlink" title="3.6 Algorithm Of $S2CL$ And ${S2CL}^{\alpha}$"></a>3.6 Algorithm Of $S2CL$ And ${S2CL}^{\alpha}$</h3><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201101152715584.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201101152715584.png" srcset="/img/loading.gif" alt></p><p>step3: 生成初始的概念空间。</p><p>step4-8: 概念识别和概念认知过程。</p><p>step9-12: 最终的预测。</p><p>如果预测值$\hat{k}$与真实标签一致，则S2CL算法是正确的。</p><p>数据集T上的精度为: $acc = \frac{N}{|T|}$，$N$表示正确预测的数目。</p><p>S2CL算法1的例子</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201103205342992.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201103205342992.png" srcset="/img/loading.gif" alt></p><p>K = 1, ${\alpha}_r = 0.5$</p><p>则概念分类流程如下:</p><ol><li>初始概念空间$G_{\mathcal{F_0 H_0}}$和$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}$.</li></ol><p>条件概念空间$G_{\mathcal{F_0 H_0}}^{0.5, 1} = \{ (1, adgjmp), (2, bdgjmp), (12, djmp) \}, \qquad G_{\mathcal{F_0 H_0}}^{0.5, 2} = \{ (3, afijoq), (4, cehkmp) \}$. </p><p>决策概念空间$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 1} = \{ (12, d_1) \}, \qquad G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 2} = \{ (34, d_2) \}$</p><p>由于${\alpha}_r = 0.5$,则条件概念空间$G_{\mathcal{F_0 H_0}}$和决策概念空间$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}$可写为$G_{\mathcal{F_0 H_0}}^{0.5}$和$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5}$</p><p>而条件概念空间$G_{\mathcal{F_0 H_0}}^{0.5} = \{ G_{\mathcal{F_0 H_0}}^{0.5, 1}, G_{\mathcal{F_0 H_0}}^{0.5, 2} \}$,决策概念空间$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5} = \{ G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 1}, G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 2} \}$.</p><ol><li>给出第5个实例,则会产生两个概念$(5, begjnp)$和$(5, d_1/d_2)$,记$C_1 = (5, begjnp)$,则$C_1$与$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 1}$之间的概念相似度为</li></ol><p>$Sim(C_1, G_{\mathcal{F_0 H_0}}^{0.5, 1}) = \{ Sim(C_1, C_j^{0.5}) \}_{j=1}^3 = \{0.333, 0.333, 0.250\}$. </p><p>则最大的CS度为</p><p>$\hat{\theta}_1 = \max \{ 0.333, 0.333, 0.250\} = 0.333$</p><p>$\overline{\theta}_1 = avg \{ 0.333, 0.333, 0.250\} = 0.305$</p><p>记$C_j^{0.5, 2} = \{ (3, afijoq), (4, cehkmp) \}$,则</p><p>$Sim(C_1, G_{\mathcal{F_0 H_0}}^{0.5, 2}) = \{ Sim(C_1, C_j^{0.5, 2}) \}_{j=1}^2 = \{0.091, 0.200 \}$.</p><p>$\hat{\theta}_2 = \max \{ 0.091, 0.200 \} = 0.200$</p><p>$\overline{\theta}_2 = avg \{ 0.091, 0.200 \} = 0.145$</p><p>则$(\hat{\theta}_1, \hat{\theta}_2)^{\top} = (0.333, 0.200)^{\top},\ \qquad (\overline{\theta}_1, \overline{\theta}_2)^{\top} = (0.305, 0.145)$</p><p>则可将第5个实例归类到类别1,即产生概念$(5, d_1)$.</p><ol><li>更新概念空间,条件概念空间写为$G_{\mathcal{F_0 H_0}}^{0.5}$和决策概念空间$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5}$,则</li></ol><p>条件概念空间$G_{\mathcal{F_0 H_0}}^{0.5, 1} = \{ (1, adgjmp), (2, bdgjmp), (12, djmp), (5, begjnp), (15, gjp), (25, bjp) \}, \qquad G_{\mathcal{F_0 H_0}}^{0.5, 2} = \{ (3, afijoq), (4, cehkmp) \}$</p><p>决策概念空间$G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 1} = \{ (125, d_1) \}, \qquad G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 2} = \{ (34, d_2) \}$</p><p>则,最终的概念空间,条件概念空间$G_{\mathcal{F_6 H_6}}^{0.5} = \{ G_{\mathcal{F_0 H_0}}^{0.5, 1}, G_{\mathcal{F_0 H_0}}^{0.5, 2} \}$,决策概念空间$G_{\mathcal{\widetilde{F_6} \widetilde{H_6}}}^{0.5} = \{ G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 1}, G_{\mathcal{\widetilde{F_0} \widetilde{H_0}}}^{0.5, 2} \}$.</p><ol><li>对于剩余的测试实例6-11,</li></ol><p>对于测试实例6,产生概念$(6, aegjnq)$</p><p>则 $\hat{k} = argmax _{k\in \mathcal{Y}}{\frac{|N_{k}^{\alpha_r}(C_j)|}{K}} $  </p><ol><li>产生概念</li></ol><p>同时易于通过使用==概念结构相似度替代算法1的step 6==得到算法${S2CL}^{\alpha}$</p><p>S2CL算法的时间复杂度主要由两部分组成：初始概念空间的构建和具有概念结构信息的概念认知过程。</p><p>设构造一个概念、计算CS度和更新概念空间的复杂度分别为$O(t_1), O(t_2), O(t_3)$。</p><p>则，step 3的时间复杂度为$O(t1|S_L|(|M|+|D|))$，通过概念识别完成概念认知过程的的复杂度为$O(|S_U|(t1+t2+t3))$。</p><p>注意，因为所提出的方法是逐个输入对象来更新，所以CCL是一个增量学习过程。因此S2CL也可以看作是动态环境下的一种SSL增量方法。</p><p>为了简便，从$S_U$中随机选择实例$E$和$C$分别作为增量学习步骤和每个增量学习步骤的实例。</p><p>因此，增量学习算法2的时间复杂度为$O(E|C|(t_1+t_2+t_3)+|T|)$。算法1和算法2的显著区别在于，算法2将在每个增量学习步骤中进行预测过程，这比算法1更耗时。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201101162323691.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201101162323691.png" srcset="/img/loading.gif" alt></p><h2 id="4-EXPERIMENTAL-RESULTS"><a href="#4-EXPERIMENTAL-RESULTS" class="headerlink" title="4 EXPERIMENTAL RESULTS"></a>4 EXPERIMENTAL RESULTS</h2><h3 id="4-1-Experimental-Setting"><a href="#4-1-Experimental-Setting" class="headerlink" title="4.1 Experimental Setting"></a>4.1 Experimental Setting</h3><p>将我们提出的两个算法$S2CL$和${S2CL}^{\alpha}$与各种流行的学习算法进行了比较，共有3类</p><p>对于概念分类：</p><ol><li>标准的有监督学习算法：SVM with the Gaussian kernel function，K-Nearest Neighbor(KNN)，Naive Bayes(NB)。</li><li>流行的半监督学习算法：semi-supervised SVM self-training(SVM-self)，Label Propagation(LP)，Label Spreading(LS)，以及最先进的SSL方法即TriTraining [6]， CoForest [59] and CoBC[60]。</li></ol><p>对于增量概念学习：我们将SVM-self，LP， LS,，增量SVM(ISVM)和增量SGD(ISGD)与所提出的S2CL进行的实验比较。</p><p>实验设备：Intel Core i5-2400 @3.10GHz CPU and 4GB main memory，我们算法用Java实现。其他算法直接从Sklearn<i><a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">https://scikit-learn.org/stable/</a></i>或KEEL<i><a href="https://sci2s.ugr.es/keel/download.php" target="_blank" rel="noopener">https://sci2s.ugr.es/keel/download.php</a></i>软件调用。</p><p>同时，在我们实验中采用了SVM-Self的高斯核和ISVM的线性核。</p><p>每种学习方法的参数设置：</p><ol><li>高斯核的参数$C$和$\gamma$在网格$[2^{-8}, 2^{-7}, \cdots, 2^8]$搜索。</li><li>对于LP和LS，根据不同的数据集调整最大迭代次数，以保证尽可能快的收敛。</li><li>对于TriTraining，实验中使用了3个C4.5分类器，CoForest的分类器数量和阈值分别为6个和0.75。</li><li>CoBC的参数主要包括3部分：<ol><li>committee members: 3。</li><li>ensemble learning: bagging。</li><li>base learner: C4.5。</li></ol></li><li>我们方法的参数$K\in \{1, 2, \cdots, 10\}$，${\alpha}_r\in \{ 0.0, 0.1, \cdots, 1.0 \}$。</li><li>另外，在Sklearning和KEEL软件工具中默认使用其他参数。</li></ol><h3 id="4-2-Dataset-Setting"><a href="#4-2-Dataset-Setting" class="headerlink" title="4.2 Dataset Setting"></a>4.2 Dataset Setting</h3><p>17个UCI<i><a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets.html</a></i>数据集。其中前15个用于概念分类，后2个用于增量学习。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028082844499.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028082844499.png" srcset="/img/loading.gif" alt></p><p>对于每个数据集，从源数据集中随机选择$\frac{2}{3}$的实例作为训练数据，剩余的用于测试。</p><p>注意，训练集数据既有带标签的数据又有不带标签的数据。</p><p>带标签的实例是从原始数据集中随机抽样的，而未标签的实例是通过忽略它们的标签信息从那些未使用的实例中选择出来的。</p><p>同时，为了公平的比较，所有的实验都重复了20次，并给出了平均准确度和标准差。</p><h3 id="4-3-Concept-Classification"><a href="#4-3-Concept-Classification" class="headerlink" title="4.3 Concept Classification"></a>4.3 Concept Classification</h3><p>从Table 1可以看出，数据集实例大小范围在101~24057之间。这些领域包括</p><p>生活(life)：Zoo，Iris，Breast Cancer,，Haberman， WDBC，Thyroid Disease， and Mushroom。</p><p>物理(physical): Glass，Ionosphere。</p><p>社会(social): Hayes-Roth，Voting Records。</p><p>经济(financial): German Credit。</p><p>“/“表示对源数据集不做处理，”Discretization”表示用Fayyad和Irani’s MDL方法将数值属性进行离散化。</p><p>我们评估了拟议的S2CL在增量学习方面的有效性。最后，我们还在MNIST1数据集上演示了S2CL的效率，MNIST1数据集是一种流行的图像数据集。</p><p>$acc = \frac{N}{|T|}$</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028083223143.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028083223143.png" srcset="/img/loading.gif" alt></p><p>图2显示了平均排名和临界差异图(CD)(即两种方法的排名在95%的置信度下有显著差异)[63]。从图2中我们可以观察到，S2CL在标签比从0.05到0.15时达到了最好的排名，并且在5%和10%的标签实例中显著高于其他方法。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028083333517.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201028083333517.png" srcset="/img/loading.gif" alt></p><h3 id="4-4-Incremental-Learning"><a href="#4-4-Incremental-Learning" class="headerlink" title="4.4 Incremental Learning"></a>4.4 Incremental Learning</h3><p>Algorithm 1 和 2 可以很自然地增量学习。采用了两个数据集HTRU2和Connect-4。</p><p>HTRU2：包含由RFI/noise生成的16259个虚假示例，以及High Time Resolution Universe(HTRU) survey 调查搜集的1639示例。</p><p>Connect-4：67557个实例组成，每个实例具有42个属性。</p><p>相同的预处理技术见表1最后两行。</p><p>设E是学习步骤，C是每个学习步骤块的大小。从Fig 3 和 4可知，我们提出的S2CL算法与其他的算法相比具有更高的精确度和更低的低标准差。</p><p>因为它们无法在未标记的实例上实现增量学习，所以本节使用了ISVM和ISGD的静态结果。</p><p>因此，在图3和图4中，ISVM和ISGD的图形在不同的学习步骤下没有表现出明显的趋势。同时，图5还描述了C=500和第6个学习步骤时这两个数据集的平均误分类率。通过图5可以看到，所提出的S2CL在这些选择的数据集上比其他算法获得更低的误分类率。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170712001.png" srcset="/img/loading.gif" class><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170736591.png" srcset="/img/loading.gif" class><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170843395.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170712001.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170736591.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170843395.png" srcset="/img/loading.gif" alt></p><h3 id="4-5-Experiment-on-the-MINST-Dataset"><a href="#4-5-Experiment-on-the-MINST-Dataset" class="headerlink" title="4.5 Experiment on the MINST Dataset"></a>4.5 Experiment on the MINST Dataset</h3><p>本节，我们将在MINST数据集上进一步评估算法S2CL for SSL的效果。</p><p>这个数据集包含60000个训练实例和10000个测试实例，其中每个实例都是手写的0~9数字，尺寸为28$\times$28px。</p><p>同时，我们从训练数据中随机选取50、100和200个有标签的样本，其余的训练样本通过忽略它们的标签被认为是无标签的数据。</p><p>同时，将S2CL的参数K和${\alpha}_r$分别设置为1和0.5，将算法S2CL与两种半监督单分类器方法即SVM-self(RBFKernel)和SETRED[64]，和半监督的多分类器即TriTraining，CoForest和CoBC。</p><p>对于SETRED，我们采用了KEEL软件中的默认参数，其他方法的调整与4.1节类似。</p><p>此外，所有的实验也用不同的标签样本(即50、100和200个标签样本)重复了20次，平均性能如表3所示。从表3可以看出，从总体上看，所提出的S2CL具有比其他方法更好或更具竞争力的泛化性能。</p><img src="/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170549746.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/10/27/2020-IEEE%20TKDE-Semi-supervised%20Concept%20Learning%20by%20Concept-cognitive%20Learning%20and%20Concept%20Space/image-20201102170549746.png" srcset="/img/loading.gif" alt></p><h2 id="5-DISCUSSION-AND-CONCLUSION"><a href="#5-DISCUSSION-AND-CONCLUSION" class="headerlink" title="5 DISCUSSION AND CONCLUSION"></a>5 DISCUSSION AND CONCLUSION</h2><p>实验结果表明， </p><ol><li>分类结果(包括静态分类和增量学习)表明基于概念层次的概念聚类可以提高模型(S2CL)的性能。</li><li>与离线初始构建概念格相比，基于GrC的模型也可以获得极好的泛化性能。</li><li>关于概念相似度的参数K和${\alpha}_r$对于一个模型来说很重要。</li></ol><p>实际上，对于增量学习的SSL，经典的SSL仍然面临2个挑战：</p><ol><li>如何重新设计经典的SSL方法。</li><li>在增量的过程中如何利用未标记的实例。</li></ol><p>同时，我们也注意到概念结构信息在人类知识组织中起着非常重要的作用。为此，我们利用概念相似度，提出了一种基于正则决策形式背景的$S2CL$(或${S2CL}^α$) for SSL理论框架来应对这些挑战，并给出了相应的算法。</p><p>与经典的SSL算法相比，我们的方法基于一种新的CCL理论而不是标准的机器学习理论来实现增量SSL。</p><p>S2CL有很多好的特性：</p><ol><li>可以通过概念相似度来最大限度地利用概念的结构信息来提高概念分类的性能，还可以通过模仿人类的认知过程来完成动态过程，从而避免了昂贵的计算。</li><li>在不同数据集上的实验结果表明，S2CL可以通过概念认知过程获得更好的SSL性能。</li></ol><p>但是，我们必须认识到，S2CL不能直接处理具有连续值的数据集，这会导致两个关键问题:</p><ol><li>需要对数值属性进行离散化。</li><li>不同的离散化技术会产生不同的分类性能。</li></ol><p>同时，值得注意的是，我们的另一个目标是试图从人类认知过程的角度探索一种新的SSL学习方法。</p><p>此外，关于S2CL的许多有趣问题的深入研究仍值得进一步研究。</p><ol><li>例如要直接处理具有连续值和不确定性的数据集，我们需要探索模糊数据[56]和固有不确定性[65]。</li><li>为了在不同的信息系统中实现概念泛化能力，我们必须研究多源数据集中的S2CL[66]。</li><li>受大数据挖掘中知识获取的启发[67]、[68]，知识抽取的方法也可以集成到S2CL中，从海量数据中学习有用的信息，做出智能决策[69]。</li><li>此外，为了进一步提高SSL的性能，可以从虚拟样本生成的角度将生成器方法[70]整合到当前的SSL方法中，并且还可以在算法设计方面将Co-training[71]等集成策略整合到当前的SSL方法中。</li><li>更重要的是，作为一项有意义的未来研究工作，分析概念聚类稳定性对分类结果的影响仍然是必要的，这也可能成为从提高分类性能的角度研究概念聚类稳定性的深入思考。文献[49]中的工作可能会为这一研究方向的开始提供一个很好的参考。</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>概念认知学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>增量学习</tag>
      
      <tag>概念认知学习</tag>
      
      <tag>半监督学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020华为杯D题--无人机集群协同对抗</title>
    <link href="/2020/09/18/2020%E5%8D%8E%E4%B8%BA%E6%9D%AFD%E9%A2%98--%E6%97%A0%E4%BA%BA%E6%9C%BA%E9%9B%86%E7%BE%A4%E5%8D%8F%E5%90%8C%E5%AF%B9%E6%8A%97/"/>
    <url>/2020/09/18/2020%E5%8D%8E%E4%B8%BA%E6%9D%AFD%E9%A2%98--%E6%97%A0%E4%BA%BA%E6%9C%BA%E9%9B%86%E7%BE%A4%E5%8D%8F%E5%90%8C%E5%AF%B9%E6%8A%97/</url>
    
    <content type="html"><![CDATA[<h1 id="无人机集群协同对抗"><a href="#无人机集群协同对抗" class="headerlink" title="无人机集群协同对抗"></a>无人机集群协同对抗</h1><p class="note note-info">华为杯、无人机、协同对抗</p><a id="more"></a><h2 id="1-问题背景"><a href="#1-问题背景" class="headerlink" title="1 问题背景"></a>1 问题背景</h2><p>蓝方：进攻方。</p><p>红方：拦截方。</p><h2 id="2-约束条件"><a href="#2-约束条件" class="headerlink" title="2 约束条件"></a>2 约束条件</h2><p>攻击纵深$BC:L = 50km$。</p><p>蓝方无人机不能越过$AD、BC$两边。</p><p>AB之间的距离为$M$</p><p>蓝方无人机速度：$V_E = 250m/s$，最小转弯半径：$R_E = 500m$。</p><p>红方无人机速度：$V_P = 200m/s$，最小转弯半径：$R_P = 350m$。</p><p>红方成功拦截：蓝方无人机与红方至少2架无人机的距离均小于$R = 300m$。</p><p>蓝方成功突防：蓝方无人机与红方至多1架无人机的距离小于$R = 300m$，且在360s内越过边界CD。</p><p>红方圆周编队：</p><ul><li>任何两架无人机的间距 $\gt$ 30m。</li><li>每一架无人机与本集群至少2架无人机的距离 $\le$ 200m。</li><li>运载机与集群中至少1架无人机距离 $\le$ 10km。</li><li>运载机与集群中任何1架无人机距离 $\gt$ 100m。</li><li>运载机与蓝方无人机的距离 $\gt$ 5km</li></ul><p>红方运载机速度：$V_红 = 300m/s$，转弯半径 $\ge$ 1000m</p><h2 id="3-问题分析"><a href="#3-问题分析" class="headerlink" title="3 问题分析"></a>3 问题分析</h2><h3 id="3-1-问题一"><a href="#3-1-问题一" class="headerlink" title="3.1 问题一"></a>3.1 问题一</h3><p> 对抗伊始</p><p>红方无人机集群圆周中心：$G_1、G_2$。</p><p>圆周半径 = $100m$。</p><p>$DG_1 = 20km$，$G_1G_2 = 30km$，$CG_2 = 20km$。</p><p>目标：==建模分析蓝方无人机突防所处位置最大空间==。</p><h3 id="3-2-问题二"><a href="#3-2-问题二" class="headerlink" title="3.2 问题二"></a>3.2 问题二</h3><p>对抗伊始</p><p>蓝方突防无人机位于AB中点。</p><p>红方无人机集群圆周中心：$G_1、G_2$。位于边界CD上，具体位置不确定。</p><p>圆周半径 = 100m。</p><p>目标：建模分析最小通道带宽$M_{min}$，及蓝方无人机时间最短突防策略。</p><h3 id="3-3-问题三"><a href="#3-3-问题三" class="headerlink" title="3.3 问题三"></a>3.3 问题三</h3><p>红方运载机分2次共发射10架无人机，组成两个集群。</p><p>每个集群无人机数量 $\ge$ 3。</p><p>初始发射集群初始队形为圆周，运载机与圆周中心的距离 = $2km$。</p><p>对抗伊始</p><p>蓝方无人机位于边界AB的中心。</p><p>通道带宽 $M = 70km$。</p><p>红方两架运载机分别位于边界CD上的$G_1$点和$G_2$点，并开始发射首次集群，运载机与集群中心位置不确定。</p><p>运载机第二次发射集群时需与首次集群满足间距约束。</p><p>目标：最优拦截。</p><ul><li>讨论红方两架运载机两次发射无人机数量。</li><li>每架运载机第二次发射的时刻和位置以及集群中心位置。</li><li>进一步分析最大通道 $M_{max}$，及最优拦截策略。</li></ul><h3 id="3-4-问题四"><a href="#3-4-问题四" class="headerlink" title="3.4 问题四"></a>3.4 问题四</h3><p>通道带宽 $M = 100km$。</p><p>蓝方3架无人机组成集群从AB开始突防。</p><p>任2架突防无人机间距 $\gt$ 30m。</p><p>红方5架运载机各携带10架无人机，从CD协同拦截。</p><p>红方每架运载机分2次发射集群，每个集群无人机数量 $\ge$ 3。</p><p>每架运载机首次发射为初始对抗时刻，与集群圆周中心距离 = $2km$。</p><p>目标：红方最优拦截策略与蓝方最优突防策略。</p><ul><li>红方运载机初始位置。</li><li>红方运载机首次发射集群中心位置。</li><li>红方运载机第二次发射集群的时刻和位置及集群中心位置。</li><li>两次发射无人机数量。</li><li>蓝方突防无人机初始位置。</li></ul><h2 id="4-延申拓展"><a href="#4-延申拓展" class="headerlink" title="4 延申拓展"></a>4 延申拓展</h2><p>平面上的红蓝双方对抗上升到实际的空间对抗。</p><p>红方无人机采用的圆周编队上升到实际需要的队形。</p><p>布设无人机集群需要一定的时间</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>数学建模</category>
      
      <category>华为杯</category>
      
    </categories>
    
    
    <tags>
      
      <tag>华为杯</tag>
      
      <tag>无人机</tag>
      
      <tag>协同对抗</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学-基于不相关属性集合的属性探索算法</title>
    <link href="/2020/09/16/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%9F%BA%E4%BA%8E%E4%B8%8D%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7%E9%9B%86%E5%90%88%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%A2%E7%B4%A2%E7%AE%97%E6%B3%95/"/>
    <url>/2020/09/16/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%9F%BA%E4%BA%8E%E4%B8%8D%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7%E9%9B%86%E5%90%88%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%A2%E7%B4%A2%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="基于不相关属性集合的属性探索算法"><a href="#基于不相关属性集合的属性探索算法" class="headerlink" title="基于不相关属性集合的属性探索算法"></a>基于不相关属性集合的属性探索算法</h1><p class="note note-info">属性探索、不相关属性、算法改进</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：沈夏炯、杨继勇、张磊。</p><p>​            2. 期刊：计算机科学。</p><p>​            3. 时间：2020。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.耗时瓶颈：“寻找下一个与专家交互的问题”这一环节存在大量冗余计算。</p><p>2.针对这个问题，通过分析伪内涵和内涵与主基的内在逻辑关系提出并证明了三个定理。</p><p>3.根据定理给出一种基于不相关属性集合的属性探索算法，该算法在计算内涵与伪内涵时，跳过对既不是内涵也不是伪内涵的属性集合的判断过程。</p><p>4，算法最好的时间复杂度为$O(mn^2P^2)$，最坏的时间复杂度为$O(mn^3P^2)$。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>1.属性探索的过程</p><script type="math/tex; mode=display">\begin{align*}首先，&根据形式背景中的属性集合，提出属性与属性间的蕴涵关系是否成立的问题。\newline其次，&由专家判断这些问题是否成立。\newline之后，&根据专家的不同回答，计算下一个需要交互的属性与属性间的蕴涵式。\newline最后，&算法循环产生问题，从而获取到领域专家知识中的主基。\end{align*}</script><p>2.属性探索过程的示例</p><script type="math/tex; mode=display">\begin{align*}用户（&对象）、权限（属性）\newline首先，&提出问题：实验室的所有人都可以打开实验室的门？\newline&专家回答：可以。\newline&说明：实验室的所有人都可以打开实验室的门，这一蕴涵成立。并提出问题：可以打开实验室门的人是不是都可以打开保险柜的门？\newline&专家回答：不行，并提出反例：小明。\newline&将反例（小明）加到形式背景中，对新的形式背景提出新的蕴涵式问题。\newline\vdots& \newline最后，&算法经过多次迭代后能够获取到访问控制授权的知识背景以及权限（属性）间的关系。\end{align*}</script><p>3.应用领域</p><p>2012，Borchmann$^{[10]}$从形式概念分析出发，为属性探索提出了一个通用的描述框架，将属性探索的变体看作通用框架的一个实例。</p><p>2015，Borchmann$^{[11]}$，提出了基于置信度概念的属性探索，为在可能有错误的形式背景中进行属性探索，提供了一个可行性方案。</p><p>2013，Glodeanu$^{[12]}$提出了带有模糊属性与背景知识的属性探索算法，该算法允许专家给出模糊的回答。</p><p>2009，Obiedkov$^{[13]}$以属性探索方式构建访问控制模型。</p><p>2013，Jaschke和Rudolph$^{[14]}$将属性探索应用于Web查询中，提出了一种基于属性探索的网络信息检索方法。</p><p>2015，Obiedkov和Romashkin$^{[15]}$根据协作属性探索来构建众包领域本体。</p><p>2018，Hanika和Zumbragel$^{[16]}$提出了协作概念探索。</p><p>2019，Codocedo$^{[17]}$使用属性探索提出了一个计算模式结构形式背景的方法。</p><p>4.计算过程过于耗时，无法满足大数据时代对海量数据知识获取的需求。</p><p>2014，Borchmann$^{[18, 19]}$为属性探索设置一个适当的前提以提高算法效率，但也使得该算法具有了一定的局限性。</p><p>2016，Kriegel$^{[20]}$提出了一种并行属性探索算法，减少了属性探索算法的整体耗时，但并没有改进单个节点串行算法的时间复杂度。</p><p>5.本文着眼于寻找并规避算法内在的冗余计算过程，以有效降低算法的时间开销。</p><p>关键瓶颈：寻找下一个与专家交互的问题。</p><p>传统的属性探索算法以计算蕴涵伪壳的方式寻找下一个交互问题。</p><script type="math/tex; mode=display">\begin{align*}& J: 蕴涵集，B，P，H，B_1，B_2：属性集 \newline定义：&固定一个蕴涵集J和B\subseteq P，若H\subseteq P满足三个条件：\newline& （1）B\subseteq P; \newline& （2）对任一蕴涵式B_1 \rightarrow B_2\in J，若B_1\subset H，则必有B_2\subseteq H; \newline& （3）在（1）（2）的意义下是最小的。\newline此时，&称为B相对于J的蕴涵伪壳（implication pseudo-hull），记为J*(B)。\end{align*}</script><p>2009，赵小香$^{[21]}$运用属性集合与蕴涵集合的相关性对属性探索算法进行改进，但是该算法在寻找下一个交互问题时，需要逐个遍历属性集合的所有方式，耗时仍较为明显。</p><p>本文发现</p><ul><li>存在一类与主基不相关的集合。这些集合包含主基中的某个蕴涵式的前件，不包含这个蕴涵式的后件。</li><li>所有的主基和内涵集合都不含有这类属性集合。</li></ul><p>对此，本文提出了一个AEUS（Attribute Exploration of Unrelated Set）算法，借助属性集合与主基不相关的关系，跳过与主基不相关的属性属性集合是否为下一个属性探索问题的判断过程，减少寻找下一个交互问题的搜索空间，降低算法的时间复杂度。</p><h2 id="2-基础知识"><a href="#2-基础知识" class="headerlink" title="2 基础知识"></a>2 基础知识</h2><p>1.伪内涵</p><script type="math/tex; mode=display">\begin{align*}定义：&设K=(U, M, I)是一个形式背景，Y\subseteq M，满足 \newline& （1）Y\neq f(g(Y)) \left(即Y\subset f(g(Y)) \right); \newline& （2）对每一个伪内涵Y_1\subset Y都有f(g(Y_1))\subseteq Y; \newline则称Y & 是一个伪内涵。\end{align*}</script><p>2.蕴涵式（值依赖）</p><script type="math/tex; mode=display">\begin{align*}定义：&设K=(U, M, I)是一个形式背景，Y_1, Y_2\subseteq M，\newline& 若g(Y_1)\subseteq g(Y_2)，则蕴涵式Y_1\rightarrow Y_2 在K中成立。 \end{align*}</script><p>3.主基</p><script type="math/tex; mode=display">\begin{align*}定义：&设K=(U, M, I)是一个形式背景，则称值依赖集合\{ B\rightarrow f(g(B)) - B | B是K的伪内涵 \}是K的主基。 \end{align*}</script><p>4.相关</p><script type="math/tex; mode=display">\begin{align*}定义：&设K=(U, M, I)是一个形式背景，蕴涵式集合J(K)，蕴涵式C\rightarrow D\in J(K)。\newline& 若属性集合T\subseteq M，当且仅当C\nsubseteq T或D\subseteq T时，称T与C\rightarrow D相关。\newline& 若T与J(K)中所有的蕴涵式都相关，则称T与J(K)相关。\end{align*}</script><p>5.字典序</p><script type="math/tex; mode=display">\begin{align*}定义：& 设K=(U, M, I)是一个形式背景，M=\{m_1, m_2, \cdots, m_n \}。\newline& M中属性满足基本线性序关系：m_1<m_2< \cdots <m_n，\newline& 则对任意的Y_1, Y_2\subseteq M, 当且仅当存在m_i\in Y_2-Y_1 \newline& 且Y_1\cap \{ m_1, m_2, \cdots, m_{i-1} \} = Y_2\cap \{ m_1, m_2, \cdots, m_{i-1} \}时，记Y_1<Y_2。\end{align*}</script><h2 id="3-属性探索算法理论研究与改进"><a href="#3-属性探索算法理论研究与改进" class="headerlink" title="3 属性探索算法理论研究与改进"></a>3 属性探索算法理论研究与改进</h2><p>1.属性探索算法以主动提出问题的方式与领域专家交互， 通过字典序遍历属性集合，并测试该集合是否是伪内涵或者 内涵。</p><p>2.利用是伪内涵的属性集合产生蕴涵式，从而构建形式背景的主基，获取到相关的背景知识。由于该字典序是所有属性幂集上的一个线性序，所以这保证了属性探索算法的完 备性。但当属性数目较多时，算法的耗时很长。 </p><p>3.==上述过程中耗时的关键在于，通过遍历的方式测试属性集是否为伪内涵或者内涵的过程存在大量冗余计算==。</p><p>4.本文发现，如果==属性集合包含主基中某个蕴涵式的前件，但是不包含后件==，那么这个==属性集合就不可能是内涵或者伪内涵==。这有助于跳过一些不必要的计算过程。</p><h3 id="3-1-理论依据"><a href="#3-1-理论依据" class="headerlink" title="3.1 理论依据"></a>3.1 理论依据</h3><p>1.开区间$<B, d>$</B,></p><script type="math/tex; mode=display">\begin{align*}定义：& 给定形式背景K=(U, M, I),属性集合B，D\subseteq M且B<D。\newline& 若集合T=\{ C|B<C<D, C\subseteq M \}，\newline& 则称T是属性集合B与属性集合D在序<上的开区间，记为<B, D>。\end{align*}</script><p>2.$B^+ \succ B$</p><script type="math/tex; mode=display">\begin{align*}定义：& 给定形式背景K=(U, M, I),属性集合B，B^+\subseteq M且<B,B^+>为空集。\newline& 则称B^+仅大于B，记为B^+ \succ B。\end{align*}</script><p>3.$B \precnsim N$</p><script type="math/tex; mode=display">\begin{align*}定义：& 给定形式背景K=(U, M, I),属性集合B，N\subseteq M, B<N,且N\nsupseteq B, \newline& 若对任意的属性集合T\in <B, N>，都有T\supset B， \newline& 则称N非平凡仅大于B，记为B \precnsim B。\end{align*}</script><p>4.不相关</p><script type="math/tex; mode=display">\begin{align*}定义：& 给定形式背景k=(U, M, I)与K上的主基J(K),蕴涵式C\rightarrow D\in J(K), \newline& 若属性集合T\subseteq M，当且仅当C\subseteq T且D\nsubseteq T时， \newline& 则称T与蕴涵式C\rightarrow D不相关。\newline& \end{align*}</script><p>5.定理1：给定形式背景$K=(U, M, I)$与K上的主基$J(K)$，任意蕴涵式$C\rightarrow D\in J(K)$。若属性集合$T$与$C\rightarrow D$不相关，则在$K$中，$T$既不是内涵也不是伪内涵。</p><script type="math/tex; mode=display">\begin{align*}证明：& \newline（1）&先证T不是内涵: f(g(T))\neq T\newline& 由于T与C\rightarrow D不相关，所以由定义11（不相关）可知C\subseteq T且D\nsubseteq T。 \newline& 由性质1（B\subseteq f(g(B))）可知T\subseteq f(g(T))，所以有C\subseteq T\subseteq f(g(T))。\newline& 而由C\subseteq T和性质1 \left( B\subseteq D \Rightarrow g(B)\supseteq g(D) \right)可知g(T)\subseteq g(C)，f(g(C))\subseteq f(g(T))。\newline& 将式f(g(C))\subseteq f(g(T))两端同时减C得到f(g(C))-C\subseteq f(g(T))-C。\newline& 又因为C\rightarrow D\in J(K)，所以f(g(C))-C=D, \newline& 而因为D\nsubseteq T且D\subseteq f(g(T))-C，所以f(g(T))-C\nsubseteq T，移项得， \newline& f(g(T)) \nsubseteq C \cup T，而C\subseteq T，则\newline& f(g(T)) \nsubseteq C \cup T = T即证f(g(T)) \nsubseteq T。 \newline\newline（2）&再证T不是伪内涵：反证法。\newline& 假设T满足条件：对每一个伪内涵Y_1\subset Y都有f(g(Y_1))\subseteq Y，则称Y是一个伪内涵。\newline& 则对任意伪内涵Y_1\subset T都有f(g(Y_1))\subseteq T。\newline& 而由于C\rightarrow D\in J(K)，所以C在K中是一个伪内涵，则有f(g(C))-C=D。 \newline& 又因为T与C\rightarrow D\in J(K)不相关，则有C\subseteq T且D\nsubseteq T。\newline& 所以有f(g(C))-C=D\nsubseteq T，则有f(g(C))\nsubseteq T。\newline& 即存在一个伪内涵C\subset T，不满足f(g(C))\subseteq T。即证。\end{align*}</script><p>定理1表明：如果属性集合与主基中任何一个蕴涵式不相关（也即包含蕴涵式前件，但是不包含蕴涵式后件）。</p><p>那么这个属性集合既不是内涵也不是伪内涵。因为在属性探索中只考虑是内涵或者伪内涵的属性集合，所以满足定理1的属性集合可以不予计算。</p><p>6.引理1：设$K=(U, M,  I)$是一个形式背景，对任意的$Y_1，Y_2\subseteq M$。若$Y_1\lt Y_2$，则 $Y_2\nsubseteq Y1$.</p><script type="math/tex; mode=display">\begin{align*}证明：& \newline& 因为Y_1\lt Y_2，由定义7（字典序）可知\exist\ m\in Y_2-Y_1且Y_1\cap \{ m_1, m_2, \cdots, m_{i-1} \} = Y_2\cap \{ m_1, m_2, \cdots, m_{i-1} \}。\newline& 若Y_2\subseteq Y_1，则有m_i = Y_2 - Y_1 = \emptyset，即不存在这样的m_i，与Y_1\lt Y_2矛盾，所以有Y_2\nsubseteq Y1。\end{align*}</script><p>7.定理2：给定形式背景$K=(U,M,I)$与$K$上的主基$J(K)$，对于任意蕴涵式$B\rightarrow f(g(B))-B\in J(K)$，若存在$B^+, N\subseteq M$，满足$B^+\succ B$且$B^+$与$J(K)$不相关，$B\precsim N$且$N$与$J(K)$相关。则在区间$<B,\ \min(f(g(b)), n)>$内既不存在内涵也不存在伪内涵。</B,\></p><script type="math/tex; mode=display">\begin{align*}证明：& 因为B^+与J(K)不相关，由定理1可知B^+既不是内涵也不是伪内涵。 \newline& (1)设f(g(B))\gt N，由定义10（B \precnsim N）可知，对于任意的属性集合C\in <B, N>，都满足B\subseteq C。\newline& 因为C\lt N\lt f(g(B))，所以由引理1可知f(g(B))\nsubseteq C，即C满足定义11（不相关）的条件，\newline& 可知C既不是内涵也不是伪内涵。而由于C是区间<B,\ N>的任意属性集合。\newline& 所以在区间<B,\ N>内，既不存在内涵也不存在伪内涵。\newline\newline& (2)设N\gt f(g(B))时，由定义10（B \precnsim N）可知，对于任意的属性集合C\in <B,\ f(g(B))>都满足B\subseteq B_k。\newline& 因为C\lt f(g(B))，所以由引理1可知f(g(B))\nsubseteq C，即C满足定义11（不相关）的条件，\newline& 可知C既不是内涵也不是伪内涵。而由于C是区间<B,\ f(g(B))>的任意属性集合。\newline& 所以在区间<B,\ f(g(B))>内，既不存在内涵也不存在伪内涵。\newline\newline& (3)设N=f(g(B))时，因为字典序是一个线性序，所以N\subseteq f(g(B))，f(g(B))\subseteq N。\newline& 由定义10（B \precnsim N）可知，对于任意的属性集合C\in <B,\ f(g(B))>都满足B\subseteq B_k。\newline& 因为C\lt f(g(B))，所以由引理1可知f(g(B))\nsubseteq C，即C满足定义11（不相关）的条件，\newline& 可知C既不是内涵也不是伪内涵。而由于C是区间<B,\ f(g(B))>的任意属性集合。\newline& 所以在区间<B,\ f(g(B))=N>内，既不存在内涵也不存在伪内涵。\newline\newline证毕。\end{align*}</script><p>定理2表明：对于主基中的任何一个蕴涵式，通过计算蕴涵式前件属性集合的仅大于属性集合的不相关性，和非平凡仅大于属性集合的相关性，可以得到一个既不存在内涵也不存在伪内涵的属性集合区间。</p><p>这为我们在以字典序遍历并判断属性集是否为伪内涵或内涵时，忽略这些属性集合区间的计算，提供了理论依据。</p><p>8.定理3：给定形式背景$K=(U,M,I)$与$K$上的主基$J(K)$和属性集合$T\subseteq M$，蕴涵式集合$J_{\lt T}(K) = \{ C\rightarrow D\ |\ C\rightarrow D\in J(K)且C\lt T \}$。若属性集合$T$与$J_{\lt T}(K)$相关，则$T$与$J(K)$相关。</p><script type="math/tex; mode=display">\begin{align*}证明：& \newline& 由题干知J_{\lt T}(K)\subseteq J(K)，因为T与J_{\lt T}(K)相关，\newline& 所以对于任意的C\rightarrow D\in J_{\lt T}(K)，都满足C\nsubseteq T或D\subseteq T。\newline& 又因为对于任意的E\rightarrow F\in J(K)-J_{\lt T}(K)都满足T\lt E，\newline& 所以由引理1可知E\nsubseteq T，即T满足定义6（相关）的条件，\newline& 即T与J(K)-J_{\lt T}(K)相关。又因为T与J_{\lt T}(K)相关，所以T与J(K)相关。\newline证毕。\end{align*}</script><p>定理3表明：给定一个形式背景和主基，若某个属性集合与主基中小于字典序的蕴涵式都相关，则该属性集合与主基相关，即某个属性集合与主基相关的必要条件是该属性集合与主基中小于字典序的蕴涵式都相关。</p><p>在属性探索算法中，以字典序遍历属性集合时，只需考虑此属性集合是否与当前的部分主基相关，不需要考虑其字典序之后的蕴涵式。定理3为我们在属性探索中判断属性集合与主基是否相关提供了理论依据。</p><h3 id="3-2-属性探索算法的改进"><a href="#3-2-属性探索算法的改进" class="headerlink" title="3.2 属性探索算法的改进"></a>3.2 属性探索算法的改进</h3><p>算法1：AEUS算法描述</p><p><img_asset image-20201002163740499.png><img_asset image-20201002165510900.png></img_asset></img_asset></p><p><img src="/peerless.github.io/2020/09/16/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%9F%BA%E4%BA%8E%E4%B8%8D%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7%E9%9B%86%E5%90%88%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%A2%E7%B4%A2%E7%AE%97%E6%B3%95/image-20201002163740499.png" srcset="/img/loading.gif"><br><img src="/peerless.github.io/2020/09/16/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%9F%BA%E4%BA%8E%E4%B8%8D%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7%E9%9B%86%E5%90%88%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%A2%E7%B4%A2%E7%AE%97%E6%B3%95/images/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%9F%BA%E4%BA%8E%E4%B8%8D%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7%E9%9B%86%E5%90%88%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%A2%E7%B4%A2%E7%AE%97%E6%B3%95/image-20201002165510900.png" srcset="/img/loading.gif" alt="image-20201002165510900" style="zoom:80%;"></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>不相关属性</tag>
      
      <tag>算法改进</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2018-中科大学报-MapReduce环境下基于概念分层的概念格并行构造算法</title>
    <link href="/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/"/>
    <url>/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="MapReduce环境下基于概念分层的概念格并行构造算法"><a href="#MapReduce环境下基于概念分层的概念格并行构造算法" class="headerlink" title="MapReduce环境下基于概念分层的概念格并行构造算法"></a>MapReduce环境下基于概念分层的概念格并行构造算法</h1><p class="note note-info">并行构造、概念分层、MapReduce</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：蔡勇、陈红梅。</p><p>​            2. 期刊：中国科学技术大学学报。</p><p>​            3. 时间：2018。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.对概念格提出运用划分分治和分层约束的方法研究MapReduce框架下并行构造概念格。</p><p>2.将形式背景按对象划分成外延独立子背景后并行计算子背景上的临时概念，融合各节点临时概念形成全局概念。</p><p>3.将全局概念按照各概念外延基数分层，通过分层约束计算概念父子节点的搜索范围和并行搜索各层概念得到父子节点。</p><h2 id="0-引言"><a href="#0-引言" class="headerlink" title="0 引言"></a>0 引言</h2><p>1.MapReduce是由Google公司提出的一种处理海量数据的并行编程模型。</p><p>2.本文思想：采用分而治之的思想，将形式背景拆分成若干个外延独立的子形式背景，集群中的各节点对子形式背景并行计算得到所有概念，然后将概念按照概念外延基数进行分层，最后各节点并行地搜索满足父子关系分层中的所有概念以确定当前概念的父子概念从而形成完备的概念格。</p><p>3.创新点：</p><ul><li>基于MapReduce平台设计了完全并行的概念格构造算法。</li><li>提出了概念分层的思想，在建立格关系的过程中能够有效提升运算效率。</li></ul><h2 id="1-概念格理论"><a href="#1-概念格理论" class="headerlink" title="1 概念格理论"></a>1 概念格理论</h2><p>对象集$X\subseteq G$和属性集$Y\subseteq M$</p><script type="math/tex; mode=display">\begin{align}& X^* = \{m | m\in M, \forall g\in X, gIm \} \newline& Y^{’} = \{ g | g\in G, \forall m\in Y, gIm \} \end{align}</script><p>概念：$X^* = Y$和$X = Y^{‘}$。</p><p>对象概念：$(g^{<em>‘}，g^</em>)$。</p><p>属性概念：$(m^{‘}, m^{‘*})$。</p><h2 id="2-概念格并行构造原理"><a href="#2-概念格并行构造原理" class="headerlink" title="2 概念格并行构造原理"></a>2 概念格并行构造原理</h2><p>本文设计的概念格并行构造算法主要分为：形式概念生成、概念分层和建立概念关系。</p><h3 id="2-1-并行生成形式概念原理"><a href="#2-1-并行生成形式概念原理" class="headerlink" title="2.1 并行生成形式概念原理"></a>2.1 并行生成形式概念原理</h3><p>$K(G, M, I)$：形式背景。</p><p>$K_i(G_i, M_i, I_i)$：子形式背景。</p><p>外延独立子背景：将形式背景按照对象划分成多个互不相交的子形式背景。</p><p>外延独立单元子背景：外延为1即一个对象的外延独立子背景。</p><p>临时概念：$C_i(X, Y)\in L(G_i, M_i, I_i)$，则$C_i(X, Y)$为形式背景$K$的一个临时概念。</p><p>最终概念：$C_f(X, Y)\in L(G, M, I)$，则$C_f(X, Y)$为形式背景$K$的一个最终概念。</p><p>具体步骤：</p><p>1.在每一个外延独立子背景上可以独立地计算出外延独立单元子背景的全部概念，这些概念是整个形式背景的临时概念。</p><p>2.对所有临时概念，采用内涵相同则外延取并集，外延相同则内涵取并集的方式得到整个形式背景的最终概念。</p><p>3.操作划分</p><ul><li>计算临时概念可以采用MapReduce模型的Map任务进行处理。</li><li>对临时概念进行合并可以采用MapReduce模型的Reduce任务来进行处理。</li></ul><h3 id="2-2-概念分层方法"><a href="#2-2-概念分层方法" class="headerlink" title="2.2 概念分层方法"></a>2.2 概念分层方法</h3><p>1.对具有相同外延基数的概念存入相同的文件，以外延基数标识对应存储的文件名字。</p><p>2.在计算概念之间的父子关系时，根据当前概念所在的文件名来约束查找父子概念的搜索范围。</p><p>分区存储和分区搜索可以约束计算概念间父子关系时的搜索范围并降低运算时间。</p><h3 id="2-3-并行构造格结构原理"><a href="#2-3-并行构造格结构原理" class="headerlink" title="2.3 并行构造格结构原理"></a>2.3 并行构造格结构原理</h3><p>概念编号：概念C所在的文件名为extSize，概念C在文件中为第n个概念，则概念C在概念格的编号为extSize-n。</p><p>父子关系：$C_1 = (X_1, Y_1)$和$C_2 = (X_2, Y_2)$是两个不同的概念，如果$Extension(C_1)\subset Extension(C_2)$且$|Extension(C_2)| - |Extension(C_1)| = 1$，则$C_2 = father(C_1)$</p><h2 id="3-概念格并行构造算法"><a href="#3-概念格并行构造算法" class="headerlink" title="3 概念格并行构造算法"></a>3 概念格并行构造算法</h2><h3 id="3-1-并行生成概念算法"><a href="#3-1-并行生成概念算法" class="headerlink" title="3.1 并行生成概念算法"></a>3.1 并行生成概念算法</h3><p>两个步骤：</p><ol><li>对所有临时概念采用内涵相同则外延取并集。</li><li>对上步输出的临时概念采取外延相同则内涵取并集。</li></ol><img src="/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914145107937.png" srcset="/img/loading.gif" class><img src="/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914145421907.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914145107937.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914145421907.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914150002312.png" srcset="/img/loading.gif" alt></p><p>为了对算法加速，在每一个Map任务执行后进行一次Combine操作。</p><p>Combine操作相当于在Reduce操作之前，在各节点先进行一次“min-Reduce”操作，这样能够减少节点间的数据传输量。</p><h3 id="3-2-并行建立格结构算法"><a href="#3-2-并行建立格结构算法" class="headerlink" title="3.2 并行建立格结构算法"></a>3.2 并行建立格结构算法</h3><p>算法3.3中Setup方法的作用是：</p><ol><li>在每一个Map任务初始化时获取当前数据分片的名字并找到所有满足计算概念父子关系时搜索范围的文件，用extSize标识文件名里的外延基数。</li><li>在Map阶段确定每一个概念的父子概念，因为所有的最终概念已经确定，只需要找到各概念的父概念就可以得到完备的概念格。</li><li>算法的最终输出形式为：概念编号，概念外延，概念内涵，父概念编号集合。</li></ol><img src="/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914153104036.png" srcset="/img/loading.gif" class><img src="/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914153312486.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914153104036.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914153312486.png" srcset="/img/loading.gif" alt></p><h3 id="3-3-算法示例"><a href="#3-3-算法示例" class="headerlink" title="3.3 算法示例"></a>3.3 算法示例</h3><p>给定形式背景如下所示：</p><img src="/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914154224073.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914154224073.png" srcset="/img/loading.gif" alt></p><p>并行流程：</p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914155241423.png" srcset="/img/loading.gif" alt></p><p>对象分层属性合并：</p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914161039581.png" srcset="/img/loading.gif" alt="image-20200914161039581"></p><p>父子关系及建格：</p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914164343945.png" srcset="/img/loading.gif" alt></p><h2 id="4-实验及算法性能分析"><a href="#4-实验及算法性能分析" class="headerlink" title="4 实验及算法性能分析"></a>4 实验及算法性能分析</h2><p>两个数据集均是UCI标准数据集经过离散化处理后得到的。</p><p>KDD是从KDDCup99第一条记录开始间隔100条记录提取一次，共提取20000条记录。</p><p>Census是人口收入数据集。</p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914164810946.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914164954857.png" srcset="/img/loading.gif" alt="image-20200914164954857"></p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914165144900.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914165408938.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914165534350.png" srcset="/img/loading.gif" alt><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914165819990.png" srcset="/img/loading.gif" alt></p><p><img src="/peerless.github.io/2020/09/13/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/images/2018-%E4%B8%AD%E7%A7%91%E5%A4%A7%E5%AD%A6%E6%8A%A5-MapReduce%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8E%E6%A6%82%E5%BF%B5%E5%88%86%E5%B1%82%E7%9A%84%E6%A6%82%E5%BF%B5%E6%A0%BC%E5%B9%B6%E8%A1%8C%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95/image-20200914165819990.png" srcset="/img/loading.gif" alt></p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>1.本文提出的基于MapReduce编程框架和概念分层方法的概念格并行构造算法有效的解决了大数据形式背景下概念格生成问题。</p><p>2.计算概念，计算概念之间的父子关系以得到最终概念格都达到完全并行的状态。</p><p>3.采用对概念进行分层再计算概念之间的父子关系的方法能够有效提高算法效率。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>并行构造</category>
      
    </categories>
    
    
    <tags>
      
      <tag>并行构造</tag>
      
      <tag>概念分层</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2018-ICCS-Towards Collaborative Conceptual Exploration</title>
    <link href="/2020/09/04/2018-ICCS-Towards%20Collaborative%20Conceptual%20Exploration/"/>
    <url>/2020/09/04/2018-ICCS-Towards%20Collaborative%20Conceptual%20Exploration/</url>
    
    <content type="html"><![CDATA[<h1 id="Towards-Collaborative-Conceptual-Exploration"><a href="#Towards-Collaborative-Conceptual-Exploration" class="headerlink" title="Towards Collaborative Conceptual Exploration"></a>Towards Collaborative Conceptual Exploration</h1><p class="note note-info">属性探索、协作探索</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Tom Hanika，Jens Zumbragel。</p><p>​            2. 会议：ICCS。</p><p>​            3. 时间：2018。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>1.提出了第一个关于协作学习小组的数学描述。</p><p>2.对给定知识领域的子领域中引入了弱局部专家。</p><p>3.建立了一个联合专家，并首次展示了该专家回答问题的能力。</p><p>4.此外，描述了结合反例如何处理错误接受的蕴涵。</p><p>词汇积累</p><ul><li>consortium：财团；联合；合伙。</li><li>decidability：可判定性。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>1.知识获取的一个特殊任务是获取给定领域中的概念。</p><p>2.一个众所周知的方法是FCA中的属性探索算法$^{[3, 5]}$。该算法使用最小数量的查询，这个查询的数量仍然可能是指数级的，即对象和属性之间的关系构成的形式概念大小。</p><p>3.实践中领域专家的可用性是未知的。</p><p>4.专家也可能不能够或不愿意回答指数级的问题。</p><p>5.==当前任务：对于某些类似这样的任务，给定一个特定的协作场景==。</p><p>即存在属性集$M = {\cup}_{i\in I} N_i$，对于每个子属性集$N_i$均设置一个局部专家$p_i$，那么对这个领域可以提出一个联合专家。</p><ol><li>联合专家处理蕴涵的能力通常不如领域专家。</li><li>由于$M = {\cup}_{i\in I} N_i$，联合专家仍能够回答大量的重要的蕴涵。</li></ol><p>6.==提出了弱局部专家的第一个完整描述以确定==</p><ol><li><p>联合专家是什么？</p></li><li><p>什么可以探索？</p></li><li><p>下一步该关注什么？</p></li></ol><p>词汇积累</p><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2 Related work"></a>2 Related work</h2><p>几个相关领域用各自的科学语言来解决互动协作学习的问题。</p><p>1.Jager和Marti$^{[7]}$针对有真值的直觉分布式知识，提出了一个多主体系统。</p><p>2.Agotnes和Wangs$^{[1]}$解决了一个群体知识分布问题。</p><p>3.Stange、Nurnberger和Heyn$^{[13]}$采用一种更虚拟的方式来获取协作性知识，其中专家使用协作图形编辑器来协商结果。</p><p>4.==本文仍然基于原始的FCA中的属性探索$^{[5]}$==。此外，还有各种高级版本，比如添加背景知识$^{[3]}$、关系探索$^{[12]}$或概念探索$^{[14]}$以及不完整知识的处理$^{[2, 6, 10]}$。</p><p>5.在FCA中，Martin和Eklund$^{[9]}$首要考虑的是构建知识库。在[15]中可以找到提取知识的协同交互概念格修改（删除或添加属性/对象/概念）的工作。这些操作可以在协作概念探索的后续版本中使用。最近专门针对协作探索的工作是[11]，提出了协作性探索的任务。</p><p>词汇积累</p><ul><li><p>modal logic：模态逻辑。</p></li><li><p>epistemic：知识的；认识的，与认识有关的。</p></li><li><p>intuitionistic：直观的；直觉的。</p></li><li><p>multi-agent：多主体的。</p></li></ul><h2 id="3-Attribute-exploration-and-FCA-basics"><a href="#3-Attribute-exploration-and-FCA-basics" class="headerlink" title="3 Attribute exploration and FCA basics"></a>3 Attribute exploration and FCA basics</h2><p>1.==本文提出了联合专家的一个描述，该专家可以作为一个提供不完整反例的领域专家来使用。==</p><p>2.==展示了如何处理反例使已经接受的蕴涵失效的策略，这是咨询联合专家时可能出现的结果。==。</p><p>词汇积累</p><ul><li>idempotent：幂等的。</li><li>exposition：博览会；阐述；展览会。</li><li>compendium：纲要；概略。</li><li>de-validating：失效的。</li></ul><h2 id="4-Consortium"><a href="#4-Consortium" class="headerlink" title="4 Consortium"></a>4 Consortium</h2><p>target domain/closure system $\mathcal{X}$：The implication (A, B) $\in$ Imp(M) is valid in $\mathcal{X}$ if  $\forall X\in \mathcal{X}：A\subseteq X \Rightarrow B\subseteq X$。</p><p>Expert：对于每个蕴涵$f = (A, B) = A\rightarrow B \in Imp(M)$，有Expert for $\mathcal{X}$ is a mapping $p$：Imp(M) $\rightarrow \mathcal{X} \cup \{\top \}$，及以下结论：</p><script type="math/tex; mode=display">\begin{align}& p(f) = \top \Rightarrow f\ is\ valid\ in\ \mathcal{X} \newline& p(f) = X \in \mathcal{X} \Rightarrow A\subseteq X \wedge B \nsubseteq X \Rightarrow f\ is\ not\ valid\ in\ \mathcal{X}\end{align}</script><p>Local Expert：Let  $N\subseteq M$。对于每个蕴涵$f = (A, B) = A\rightarrow B \in Imp(N)$，有A local expert for $\mathcal{X}\ on\ N$ is a mapping $p_N：Imp(N) \rightarrow \mathcal{X}_N \cup \{\top \}\ with\ \mathcal{X}_N := \{X\cap N | X\in \mathcal{X} \}$及以下结论：  </p><script type="math/tex; mode=display">\begin{align}& p_N(f) = \top \Rightarrow f\ is\ valid\ in\ \mathsf{X} \newline& p_N(f) = X \in \mathcal{X}_N \Rightarrow A\subseteq X \wedge B \nsubseteq X \Rightarrow f\ is\ not\ valid\ in\ \mathcal{X}_N\end{align}</script><p>Local pre-expert：对于任意蕴涵$f = (A, B) = A\rightarrow B \in Imp(N)$，有A local pre-expert for $\mathcal{X}$ on $N\subseteq M$ is a mapping $p_N^*$：Imp(N) $\rightarrow \mathcal{X_N} \cup \{\top \}$，有</p><script type="math/tex; mode=display">\begin{align}& p_N^*(f) = X\in \mathsf{X}_N \Rightarrow A\subseteq X \wedge B \nsubseteq X\end{align}</script><p> Consortial domain：M 是某些属性集，$\mathcal{X} \subseteq P(M)$为目标领域。如果${\cup}_{i\in I}N_i = M$，则对于某些I有$\mathcal{M} = \{N_i | i\in I \} \subseteq P(M)$。</p><p>如果$\mathcal{M} \neq M$，则称$\mathcal{M} \subseteq p(M)$为一个适当的联合专家领域。</p><p>Well-formed query：对于$M$上某些合适的联合专家领域$\mathcal{M}，$如果$\mathcal{M}^*(A \cup \{b \}) \neq M$则$f=(A, \{b \}) \in Imp(M)$称为Well-formed。即如果存在$N_i \in \mathcal{M}$，则有$A\cup \{b \} \subseteq N_i$。</p><p>==良构查询实际上是在适当的联合域中唯一可以决定的蕴涵是否成立的查询==。</p><p>Consortium for $\mathcal{X}$：对于属性集$M$和在$M$上的目标领域$\mathcal{X}$，$\mathcal{M} = \{N_i | i\in I \}$是$M$上的一个联合领域。$\mathcal{X}$上的一个联合体是族$ C := \{p_i \}_{i\in I}$，$p_i$是目标领域$\mathcal{X}$在$N_i$上的局部前专家。</p><p>==联合专家能够决定任何格式的良构查询==。</p><p>==因此，只要所有查询都是良构的，就可以使用联合专家来代替领域专家==。</p><p>Strong consortial expert：$C = \{p_i \}_{i\in I}$是针对$\mathcal{X}$在M是一个联合体。一个强大的联合专家是一个映射$p_c：{\cup}_{i\in I}Imp(N_i) \rightarrow {\cup}_{i\in I} {\mathcal{X}_i} \cup \{\top \}$，那么对于每个蕴涵$f = (A, B) \in {\cup}_{i\in I}Imp(N_i)$有：</p><script type="math/tex; mode=display">\begin{align}& \exist p_i \in C，p_i(f) \neq \top \newline& p_c(f) = X \in {\cup}_{i\in I}{\mathcal{X}_i} \Rightarrow A\subseteq X \wedge B \nsubseteq X\end{align}</script><p>Consortial expert：$C = \{p_i \}_{i\in I}$是针对$\mathcal{X}$在M是一个联合体。联合专家是一个映射$p_c：{\cup}_{i\in I}Imp(N_i) \rightarrow {\cup}_{i\in I} {\mathcal{X}_i} \cup \{\top \}$，那么对于每个蕴涵$f = (A, B) \in {\cup}_{i\in I}Imp(N_i)$有：</p><script type="math/tex; mode=display">\begin{align}& \exist p_i \in C，p_i(f) \neq \top \Rightarrow p_c(f) \neq \top \newline& p_c(f) = X \in {\cup}_{i\in I}{\mathcal{X}_i} \Rightarrow A\subseteq X \wedge B \nsubseteq X\end{align}</script><p>词汇积累</p><ul><li>ambiguity：模棱两可。</li><li>donut：环状线圈。</li><li>speak about：谈论关于。</li><li>well-formed：身材苗条的；结构良好的；符合语法规则的。</li></ul><h2 id="5-Exploration-with-consortial-experts"><a href="#5-Exploration-with-consortial-experts" class="headerlink" title="5 Exploration with consortial experts"></a>5 Exploration with consortial experts</h2><p>1.通常，对于带有部分示例的属性探索来探索领域，可以使用一些强大的联合专家来代替领域专家。然而有3个问题可能需要处理。</p><ol><li>对于联合专家涉及的蕴涵可能不是良构查询。</li><li>如果一个由当地专家组成的联合体确实接受了一个蕴涵，这并不一定意味着这个有问题的蕴涵在该领域是有效的。</li><li>在选择C的子集时，联合专家可能遗漏了一位可能知道反例的当地预专家。</li></ol><p>2.三个问题的回答。</p><ol><li>联合专家无法解决。而当不存在当地预专家来咨询时，对于某些蕴涵仅有的选择是接受或一种更合适的方式是返回Null。</li><li>需要修复一组已经接受了的蕴涵，以防后面的处理过程中出现反例。当然也有可能出现联合专家没有能力发现一个被接受的无效蕴涵。这就会导致算法返回的不是目标域而是另一个闭包系统。这个闭包系统可能和目标域存在一定的“接近”程度。</li><li>需要一个强有力的联合专家。可采用统计方法来量化需要咨询的专家人数，以获得较低的误差幅度。</li></ol><h3 id="5-1-Correcting-falsely-accepted-implications"><a href="#5-1-Correcting-falsely-accepted-implications" class="headerlink" title="5.1 ==Correcting falsely accepted implications=="></a>5.1 ==Correcting falsely accepted implications==</h3><p>1.使用联合专家进行探索的一个主要问题是可能错误地接受蕴涵。</p><p>当联合专家开始接受新的反例$O\subseteq M$时，探索算法还必须检查$O$是否是蕴涵集$L$中已经接受蕴涵的反例。</p><p>当发现这样的蕴涵$f = (A, B)$时，我们需要将$f$的结论限制在尚未被反驳的子集中，并且还需要添加更强的前提的蕴涵。</p><p>特别地，因为$f = (A, B)$被错误地接受，则可将$f$替换为$A\rightarrow B \cap C$，并将蕴涵$A\cup \{m \} \rightarrow B\ for\ m\in M\backslash (A\cup C)$加入到蕴涵集$L$。</p><p>这种方法可能会极大的增加已经被接受的蕴涵的大小，将返回非常大的一组蕴涵，而不是规范基。</p><p>这个问题可以通过在每次替换$L$中的蕴涵后利用[4，Algorithm19]来处理。该算法采用一组蕴涵并返回规范基，之后，可以基于到目前为止收集的一组蕴涵以及已经收集的反例来计算下一个查询。</p><h3 id="5-2-Consistency"><a href="#5-2-Consistency" class="headerlink" title="5.2 Consistency"></a>5.2 Consistency</h3><p>1.从解决联合专家可能发生的冲突提出一致联合的想法。</p><p>Consistent experts：$C = \{p_i \}_{i\in I}$是针对$\mathcal{X}$在M是一个联合体，$\check{C} \subseteq C$是C上的局部专家集。如果对于$i, j\in I\ with\ p_i, p_j\in \check{C}$并且对于所有的蕴涵$f\in Imp(N_i\cap N_j)$都有$p_i(f) = \top \Leftrightarrow p_j(f) = \top$成立。则把带有一致专家的C称为一致专家联合体。</p><p>Consistent consortium：$C = \{p_i \}_{i\in I}$是针对$\mathcal{X}$在M是一个联合体，如果对于所有的$i, j\in I$并且对于所有的蕴涵$f\in Imp(N_i\cap N_j)$都有$p_i(f) = \top \Leftrightarrow p_j(f) = \top$成立。则称C为一致联合体。</p><p>因此，所有本地预专家要么能够提出提出一些，但不一定相同的反例，以说明蕴涵，要么所有人都接受。</p><p>词汇积累</p><ul><li>in consequence：因此。</li></ul><h3 id="5-3-Abilities-and-limitations-of-a-consortium"><a href="#5-3-Abilities-and-limitations-of-a-consortium" class="headerlink" title="5.3 Abilities and limitations of a consortium"></a>5.3 Abilities and limitations of a consortium</h3><p>1.对于有限属性集M，任何的蕴涵集$\mathcal{F} \subseteq Imp(M)$均可构成一个闭包系统。</p><script type="math/tex; mode=display">\mathcal{X}_{\mathcal{F} } := \{X\in P(M) | \forall f = (A, B) \in \mathcal{F}:A\subseteq X \Rightarrow B\subseteq X \}</script><p>相反地，任何闭包系统$\mathcal{X}$都定义了其有效蕴涵的集合$\mathcal{F}_ \mathcal{X} \subseteq Imp(M)$，并且我们有$\mathcal{X}_{\mathcal{F}_\mathcal{X} } = \mathcal{X}、\mathcal{F}_{\mathcal{X}_\mathcal{F} } = \mathcal{F}$。</p><p>假设S是M上的一类包含目标域的闭包系统$\mathcal{X} \subseteq P(M)$，这个集合S描述了我们可能预先拥有的关于目标域的一些信息。</p><p>假设$\mathcal{M} = \{N_i | i\in I \}$是一个联合域，对于某些$\mathcal{X} \in S$，$N_i\in M$上的一组局部专家$p_i：Imp(N_i) \rightarrow \mathcal{X}_{N_i}\cup \{\top \}$。特别地，$p_i(f) = \top$当且仅当$f$在$\mathcal{X}$上成立。则我们有</p><script type="math/tex; mode=display">\mathcal{F}_\mathcal{M} := \{f\in \cup_{i\in I}Imp(N_i) | f\ is\ valid \} \subseteq \mathcal{F}_\mathcal{X}</script><p>即为所有良构有效的蕴涵集。</p><p>并令$\mathcal{X}_\mathcal{M} := \mathcal{X}_{\mathcal{F}_\mathcal{M} }$，这是由联合重构的闭包系统。显然，$\mathcal{X}_\mathcal{M} \supseteq \mathcal{X}$，则有以下结论：</p><p>Ability of a consortium：$\mathcal{M}$为联合域，局部专家为$p_i：Imp(N_i) \rightarrow \mathcal{X}_{N_i} \cup \{\top \}\ for\ N_i\in \mathcal{M}$，能够通过M上的一类闭包系统S重构目标域$\mathcal{X}$。即对于每个$Y\in S$，当且仅当$Y_\mathcal{M} = X_\mathcal{M} \Rightarrow Y = S$。</p><p>以两个极端的例子解释这些定义：</p><ol><li><p>假设$\mathcal{X} = \{M \}$，那么每个蕴涵均成立，即$\mathcal{F}_\mathcal{X} = Imp(M)$。因为每个联合域$\mathcal{M} = \{N_i | i\in I \}$都有覆盖性质$\cup_{i\in I}N_i = M$，则有$\mathcal{X}_\mathcal{M} = \mathcal{X}$。因此如果$Y_\mathcal{M} = \mathcal{X}_\mathcal{M}$，那么$Y_\mathcal{M} = \{M \}$，所以$Y = \{M \} = \mathcal{X}$，即联合体总是能够重构M上的闭包系统$\mathcal{X}$。</p></li><li><p>考虑$\mathcal{X} = P(M)$并假设$\mathcal{M} = \{N_i| i\in I \}$是一个合适的联合域。对于任何$m\in M$我们有$M\backslash {m} \rightarrow {m} \notin \cup_{i\in I}Imp(N_i)$，而由于$Y_\mathcal{M} = \mathcal{X}_\mathcal{M}\ for\ Y = P(M) \backslash \{M \backslash \{m \} \} \neq \mathcal{X}$，因此没有合适的联合能够重建目标域。</p></li></ol><p>令一组蕴涵集$\mathcal{F} \subseteq Imp(M)$的前提复杂度为$c(\mathcal{F}) = \max\{|A| | f = (A, B) \in \mathcal{F} \}\ if\ \mathcal{F} \neq \varnothing\ and\ c(\varnothing) = -1$。</p><p>同理可知M上的一个闭包系统$\mathcal{X} \subseteq P(M)$的前提复杂度为$c(\mathcal{X}) := \min \{c(\mathcal{F}) | \mathcal{X}_\mathcal{F} = \mathcal{X} \}$，这等于规范基的前提复杂度。</p><p>2.Reconstructability in boounded premise complexity：联合域$\mathcal{M}$上的专家能够重建类$S_k$上的目标域，且当且仅当大小为k+1的每个子集$O\subseteq M$包含在某些$N\in \mathcal{M}$中时，才能重构目标域。</p><p>证明</p><script type="math/tex; mode=display">\begin{align}& 首先假设 k+1 大小的每个子集 O\subseteq M 是包含在某些 N\in \mathcal{M} 中。 \newline& 则对于每个闭包系统 \mathcal{X}\in S_k 都有 \mathcal{X}_\mathcal{M} = \mathcal{X} ，因此 S_k 内的目标域是能够重构的。 \newline& 令 \mathcal{X} \in S_k ，则对于前提复杂度为 c(\mathcal{F}) \le k 的一个蕴涵集 \mathcal{F} 有 \mathcal{X} = \mathcal{X}_\mathcal{F}。 \newline& 我们能够假设每个蕴涵 f\in \mathcal{F} 的形式为 f = (A, \{b \}) 。 \newline& 则有 \mathcal{F} \subseteq _{N\in \mathcal{M} }Imp(N) ，所以有 \mathcal{F} \subseteq \mathcal{F}_\mathcal{X} \cap \bigcup_{N\in \mathcal{M} }Imp(N) = \mathcal{F}_\mathcal{M} \subseteq \mathcal{F}_\mathcal{X}。 \newline& 这意味着 \mathcal{X}_\mathcal{F} = \mathcal{X}_{\mathcal{F}_\mathcal{M} } 即 \mathcal{X}_\mathcal{M} = \mathcal{X} 即为所求。\end{align}</script><p>相反地，</p><script type="math/tex; mode=display">\begin{align}& 假设存在一个 k+1 的子集 O\subseteq M 并不包含在任何的 N\in \mathcal{M}中。 \newline & 选择某些属性 b\in O ，令 A := O\backslash \{b \} ，所以 |A| = k 。 \newline& 考虑蕴涵 f := (A, \{b \}) ，那么我们有 f\notin \bigcup_{N\in M}Imp(N)。 \newline& 现在令 \mathcal{X} := P(M) 和 Y := \mathcal{X}_{\{f \} } 。 \newline& 则我们有 \mathcal{X}、Y\in S_k 且 \mathcal{X}_\mathcal{M} = P(M) = Y_M ，说明 \mathcal{X} 不能在 S_k 内重构。\end{align}</script><h3 id="5-4-Extensions"><a href="#5-4-Extensions" class="headerlink" title="5.4 Extensions"></a>5.4 Extensions</h3><p>1.Combining counterexamples：由于联合无法处理类似的反例，组合反例使联合超越了当地预专家知识的总和。</p><p>为此，我们需要通过反例的背景本体来扩充联合。最简单的方法是通过匹配反例的名称来确定来自两个不同的局部预专家的两个反例，专家也需要提供这两个反例。</p><p>在FCA中，在提供反例的同时，联合专家需要知道局部预专家提供的仅限于其属性集的反例在该领域中是否具有相同的反例。</p><p>一个例子</p><p>属性集 M = {mammal, does not lay eggs, is not poisonous}</p><p>两个局部预专家属性集 $N_1$ = {mammal, does not lay eggs}；$N_2$ = {mammal,  is not poisonous}</p><p>则只有专家 $p_1$ 能够确定蕴涵 {mammal} $\rightarrow$ {does not lay eggs} 的成立与否。 $p_1$拒绝，例子：鸭嘴兽。</p><p>当探索下一个查询 {mammal} $\rightarrow$ {is not poisonous} ，$p_1$无法回答。</p><p>当地预专家 $p_2$拒绝，例子：鸭嘴兽。</p><p>与其收集两个不同的反例，我们能够将这两个结合起来说明 {mammal} 不仅是$\mathcal{X_1}$和$\mathcal{X_2}$的元素，也是$\mathcal{X}$ 的元素。</p><p>未来的工作：例如，从某个专家那里获得了一个蕴涵的反例之后，可以询问所有专家它们是否知道这个反例名称。以及它们是否可以从它们的本地属性集中贡献更多的属性。</p><p>==Coping with wrong counterexamples==：事实上，因为每一个 $\mathcal{X_i}$都是对目标域的限制，所以联合专家不能故意用错误的反例来反驳蕴涵。因此，所有由局部预专家提供的反例都是“正确的”。</p><p>为了运行联合体提供错误的反例，必须将一些专家 $p_i$ 的封闭系统与目标域 $\mathcal{X}$ 分离，这也将扩大联合体专家处理反例的可能性。可以从简单多数投票。</p><p>词汇积累</p><ul><li>whence：由于，因此。</li><li>platypus：鸭嘴兽。</li></ul><h2 id="6-Conclusion-and-outlook"><a href="#6-Conclusion-and-outlook" class="headerlink" title="6 Conclusion and outlook"></a>6 Conclusion and outlook</h2><p>本文给出了</p><ol><li>如何将用于属性探索的领域专家拆分为本地预专家联合的形式化描述。</li><li>如何评估联合？</li><li>如何构建联合？</li><li>如何从可能更大的专家集中选择联合？</li><li>如何处理接受错误的蕴涵？</li><li>如何增加一致性的初步结果？</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>协作探索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模糊XML的压缩查询</title>
    <link href="/2020/08/20/%E6%A8%A1%E7%B3%8AXML%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%9F%A5%E8%AF%A2/"/>
    <url>/2020/08/20/%E6%A8%A1%E7%B3%8AXML%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%9F%A5%E8%AF%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="模糊XML的压缩查询"><a href="#模糊XML的压缩查询" class="headerlink" title="模糊XML的压缩查询"></a>模糊XML的压缩查询</h1><p>1.精确XML：</p><p><img src="/peerless.github.io/2020/08/20/%E6%A8%A1%E7%B3%8AXML%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%9F%A5%E8%AF%A2/images/%E6%A8%A1%E7%B3%8AXML%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%9F%A5%E8%AF%A2/image-20200820144133616.png" srcset="/img/loading.gif" alt="image-20200820144133616"></p><p>2.模糊XML：</p><p><img src="/peerless.github.io/2020/08/20/%E6%A8%A1%E7%B3%8AXML%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%9F%A5%E8%AF%A2/images/%E6%A8%A1%E7%B3%8AXML%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%9F%A5%E8%AF%A2/image-20200820144330580.png" srcset="/img/loading.gif" alt="image-20200820144330580"></p><p>3.模糊XML的压缩：</p><pre><code class="hljs angelscript"><span class="hljs-number">1.</span>输入模糊XML文件，获取到XML的文档树。<span class="hljs-number">2.</span>从文档树中找到树的根结点，从根节点开始遍历。<span class="hljs-number">3.</span>判断当前结点是否含有子结点？<span class="hljs-number">4.</span>若有子结点，则遍历子结点的信息，并对子结点进行编码和保存到数据集。<span class="hljs-number">5.</span>若没有子结点，则对当前结点进行编码，并将当前结点信息存入结果集。<span class="hljs-number">6.</span>判断当前结点是否为模糊结点？<span class="hljs-number">7.</span>若当前结点为模糊结点，则计算模糊结点的隶属度。<span class="hljs-number">8.</span>若当前结点不是模糊结点，则不做处理。<span class="hljs-number">9.</span>输出压缩的模糊XML文档。</code></pre><p>4.模糊XML的关键字查询：</p><pre><code class="hljs angelscript"><span class="hljs-number">1.</span>获取到上一步输出的压缩XML文档的结果集。<span class="hljs-number">2.</span>输入查询的关键字<span class="hljs-number">3.</span>对压缩XML文档的结果集进行</code></pre><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2016-CLA-A New Practical Tool for Performing Interactive Exploration over Concept Lattices</title>
    <link href="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/"/>
    <url>/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/</url>
    
    <content type="html"><![CDATA[<h1 id="LatViz-A-New-Practical-Tool-for-Performing-Interactive-Exploration-over-Concept-Lattices"><a href="#LatViz-A-New-Practical-Tool-for-Performing-Interactive-Exploration-over-Concept-Lattices" class="headerlink" title="LatViz: A New Practical Tool for Performing Interactive Exploration over Concept Lattices"></a>LatViz: A New Practical Tool for Performing Interactive Exploration over Concept Lattices</h1><p class="note note-info">属性探索、应用工具</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Mehwish Alam，Thi Nhu Nguyen Le，and Amedeo Napoli。</p><p>​            2. 会议：CLA。</p><p>​            3. 时间：2016。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol><li>由于web of Data(WOD) 的增长，在探索、交互、分析和发现方面出现了许多新的挑战。</li><li>一个基本问题：在WOD之上获得这些概念格之后，用户如何通过概念格交互地探索和分析这些数据。</li><li>引入一个新工具：LatViz；原功能：构造概念格及其导航；新功能：专家交互、模式结构的可视化、AOC偏序集、概念标注、基于若干过滤准则的概念格以及蕴涵的直观可视化。</li></ol><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li>WOD 包含的所有信息都以实体和关系的形式表示，从而允许将语义嵌入到该数据的表示中。</li><li>WOD主要以资源描述框架(RDF，Resource Description Framework)的形式表示数据。</li><li>已有几种方法可以访问此数据，并用于数据分析的可视化与交互探索。<ol><li>LODLive$^{[1]}$：用户可以选择 DBpedia 和 Freebase 等数据集，并指定一个实体作为浏览节点弧标记图的起点。</li><li>RelFinder$^{[2]}$：在给定几个实体的情况下，该工具会自动找到连接这些实体的路径。</li></ol></li><li>这些工具有助于深入了解 RDF 图包含的内容，但它们不是为知识发现的目的而构建的。</li><li>本文介绍了一个新的工具 LatViz，它通过引入多种功能来显著改善用户与概念格之间的交互性。</li></ol><h2 id="2-Motivating-Example"><a href="#2-Motivating-Example" class="headerlink" title="2 Motivating Example"></a>2 Motivating Example</h2><p>一个例子：搜索特定团队与其研究领域相关的会议或期刊的论文。</p><p>1.通过特定的关键字或作者名，找到感兴趣的论文。</p><p>2.通过增加更多的查询条件以缩小查询范围，获得关于特定关键字或作者组的论文。</p><p>3.如果专家想要了解团队与团队外其他研究界成员的合作情况，以及团队成员的多样性和专业性，这不是简单的查询就能直接获得的。</p><p>4，基于这个场景，本文展示了如何通过一个合适的可视化工具来指导专家，以便在概念格的帮助下获得这些感兴趣的信息。</p><h2 id="3-Preliminaries"><a href="#3-Preliminaries" class="headerlink" title="3 Preliminaries"></a>3 Preliminaries</h2><h3 id="3-1-Pattern-Structures"><a href="#3-1-Pattern-Structures" class="headerlink" title="3.1 Pattern Structures"></a>3.1 Pattern Structures</h3><p>基本概念：</p><script type="math/tex; mode=display">\begin{align}&模式结构：(G，(D, \sqcap)，\delta) \newline&G：对象集。 \newline&(D，\sqcap)：描述 D 的交半格。 \newline&\delta ：G\rightarrow D，将对象映射到其描述。\end{align}</script><p>算子运算：</p><script type="math/tex; mode=display">\begin{align}A^{\Box} &= \sqcap_{g\in A}\delta(g) \quad &for\ A\subseteq G \newlined^{\Box} &= \{ g\in G | d\sqsubseteq \delta(g) \} \quad &for\ d\in D \end{align}</script><p>偏序关系：</p><script type="math/tex; mode=display">\begin{align}&c\sqsubseteq d \Leftrightarrow c\sqcap d = c \newline&(A_1, d_1) \le (A_2, d_2) \Leftrightarrow A_1\subseteq A_2(d_2\sqsubseteq d_1)\end{align}</script><p>概念：</p><script type="math/tex; mode=display">\begin{align}A^{\Box} &= d \newlineA &= d^{\Box}\end{align}</script><p>区间模式结构（Interval Pattern Structures）：</p><script type="math/tex; mode=display">\begin{align}&c = (a_i, b_i) \quad d = (e_i, f_i) \newline&c \sqcap d = (min(a_i, e_i)，max(b_i, f_i))\end{align}</script><h3 id="3-2-Web-of-Data-and-its-Classification"><a href="#3-2-Web-of-Data-and-its-Classification" class="headerlink" title="3.2 Web of Data and its Classification"></a>3.2 Web of Data and its Classification</h3><p>RDF被写作三元组：&lt;$subject, predicate, object$&gt;。</p><p>DBLP的RDF存储示例：</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806113720037.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806113720037.png" srcset="/img/loading.gif" alt></p><p>$t_1$：论文$s_1$具有“模式结构”的关键字。</p><p>为了允许对RDF数据进行交互式数据探索，专家通过定义任务需求提出了一组限制条件，并根据该任务需求创建SPARQL查询以获得特定的数据。</p><h2 id="4-LatViz-for-Interactive-Exploration-of-Concept-Lattices"><a href="#4-LatViz-for-Interactive-Exploration-of-Concept-Lattices" class="headerlink" title="4 LatViz for Interactive Exploration of Concept Lattices"></a>4 LatViz for Interactive Exploration of Concept Lattices</h2><h3 id="4-1-User-Interface"><a href="#4-1-User-Interface" class="headerlink" title="4.1 User Interface"></a>4.1 User Interface</h3><p>LatViz：实现两种从二进制的形式背景中构建概念格的算法。</p><p>其中一种在$^{[7]}$，另一种构建概念格的高效算法是 AddIntent$^{[8]}$。</p><p>演示地址：<a href="http://latviz.loria.fr/latviz/。" target="_blank" rel="noopener">http://latviz.loria.fr/latviz/。</a></p><p>一个例子</p><p>基于 Table 1 将$Subject$作为对象集，将$Object$作为属性集。</p><p>RDF三元组：LORIA知识发现团队的出版物。</p><p>对象：343个。</p><p>属性：1516个。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806115136696.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806115136696.png" srcset="/img/loading.gif" alt></p><h3 id="4-2-AOC-Posets"><a href="#4-2-AOC-Posets" class="headerlink" title="4.2 AOC-Posets"></a>4.2 AOC-Posets</h3><p>AOC-Posets：属性和对象概念的偏序集合，在$^{[9, 10]}$中首次引入。</p><script type="math/tex; mode=display">\begin{align}&对象概念：(g^{"}, g') \quad &g\in G \newline&属性概念：(m', m^{"}) \quad &m\in M\end{align}</script><p>对象概念$(g^{“}, g’)$外延包含$g$的低概念集合，从下到上。</p><p>属性概念$(m^{“}, m’)$内涵包含$m$的高概念集合，从上到下。</p><h3 id="4-3-Displaying-Concept-Lattice-Level-wise"><a href="#4-3-Displaying-Concept-Lattice-Level-wise" class="headerlink" title="4.3 Displaying Concept Lattice Level-wise"></a>4.3 Displaying Concept Lattice Level-wise</h3><p>LatViz允许通过交互来层次化地创建概念格。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121302798.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121302798.png" srcset="/img/loading.gif" alt></p><p>第一层：专家使用 Amedeo Napoli 来定位论文，显示由 Amedeo Napoli 撰写的论文数量为152.</p><p>第二层：加入formal concept analysis来定位论文，结果由152降到了55。</p><h3 id="4-4-Display-Sub-Super-Concepts-of-a-Concept"><a href="#4-4-Display-Sub-Super-Concepts-of-a-Concept" class="headerlink" title="4.4 Display Sub/Super Concepts of a Concept"></a>4.4 Display Sub/Super Concepts of a Concept</h3><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121541550.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121541550.png" srcset="/img/loading.gif" alt></p><h3 id="4-5-Display-Hide-the-Sub-lattice"><a href="#4-5-Display-Hide-the-Sub-lattice" class="headerlink" title="4.5 Display/Hide the Sub-lattice"></a>4.5 Display/Hide the Sub-lattice</h3><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121758814.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806121758814.png" srcset="/img/loading.gif" alt></p><p>专家只对Amedeo Napoli 关于知识表示的论文感兴趣的概念子格。</p><h3 id="4-6-Interval-Pattern-Structures"><a href="#4-6-Interval-Pattern-Structures" class="headerlink" title="4.6 Interval Pattern Structures"></a>4.6 Interval Pattern Structures</h3><p>本文提取了论文的三个属性，即论文发表的年份、论文发表时的会议排名以及最终的页数。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123124284.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123124284.png" srcset="/img/loading.gif" alt></p><h3 id="4-7-Lattice-Filtering-Criteria"><a href="#4-7-Lattice-Filtering-Criteria" class="headerlink" title="4.7 Lattice Filtering Criteria"></a>4.7 Lattice Filtering Criteria</h3><p>两类过滤：一是二值形式背景构建的概念格；二是区间模式结构构建的概念格。</p><p>过滤条件：stability, lift, extent size, intent size。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123454487.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806123454487.png" srcset="/img/loading.gif" alt></p><p>查找Amedeo Napoli发表的关于模式结构和FCA主题的论文。</p><p>属性数量设为 3（Amedeo Napoli、formal concept analysis、pattern structure）。</p><p>寻找2012-2015年在排名1-4的会议上发表的论文，并且具有不少于2页且不超过42页的页数，则可以为所有三个属性的值设置相应的过滤器。</p><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124240193.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124240193.png" srcset="/img/loading.gif" alt></p><h3 id="4-8-Attribute-Implications"><a href="#4-8-Attribute-Implications" class="headerlink" title="4.8  Attribute Implications"></a>4.8  Attribute Implications</h3><img src="/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124601090.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/08/06/2016-CLA-A%20New%20Practical%20Tool%20for%20Performing%20Interactive%20Exploration%20over%20Concept%20Lattices/image-20200806124601090.png" srcset="/img/loading.gif" alt></p><h2 id="5-Related-Tools"><a href="#5-Related-Tools" class="headerlink" title="5   Related Tools"></a>5   Related Tools</h2><p>RV-Xplorer（RDF View Explorer）$^{[3]}$：一个在RDF图形上可视化视图的工具，主要用于识别数据中感兴趣的部分并允许数据分析。它还被扩展为集群SPARQL查询答案。</p><p>CREDO$^{[12]}$和FooCA$^{[13]}$：从针对搜索引擎提出的查询中获取答案，并创建概念网格，然后将其显示给专家进行交互。</p><p>CEM$^{[15]}$：一个电子邮件管理器，允许快速搜索电子邮件，通常处理较小的概念格。</p><p>Gamelis$^{[16]}$：一个基于FCA的文档组织系统，允许几个导航操作。</p><p>Sewelis$^{[17]}$和Sparkis$^{[18]}$：允许在知识图上导航/交互。</p><p>ToscanaJ$^{[19]}$：在算法$^{[7]}$的帮助下，重用了构建概念格的源代码。它不仅可以应用于WOD，而且已经扩展到可以解释任何类型的数据。</p><h2 id="6-Discussion-and-Future-Improvements"><a href="#6-Discussion-and-Future-Improvements" class="headerlink" title="6 Discussion and Future Improvements"></a>6 Discussion and Future Improvements</h2><p>1.从未来的角度看，希望增加实现模式结构的变体。</p><p>2.将蕴涵扩展到关联规则。</p><p>3.考虑矩阵因子分解。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>应用工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学与探索-蕴涵的决策蕴涵表示研究</title>
    <link href="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/"/>
    <url>/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="蕴涵的决策蕴涵表示研究"><a href="#蕴涵的决策蕴涵表示研究" class="headerlink" title="蕴涵的决策蕴涵表示研究"></a>蕴涵的决策蕴涵表示研究</h1><div class="note note-primary">            <p>属性探索、决策蕴涵</p>          </div> <a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：王亚丽、翟岩慧、张少霞、贾楠、李德玉。</p><p>​            2.期刊：计算机科学与探索。</p><p>​            3.时间：2020/07/23。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.决策蕴涵是一种特殊的蕴涵，而决策蕴涵的研究就是在蕴涵中建立并研究一个/多个封闭的子系统(包括决策蕴涵子系统及相应的语义和语构子系统)。</p><p>2.为了进一步厘清蕴涵和决策蕴涵之间的关系，对由决策蕴涵子系统能不能得到整蕴涵系统进行了研究。</p><p>3.首先给出了蕴涵可以由决策蕴涵表示的充要条件；</p><p>接着通过实例表明，存在一些蕴涵不可由决策蕴涵表示，因此，进一步区分了直接表示和间接表示；</p><p>随后，通过研究决策背景中只有一个决策属性时不可被直接表示的蕴涵所具有的特点，给出了蕴涵不可由决策蕴涵直接表示的充要条件，并给出了不可被直接表示蕴涵的生成方法。</p><p>4.这种研究为蕴涵和规范基的研究提供了一种新视角，同时也为形式概念分析更深入的理论研究工作奠定了基础。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>1.形式概念分析(Formal Concept Analysis，FCA)是由德国 Wille R 教授于 1982 年提出的一种通过形式背景建立概念格来对数据进行分析和提取规则的一个强有力的工具[1-2]。</p><p>2.目前，FCA 已被广泛地应用到机器学习、社会网络、软件工程、信息检索、基于认知的概念学习、知识约简等相关领域[3-15]。</p><p>3.形式概念分析(概念格)中对知识获取的研究就是对蕴涵的研究[16-21]，但由于蕴涵数目庞大，无法满足用户的需求，因此如何获取完备无冗余的蕴涵规则集仍是研究的热点[17]。</p><p>4.文献[17]从逻辑方面对完备性和无冗余性进行了讨论，其中 Ganter 等已经讨论了蕴涵的语义特征和语构特征，提出了三条蕴涵推理规则，而且证明了这三条推理规则相对于蕴涵的语义是完备的，并给出了源于文献[22]的一个蕴涵基(完备无冗余的蕴涵集合)。。已经证明，该蕴涵基在所有蕴涵基中所含的蕴涵个数最少。</p><p>5.曲开社等进一步讨论蕴涵和逻辑的关系，提出了一种新的蕴涵基，并给出一种有效的方法来生成该蕴涵基[23]。</p><p>6.为了减少蕴涵的数目，曲开社等提出了决策背景及决策蕴涵的概念[24]，并讨论了一种直观的推理规则( 推理规则)，该推理规则通过增加决策蕴涵的前提或者减小决策蕴涵的结论来导出新的决策蕴涵[20]。</p><p>7.文献[20]还讨论了基于该推理规则的完备性和冗余性，并提出了一种基于最小生成子[25]的决策蕴涵规范基生成算法。另外，文献[26]提出了一种基于真前提的决策蕴涵规范基生成算法，实验结果表明该算法效率更高。</p><p>8.研究发现， 推理规则在语构特征上并非是完备的，因此文献[24]提出了合并推理规则，并证明合并推理规则和 推理规则(扩增推理规则)是完备的推理规则集。</p><p>9.在此基础上，文献[27]又给出一条新的推理规则——后件合并推理规则，它只对前件相同的决策蕴涵的后件进行合并。因此，后件合并推理规则在形式上更简洁。文献[27]也证明了扩增推理规则和后件合并推理规则是合理的、完备的并且是无冗余的。</p><p>10.此外，文献[28]给出一个决策蕴涵基(称为决策蕴涵规范基)。该规范基基于决策前提[29]，即由决策前提作为该决策蕴涵集的前提，由决策前提相对于决策子背景的闭包作为该决策蕴涵集的结论。文献[28]还证明了该决策蕴涵规范基是完备的、无冗余的、并且在所有完备的决策蕴涵集中所含的决策蕴涵最少，因而决策蕴涵规范基是最精简的和最优的。</p><p>11.研究结果表明，决策蕴涵规范基是蕴涵规范基[17]在决策背景下的对应概念，并且具有蕴涵规范基的所有优点。</p><p style="text-indent:2em">本文将从语构方面深入研究由这些子系统(决策蕴涵)能不能得到整个系统(所有的蕴涵)。如果蕴涵可以由决策蕴涵推出，那么关于蕴涵的研究就可以转化为决策蕴涵的研究。这样就可以由决策蕴涵来生成蕴涵，甚至可以由决策蕴涵规范基生成蕴涵规范基。</p><h2 id="2-FCA基本概念"><a href="#2-FCA基本概念" class="headerlink" title="2 FCA基本概念"></a>2 FCA基本概念</h2><h2 id="3-蕴涵"><a href="#3-蕴涵" class="headerlink" title="3 蕴涵"></a>3 蕴涵</h2><p>蕴涵的语义特征：</p><p><strong>定义6$^{[17]}$</strong> 设$K=(G, M, I)$是一个形式背景，$T\subseteq M$且$A\rightarrow B$是形式背景$K$下的一个蕴涵。如果属性子集$A\nsubseteq T$或$B\subseteq T$，则称$T$是蕴涵$A\rightarrow B$的一个模型，记为$T\models (A\rightarrow B)$。设$\Theta$为一个蕴涵集，如果对于每一个$(A\rightarrow B)\in \Theta$都有$T\models (A\rightarrow B)$，则称$T$是$\Theta$的一个模型，记为$T\models \Theta$。</p><p><strong>定义7$^{[17]}$</strong> 设$K=(G, M, I)$$是一个形式背景，$ $\Theta$为$K$的一个蕴涵集，若对任意的$T\subseteq M$，$T\models \Theta$蕴涵$T\models A\rightarrow B$，则称$A\rightarrow B$可以从$\Theta$语义导出。记为$\Theta \vdash A\rightarrow B$。若对任意的$(A\rightarrow B)\in \Theta$且$(\Theta \backslash (A\rightarrow B)) \vdash (A\rightarrow B)$，则称$A\rightarrow B$相对于$\Theta$是冗余的。</p><p><strong>定义8$^{[17]}$</strong> 设$K=(G, M, I)$$是一个形式背景，$ $\Theta$为$K$的一个蕴涵集，若对任意的$A\rightarrow B,\ \Theta \vdash A\rightarrow B$均成立，则称$\Theta$是$K$的一个完备集。</p><p>蕴涵的语构特征：</p><p>（1）$X\rightarrow X,\ X\subseteq M$；（自反性）</p><p>（2）若$X\rightarrow Y \in \Theta$，则$X\cup Z \rightarrow Y \in \Theta,\ X, Y, Z \subseteq M$；（增广性）</p><p>（3）若$X\rightarrow Y \in \Theta$且$Y\cup Z \rightarrow W \in \Theta$，则$X\cup Z \rightarrow W \in \Theta,\ X, Y, Z \subseteq M$。（伪传递性）</p><p>文献[30]已经证明这三条推理规则相对于蕴涵的语义是完备的，即从$\Theta$中导出的任意蕴涵都可以重复使用上述三条推理规则从$\Theta$中推出。</p><h2 id="4-决策蕴涵"><a href="#4-决策蕴涵" class="headerlink" title="4 决策蕴涵"></a>4 决策蕴涵</h2><h3 id="4-1-决策蕴涵的语义特征"><a href="#4-1-决策蕴涵的语义特征" class="headerlink" title="4.1 决策蕴涵的语义特征"></a>4.1 决策蕴涵的语义特征</h3><p><strong>定义9$^{[20]}$</strong> 设$K=(G, M, I)<script type="math/tex">是一个形式背景，如果令</script>M=C\cup D,\ I=I_C \cup I_D$，其中$C$是条件属性集，$D$是决策属性集，$C\cap D = \varnothing,\ I_C = G \times C$是条件关系，$I_D = G \times D$是决策关系，此时$K=(G, C, D, I)$为一个以$C$为条件，$D$为决策的决策背景。</p><p><strong>定义11$^{[20]}$</strong> 设$K=(G, C\cup D, I_C \cup I_D)$是一个决策背景，若$A\subseteq C$且$B\subseteq D$，则称$A\rightarrow B$是一个决策蕴涵。此时，$A$为该决策蕴涵的前提，$B$为该决策蕴涵的结论。</p><h3 id="4-1-决策蕴涵的语构特征"><a href="#4-1-决策蕴涵的语构特征" class="headerlink" title="4.1 决策蕴涵的语构特征"></a>4.1 决策蕴涵的语构特征</h3><p>决策蕴涵的语构方面主要研究推理规则的合理性、完备性和无冗余性。</p><p>文献[24]提出两条推理规则：</p><script type="math/tex; mode=display">\begin{align}扩增推理规则&：\frac{A\rightarrow B, A_1 \supseteq A, B_1 \subseteq B}{A_1 \rightarrow B_1} \newline合并推理规则&：\frac{A\rightarrow B, A_1 \rightarrow B_1}{A\cup A_1 \rightarrow B \cup B_1}\end{align}</script><p>并且证明了这两条规则是合理、完备和非冗余的。</p><p>文献[27]在此基础上提出了一条新的推理规则：</p><script type="math/tex; mode=display">\begin{align}后件合并推理规则&：\frac{A \rightarrow B_1, A\rightarrow B_2}{A\rightarrow B_1 \cup B_2}\end{align}</script><p>并且证明了这条规则是合理、完备和非冗余的。</p><h2 id="5-决策蕴涵表示蕴涵"><a href="#5-决策蕴涵表示蕴涵" class="headerlink" title="5 决策蕴涵表示蕴涵"></a>5 决策蕴涵表示蕴涵</h2><h3 id="5-1-蕴涵表示的逻辑简化"><a href="#5-1-蕴涵表示的逻辑简化" class="headerlink" title="5.1 蕴涵表示的逻辑简化"></a>5.1 蕴涵表示的逻辑简化</h3><p><strong>引理1</strong> 设$A\rightarrow B$是形式背景$K$下的一个蕴涵，则蕴涵$A\rightarrow B$成立，当且仅当$\forall b_i \in B, A\rightarrow b_i$均成立。</p><p><strong>证明</strong> </p><script type="math/tex; mode=display">\begin{align*}必要性：\newline因为& b_i \in B，所以有B=\{b_i\} \cup \{B-\{b_i\}\} \newline又因为& A\rightarrow B成立，所以A \rightarrow \{b_i\} \cup \{B-\{b_i\}\} \newline由自反&性推理规则可知，\{b_i\} \rightarrow \{b_i\}成立。 \newline再由增&广性推理规则可知，\{b_i\} \cup (B-\{b_i\}) \rightarrow \{b_i\}成立。\newline最后由&伪传递性推理规则及 \begin{cases} A \rightarrow \{b_i\} \cup \{B-\{b_i\}\} \newline\newline\{b_i\} \cup (B-\{b_i\}) \rightarrow \{b_i\} \end{cases}成立。\newline可知&A \rightarrow b_i成立。\newline\newline充分性: \newline我们&先证明 A\rightarrow \{b_1\} \cup \{b_2\}成立。\newline由自反&性推理规则可知 \{b_1\} \cup \{b_2\}\rightarrow \{b_1\} \cup \{b_2\}成立。\newline而因为& A \rightarrow b_i成立则由伪传递性推理规则及 \begin{cases}A \rightarrow b_1 \newline\newline\{b_1\} \cup \{b_2\}\rightarrow \{b_1\} \cup \{b_2\}\end{cases}成立。\newline可知& A \cup \{b_2\} \rightarrow \{b_1\} \cup \{b_2\}成立。\newline因为& A\cup \{b_2\} = A，即A \rightarrow \{b_1\} \cup \{b_2\} \newline接下来&，令B_1 \triangleq \{b_1\} \cup \{b_2\} \newline同理&，可证A \rightarrow B_1 \cup \{b_3\} \newline以此类&推，即可证明A \rightarrow B成立。\end{align*}</script><p><strong>引理2</strong> 设$A \rightarrow b$是形式背景$K$的一个蕴涵，$A_1 \subseteq A$。若$A_1 \rightarrow b$成立，则由增广性推理规则可知$A \rightarrow b$成立。</p><p>引理2 说明，只需求出$b$成立的最小前件$A_1$，就可得到蕴涵$A \rightarrow b$。显然此时$b \notin A$。 </p><h3 id="5-2-蕴涵表示的几种情况"><a href="#5-2-蕴涵表示的几种情况" class="headerlink" title="5.2 蕴涵表示的几种情况"></a>5.2 蕴涵表示的几种情况</h3><p>在形式背景$K=(G, M, I)$中，令$M=C \cup D，I = I_C \cup I_D$，即$K=(G, C \cup D, I_C \cup I_D)$，则蕴涵$A \rightarrow b$在$K$中存在六种情形：</p><p>（1）$A \subseteq C，b \in D$；</p><p>（2）$A \subseteq D，b \in D$；</p><p>（3）$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D$；</p><p>（4）$A \subseteq C，b \in C$；</p><p>（5）$A \subseteq D，b \in C$；</p><p>（6）$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C$；</p><p>由于情形（1）（5）为决策蕴涵，情形（2）（4）为子背景上的蕴涵，则需判断情形（3）（6）能否由（1）（2）（4）（5）推出，而互换$C、D$，则情形（3）（6）可互换。则仅考虑情形（3）即可。</p><p>首先，有</p><p><strong>引理 3</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D$。则如果$A \rightarrow b$可以由决策蕴涵表示，则$A \rightarrow b$必然是由伪传递性推理规则推导出的。</p><p><strong>证明</strong></p><script type="math/tex; mode=display">\begin{align*}对于自反性推理规则&，由于b \notin A，则A \rightarrow b不具有自反性。\newline\newline对于增广性推理规则&，若X \cup Z \rightarrow Y可应用于A \rightarrow b，则X \cup Z = A 和 Y = \{b\}\newline由于A为使蕴涵成立&的最小前件，则\forall N\subset A，N \rightarrow b均不成立。\newline显然为了使推理规则&有意义，则Z \neq \varnothing，此时X \subseteq A，\begin{cases} 当X \subset A，X \rightarrow b不成立。\newline当X=A时，X\rightarrow b即为 A \rightarrow b  \end{cases}\newline\newline对于伪传递性推理规则&，如果A\rightarrow b可由\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}推导出，\newline则有，X \cup Z = A\end{align*}</script><p><strong>定理 1</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p><strong>证明</strong></p><script type="math/tex; mode=display">\begin{align*}必要性& \newline&若X=\varnothing，则Z=A，而\varnothing \rightarrow Y成立，则Y=\varnothing，\newline&从而Y\cup Z \rightarrow b即为A \rightarrow b，因此X \neq \varnothing \newline\newline&证Y \neq \varnothing，由于A为使蕴涵成立的最小前件，则\forall N\subset A，N \rightarrow b均不成立。\newline&由X\cup Z = A，可知Z\subseteq A。\newline&当Z\subset A时，Z\rightarrow b不成立。为使Y\cup Z \rightarrow b成立，有Y \neq \varnothing。\newline&当Z=A时，Z \rightarrow b即为A \rightarrow b，因此Y \neq \varnothing。\newline\newline&现假设b\in Y，因为X\rightarrow Y成立。则由引理1可知X \rightarrow b成立。\newline&此时，由X \cup Z = A，可知X\subseteq A。\newline&由于X\cup Z为使蕴涵X \cup Z \rightarrow b成立的最小前件。\newline&因此，当X\subset A时，X \rightarrow b不成立。显然矛盾。\newline&当X=A时，X\rightarrow b 即为A \rightarrow b。则b\notin Y。\newline\newline&现假设Y \cup Z \subset A。由于A为使蕴涵成立的最小前件，\newline&因此，当Y\cup Z \subset A时，Y \cup Z \rightarrow b不成立，矛盾，则Y\cup Z \nsubseteq A。\newline\newline&现假设A\subseteq Y\cup Z，由增广性推理规则可知Y \cup Z \rightarrow b可由A \rightarrow b推出。则A\nsubseteq Y \cup Z。\newline\newline充分性& \newline&由于A为使蕴涵成立的最小前件，则\forall N\subset A，N \rightarrow b均不成立，\newline&则Y \cup Z \nsubseteq A保证了Y \cup Z \rightarrow b的可成立性。\newline&为证明A\rightarrow b可由伪传递性推理规则推出，\newline&只需证X \rightarrow Y和Y \cup Z \rightarrow b不等于且不依赖于A\rightarrow b。\newline&\begin{cases}X\rightarrow Y依赖于A \rightarrow b：指A\subseteq X且b\in Y。\newlineY\cup Z \rightarrow b依赖于A \rightarrow b：指A\subseteq Y\cup Z。\end{cases} \newline&由b\notin Y可知，X\rightarrow Y 不等于且不依赖于A \rightarrow b。\newline&由A\nsubseteq Y \cup Z可知，Y \cup Z \rightarrow b不等于且不依赖于A \rightarrow b。\end{align*}</script><p>直接表示：可由情形（1）（2）（4）（5）推导出情形（3）。</p><p>间接表示：情形（3）必须依赖于情形（3）/（6）才能推导出。</p><p>本文只考虑直接表示。</p><p>首先给出$A \rightarrow b$直接表示的判定条件。</p><p><strong>定理 2</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且满足以下条件之一：$A_2\subseteq C，A_3\subseteq D$</p><p>（1）$X=A_2，Z=A_3$，且$\exists\ Y \subseteq D$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>（2）$X=A_3，Z=A_2$，且$\exists\ Y \subseteq $C使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>证明</p><script type="math/tex; mode=display">\begin{align*}必要性& \newline&根据题设，A\rightarrow b属于情形（3）中的蕴涵。若A\rightarrow b可由\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}直接表示。\newline&则X\rightarrow b和Y\cup Z \rightarrow b均不属于情形（3）/（6）。\newline&因此，X不满足X\cap C \neq \varnothing，X\cap D \neq \varnothing；\newline&Z不满足Z\cap C \neq \varnothing，Z\cap D \neq \varnothing。\newline&即为（1）（2）（4）（5）的情形，即X=A_2，Z=A_3或X=A_3，Z=A_2。\newline&当X=A_2，Z=A_3时，X\rightarrow Y必不为（3）/（6）。则Y\cup Z \rightarrow b均不属于情形（3）/（6）时，\newline&有Y\cap C = \varnothing，即Y\subseteq D；\newline&当X=A_3，Z=A_2时，X\rightarrow Y必不为（3）/（6）。则Y\cup Z \rightarrow b均不属于情形（3）/（6）时，\newline&有Y\cap D = \varnothing，即Y\subseteq C；\newline\newline充分性& \newline&由假设和定理1，可知A\rightarrow b可由伪传递性推理规则表示。\newline&现假设条件（1）（2）成立时，X\rightarrow Y和Y\cup Z\rightarrow b必不属于或依赖于情形（3）/（6）。\newline&当X=A_2，Z=A_3，且\exists\ Y \subseteq D时，显然X\rightarrow Y属于情形（1），Y\cup Z \rightarrow b属于情形（2）。\newline&当X=A_3，Z=A_2，且\exists\ Y \subseteq C时，显然X\rightarrow Y属于情形（5），Y\cup Z \rightarrow b属于情形（4）。\newline\end{align*}</script><p>下面给出一个例子说明情形（3）不是冗余的，即存在情形（3）所示的蕴涵不能被决策蕴涵表示。</p><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730115313378.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730115313378.png" srcset="/img/loading.gif" alt></p><p>因为$\{a,d\}^{“} = \{x_2\}^{‘} = \{a,b,d\}$，显然$\{a\} \cup \{d\} \rightarrow \{b\}$是该决策背景的蕴涵。</p><p>令$A_2 \triangleq \{a\}，A_3 \triangleq \{d\}$，则$\{a\} \cup \{d\} \rightarrow \{b\}$属于情形（3）。接下来，利用伪传递性推理规则来推导该蕴涵。</p><script type="math/tex; mode=display">\begin{align*}由定理1，&我们首先假设X \neq \varnothing，Y \neq \varnothing。\newline&首先考虑Z = \varnothing的情况。此时，需要找到Y使 \frac{\{a\} \cup \{d\} \rightarrow Y,Y\rightarrow b}{\{a\} \cup \{d\} \rightarrow \{b\} }成立。\newline&而Y所有可能的取值为\{a,b,d,ab,ad,bd,abd\}。\newline&当Y取\{a,d\}中任意一个时，Y\rightarrow b均不成立。\newline&当Y取\{b,ab,ad,bd,abd\}中任意一个时，用到了\{a\} \cup \{d\} \rightarrow \{b\}自身。\newline&因此，当Z=\varnothing时，不存在Y使\{a\} \cup \{d\} \rightarrow \{b\}可由其他蕴涵推出，即\{a\} \cup \{d\} \rightarrow \{b\}不冗余。\newline\newline&接下来，考虑Z \neq \varnothing的情况，需要找到Y使 \frac{\{a\} \rightarrow Y,Y \cup \{d\} \rightarrow b}{\{a\} \cup \{d\} \rightarrow \{b\} }或 \frac{ \{d\} \rightarrow Y,Y \cup \{a\}  \rightarrow b}{\{a\} \cup \{d\} \rightarrow \{b\} }成立。\newline&而Y所有可能的取值为\{a,b,d,ab,ad,bd,abd\}。\newline&当Y取\{a,b,d,ab,ad,bd,abd\}。\newline&当Y取\{a\}时，用到了\{a\} \cup \{d\} \rightarrow \{b\}本身。\newline&当Y取\{b,d,ab,ad,bd,abd\}中的任意一个时，\{a\} \rightarrow Y不成立。\newline&类似的，当Y取\{d\}时，用到了\{a\} \cup \{d\} \rightarrow \{b\}本身。\newline&当Y取\{a,b,ab,ad,bd,abd\}中的任意一个时，\{d\} \rightarrow Y不成立。\newline&因此，当Z \neq \varnothing时，不存在Y使\{a\} \cup \{d\} \rightarrow \{b\}可由其他蕴涵推出，即\{a\} \cup \{d\} \rightarrow \{b\}不冗余。\end{align*}</script><h3 id="5-3-蕴涵表示的具体方法"><a href="#5-3-蕴涵表示的具体方法" class="headerlink" title="5.3 蕴涵表示的具体方法"></a>5.3 蕴涵表示的具体方法</h3><p>例1 说明某些蕴涵确实不可直接归结为决策蕴涵和子背景的蕴涵。</p><p>因此，我们需找到蕴涵不可以被直接表示时 应满足的条件，并生成相应的蕴涵。</p><p>为此，我们首先讨论决策属性只有一个属性的情形下， $Y$存在或不存在的情况。</p><p><strong>引理 5</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in D$。若决策属性$D$中只有一个属性，则$A \rightarrow b$是冗余的。</p><p>证明</p><script type="math/tex; mode=display">\begin{align*}根据题设&，A \rightarrow b属于情形（3）的蕴涵。\newline&若决策属性D只有一个属性，由于b\in D，所以A_3 = \{b\}，\newline&则由自反性推理规则\{b\} \rightarrow \{b\}以及增广性推理规则b\in A，\newline&则可知，A \rightarrow b是冗余的。\end{align*}</script><p>接下来，分析情形（6）中的蕴涵是否可被直接表示。</p><p>容易证明定理1、2对于情形（6）也是成立的。可得</p><p><strong>推论 1</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p><strong>推论 2</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C。A \rightarrow b$可以使用伪传递性推理规则推导出，当且仅当$\exists X, Y, Z \subseteq C \cup D，$满足$C\neq \varnothing，Y \neq \varnothing，b \notin Y，Y\cup Z \nsubseteq A，A \nsubseteq Y \cup Z，X \cup Z = A$且满足以下条件之一：$A_2\subseteq C，A_3\subseteq D$</p><p>（1）$X=A_2，Z=A_3$，且$\exists\ Y \subseteq D$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>（2）$X=A_3，Z=A_2$，且$\exists\ Y \subseteq C$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>下面的例子表明，当决策属性$D$中只有一个属性时，对于推论2中的两种情况，情形（6）中的蕴涵不总是可以被直接表示。</p><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131326376.png" srcset="/img/loading.gif" class><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131536900.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131326376.png" srcset="/img/loading.gif" alt><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730131536900.png" srcset="/img/loading.gif" alt></p><p>首先考虑推论2中的（1）。此时，需要找到$Y\subseteq D$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。则$Y=\{d\}$，由$\{d\} \rightarrow b$不成立可知$\{a\} \cup \{d\} \rightarrow \{b\}$不可以被直接表示。</p><p>接下来，考虑推论2中的（2）。此时，需要找到$Y\subseteq C$使$\frac{X\rightarrow Y，Y \cup Z \rightarrow b \Rightarrow X \cup Z \rightarrow b}{A \rightarrow b}$成立。</p><p>$Y$所有可能的取值为$\{a,b,ab\}$。无论$Y$取何值，$\{d\} \rightarrow Y$都不成立，因此$\{a\} \cup \{d\} \rightarrow \{b\}$不可以被直接表示。</p><p>由例2可知，当决策属性$D$中只有一个属性时，对于推论 2 所示的两种情况，情形（6）中的蕴涵$A\rightarrow b$不可以被直接表示的充要条件及发现算法。</p><p><strong>引理 5</strong> 设$K=(G, C \cup D, I_C \cup I_D)$ 是一个决策背景，$A \rightarrow b$是$K$上的一个蕴涵，其中$A \cap C \neq \varnothing，A \cap D \neq \varnothing，b \in C$。若决策属性$D$中只有一个属性，则$X=A_2，Z=A_3$，且$\exists\ Y \subseteq D$时，$A \rightarrow b$不可以被直接表示。</p><p>证明</p><script type="math/tex; mode=display">\begin{align*}&当决策属性D中只有一个属性d时，由Y \neq \varnothing和Y\subseteq D可知Y=\{d\}。\newline&由Z=A_3 \subseteq D可知Z=\{d\}，由A\cap D \neq \varnothing和A\cap C \neq \varnothing可知Y\cup Z \subsetneq A。\newline&再由于A为使蕴涵成立的最小前件，可知Y \cup Z \rightarrow b 不成立，从而，A \rightarrow b不可以被直接表示。\end{align*}</script><p>接下来，给出推论2中（2）情形（6）中的蕴涵$A \rightarrow b$也不可以被直接表示的判定条件。</p><img src="/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730140328255.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730140328255.png" srcset="/img/loading.gif" alt></p><p>当 中只有一个属性 时，由引理 5 可知，情形（3）中的蕴涵都是冗余的，因此不必生成;对于情形（6），定理 3 事实上给出了不可被直接表示的蕴涵的生成方法。</p><p><img src="/peerless.github.io/2020/07/28/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/images/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8E%A2%E7%B4%A2-%E8%95%B4%E6%B6%B5%E7%9A%84%E5%86%B3%E7%AD%96%E8%95%B4%E6%B6%B5%E8%A1%A8%E7%A4%BA%E7%A0%94%E7%A9%B6/image-20200730140851645.png" srcset="/img/loading.gif" alt></p><p>显然，上述算法的复杂度较高，因此难以应用于具体的数据集中。</p><h2 id="6-结论与展望"><a href="#6-结论与展望" class="headerlink" title="6 结论与展望"></a>6 结论与展望</h2><p>本文使用蕴涵推理规则来研究蕴涵是否可由决策蕴涵表示。</p><ol><li><p>首先给出蕴涵可以被直接表示时应满足的条件。</p></li><li><p>找出不可以直接归结为决策蕴涵的的蕴涵应满足的充要条件。</p></li><li><p>给出了不可以直接归结为决策蕴涵的蕴涵的生成方法。</p></li></ol><p>存在的问题：</p><ol><li>由于蕴涵推理的复杂性，未对蕴涵的间接表示进行深入的研究。</li><li>不能被直接表示蕴涵的生成算法，复杂度较大，无法用于实际的数据集。</li><li>决策蕴涵规范基与蕴涵规范基之间是否存在着联系。</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>决策蕴涵</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第三章 概率论</title>
    <link href="/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    <url>/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="第三章-概率论"><a href="#第三章-概率论" class="headerlink" title="第三章 概率论"></a>第三章 概率论</h1><p class="note note-info">深度学习、花书、概率论</p><a id="more"></a><h2 id="3-1-概率与随机变量"><a href="#3-1-概率与随机变量" class="headerlink" title="3.1 概率与随机变量"></a>3.1 概率与随机变量</h2><ul><li>频率学派概率（Frequentist Probability）：认为概率和事件发生的频率相关。</li><li>贝叶斯学派概率（Bayesian Probability）：认为概率是对某件事发生的确定程度，可以理解成是确信的程度。</li><li>随机变量（Random Variable）：一个可能随机取不同值的变量。例如：抛掷一枚硬币，出现正面或者反面的结果。</li></ul><h2 id="3-2-概率分布"><a href="#3-2-概率分布" class="headerlink" title="3.2 概率分布"></a>3.2 概率分布</h2><p><span style="color:red">概率质量函数（PMF，Probabily Mass Function）</span>：对于离散型变量，我们先定义⼀个随机变量，然后⽤ ~ 符号来说明它遵循的分布：$x \sim P(x)$，函数$P$是随机变量$x$的$PMF$。</p><p>例如，考虑一个离散型随机变量$x$有$k$个不同的值，我们可以假设$x$是均匀分布的，则它的$PMF$可设为：</p><script type="math/tex; mode=display">P(x=x_i) = \frac1 k</script><p>对所有的$i$都成立。</p><p><span style="color:red">概率密度函数（PDF，Probabily Density Function）</span>：对于连续型变量，如果一个函数$p$是$PDF$，则</p><script type="math/tex; mode=display">\begin{align}分布满足非负性条件&：\forall x\in \mathsf{x}， p(x) \ge 0 \newline分布满足归一化条件&：\int_{-\infty}^{\infty} p(x)dx = 1\end{align}</script><p>例如在（a, b）上的均匀分布：</p><script type="math/tex; mode=display">U(x; a, b) = \frac {1_{ab(x)} }{b-a}</script><p>其中$1_{ab(x)}$表示在（a, b）内为1，否则为0。</p><p><span style="color:red">累积分布函数（CDF，Cummulative Distribution Function）</span>：表示对小于$x$的概率的积分：</p><script type="math/tex; mode=display">CDF(x) = \int_{-\infty}^{x} p(t) dt</script><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> uniform%matplotlib inline<span class="hljs-comment"># 生成样本</span>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)r = uniform.rvs(loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>, size=<span class="hljs-number">1000</span>)ax.hist(r, density=<span class="hljs-literal">True</span>, histtype=<span class="hljs-string">'stepfilled'</span>, alpha=<span class="hljs-number">0.5</span>)<span class="hljs-comment"># 均匀分布 pdf</span>x = np.linspace(uniform.ppf(<span class="hljs-number">0.01</span>), uniform.ppf(<span class="hljs-number">0.99</span>), <span class="hljs-number">100</span>)ax.plot(x, uniform.pdf(x), <span class="hljs-string">'r-'</span>, lw=<span class="hljs-number">5</span>, alpha=<span class="hljs-number">0.8</span>, label=<span class="hljs-string">'uniform pdf'</span>)</code></pre><img src="/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200731122057031.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200731122057031.png" srcset="/img/loading.gif" alt></p><h2 id="3-3-条件概率与条件独立"><a href="#3-3-条件概率与条件独立" class="headerlink" title="3.3 条件概率与条件独立"></a>3.3 条件概率与条件独立</h2><p><span style="color:red">边缘概率（Marginal Probability）</span>：定义在子集上的概率分布被称为边缘概率分布。</p><script type="math/tex; mode=display">\forall x\in \mathsf{x}，P(\mathsf{x} = x) = \sum\limits_y P(\mathsf{x}=x, \mathsf{y}=y)</script><p><span style="color:red">条件概率（Conditional Probability）</span>：某个事件在给定其他事件发生时的概率。</p><script type="math/tex; mode=display">P(\mathsf{y}=y | \mathsf{x}=x) = \frac{P(\mathsf{y}=y , \mathsf{x}=x)}{P(\mathsf{x}=x)}</script><p><span style="color:red">条件概率的链式法则（Chan Rule of Conditional Probability）</span>：任何多维随机变量的联合概率分布，都可以分解成只有⼀个变量的条件概率相乘的形式。</p><script type="math/tex; mode=display">P(x_1, ..., x_n) = P(x_1) \Pi_{i=2}^{n} (x_i|x_1, ..., x_{i-1})</script><p><span style="color:red">独立性（Independence）</span>：两个随机变量$x$和$y$的概率分布能够表示成两个因子的乘积形式。</p><script type="math/tex; mode=display">\forall x\in \mathsf{x}, y\in \mathsf{y}, p(\mathsf{x}=x, \mathsf{y}=y) = p(\mathsf{x}=x)p(\mathsf{y}=y)</script><p><span style="color:red">条件独立性（Conditional Independence）</span>：两个随机变量$x$和$y$的条件概率对于$z$的每一个值都可以写成乘积的形式。</p><script type="math/tex; mode=display">\forall x\in \mathsf{x}, y\in \mathsf{y}, z\in \mathsf{z}, p(\mathsf{x}=x, \mathsf{y}=y | \mathsf{z}=z) = p(\mathsf{x}=x | \mathsf{z}=z)p(\mathsf{y}=y | \mathsf{z}=z)</script><h2 id="3-4-随机变量的度量"><a href="#3-4-随机变量的度量" class="headerlink" title="3.4 随机变量的度量"></a>3.4 随机变量的度量</h2><p><span style="color:red">期望（Expectation）</span>：函数$f$关于概率分布$P(x)$或$p(x)$的期望表示由概率分布产生$x$，再计算$f$作用到$x$后$f(x)$的平均值。</p><p>对于离散型随机变量，可以通过求和得到：</p><script type="math/tex; mode=display">\mathbb{E}_{X \sim P}[f(x)] = \sum\limits_x P(x)f(x)</script><p>对于连续型随机变量，可以通过求积分得到：</p><script type="math/tex; mode=display">\mathbb{E}_{X \sim P}[f(x)] = \int\limits P(x)f(x) dx</script><p>另外，期望是线性的：</p><script type="math/tex; mode=display">\mathbb{E}_{X}[\alpha f(x) + \beta g(x)] = \alpha \mathbb{E}_{X}[f(x)] + \beta  \mathbb{E}_{X}[g(x)]</script><p><span style="color:red">方差（Variance）</span>：依据随机变量$x$的概率分布，衡量随机变量$x$的函数值会呈现多大的差异。描述采样得到的函数值在期望上的波动程度：</p><script type="math/tex; mode=display">Var(f(x)) = \mathsf{E}[(f(x) - \mathbb{E}[f(x)])^2]</script><p>将方差开平方即为<span style="color:red">标准差（Standard Deviation）</span>。</p><p><span style="color:red">协方差（Covariance）</span>：衡量两组值之间的线性相关程度。</p><script type="math/tex; mode=display">Cov(f(x), g(y)) = \mathbb{E}[\left(f(x) - \mathbb{E}[f(x)]\right) \left(g(y) - \mathbb{E}[g(y)] \right)]</script><p>注意：独立比0协方差要求更强，因为独立还排除了非线性的相关。</p><img src="/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200801143413314.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/21/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E6%A6%82%E7%8E%87%E8%AE%BA/image-20200801143413314.png" srcset="/img/loading.gif" alt></p><h2 id="3-5-常用概率分布"><a href="#3-5-常用概率分布" class="headerlink" title="3.5 常用概率分布"></a>3.5 常用概率分布</h2><h3 id="3-5-1-伯努利分布（两点分布）"><a href="#3-5-1-伯努利分布（两点分布）" class="headerlink" title="3.5.1 伯努利分布（两点分布）"></a>3.5.1 伯努利分布（两点分布）</h3><p><span style="color:red">伯努利分布（Bernoulli Distribution）</span>：单个二值（0/1）随机变量的分布。</p><script type="math/tex; mode=display">\begin{align}& P(x = 1) = \phi \newline& P(x = 0) = 1 - \phi \newline& P(\mathsf{x} = x) = {\phi}^x (1 - \phi)^{1 - x}\end{align}</script><p>表示一次试验的结果要么成功要么失败。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
      <tag>概率论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2016-ICCS-Parallel Attribute Exploration</title>
    <link href="/2020/07/14/2016-ICCS-Parallel%20Attribute%20Exploration/"/>
    <url>/2020/07/14/2016-ICCS-Parallel%20Attribute%20Exploration/</url>
    
    <content type="html"><![CDATA[<h1 id="Parallel-Attribute-Exploration"><a href="#Parallel-Attribute-Exploration" class="headerlink" title="Parallel Attribute Exploration"></a>Parallel Attribute Exploration</h1><p class="note note-info">属性探索、并行算法</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel。</p><p>​            2. 会议：International Conference on Conceptual Structures。</p><p>​            3. 时间：2016。</p><p>吴恩达的三遍读文献法</p><h2 id="1-第一遍：标题、摘要、关键词（5-minutes）"><a href="#1-第一遍：标题、摘要、关键词（5-minutes）" class="headerlink" title="1 第一遍：标题、摘要、关键词（5 minutes）"></a>1 第一遍：标题、摘要、关键词（5 minutes）</h2><p>1.对无法获得的形式背景的属性探索增加了专家交互。</p><p>2.提出并分析一种并行属性探索的算法。</p><h2 id="2-第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30-minutes）"><a href="#2-第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30-minutes）" class="headerlink" title="2 第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30 minutes）"></a>2 第二遍：导言、结论、图表以及快速扫描剩余内容（小结论的总结，但跳过补充信息），把握论文中的关键信息。（30 minutes）</h2><h3 id="2-1-Introduction"><a href="#2-1-Introduction" class="headerlink" title="2.1 Introduction"></a>2.1 Introduction</h3><p>1.文献[5]中，Ganter 引入 NextCloure 来计算主基并证明了算法的正确性。</p><p>2.属性探索是 NextCloure 算法的一种扩展。</p><p>3.在文献[10, 11]中，作者引入 NextCloures 算法以非线性顺序实现了算法的并行。</p><p>4.文献[9]中描述了以 Java 8 来实现 NextCloures 算法。</p><p>5.本文向 NextCloures 算法中引入专家交互来处理不完备的形式背景，此外还可利用子形式背景来减少向专家询问的问题数量。</p><h3 id="2-2-Discussion"><a href="#2-2-Discussion" class="headerlink" title="2.2 Discussion"></a>2.2 Discussion</h3><p>1.文献[4-7, 13]对经典属性探索算法引入了多个专家，但对时间性能没有任何提升。</p><p>2.比较本文 Algorithm 1 与经典属性探索算法的不同，经典属性探索算法以字典序来枚举，而本文以蕴涵前提的基数递增来进行枚举。</p><ol><li>一方面，这意味着多个蕴涵式能以并行的方式进行处理。</li><li>另一方面，以前提基数递增向专家询问问题的难度将会增加。</li></ol><p>3.对于多个专家的考虑，</p><ol><li>随机选择一个专家并提出问题。</li><li>向所有专家提出问题，返回第一个答案。</li><li>向所有专家提出问题，所有专家接受才接受。</li><li>向所有专家提出问题，至少一个专家接受才接受。</li></ol><h3 id="2-3-Conclusion"><a href="#2-3-Conclusion" class="headerlink" title="2.3 Conclusion"></a>2.3 Conclusion</h3><p>1.本文考虑了并行属性探索来处理不完备形式背景的问题。</p><p>2.该 Parallel Attribute Exploration 算法是 文献[10, 11] NextCloures 算法的一个扩展。</p><p>3.下一步，作者将考虑将该算法扩展到处理背景知识的情形。</p><p>4.该算法可以扩展到分级完全格描述的数据集的情形。</p><h2 id="3-第三遍：整篇论文，跳过看不懂的公式、技术术语。（2-hours）"><a href="#3-第三遍：整篇论文，跳过看不懂的公式、技术术语。（2-hours）" class="headerlink" title="3 第三遍：整篇论文，跳过看不懂的公式、技术术语。（2 hours）"></a>3 第三遍：整篇论文，跳过看不懂的公式、技术术语。（2 hours）</h2><h3 id="3-1-Abstract"><a href="#3-1-Abstract" class="headerlink" title="3.1 Abstract"></a>3.1 Abstract</h3><p>研究背景：形式背景的主基是最小的完整非冗余蕴涵基。最近一篇论文为主基的并行计算提供了一种新的算法。</p><p>本文提出并分析了一种支持并行属性探索的算法，并引入专家交互以便探索不可访问的形式背景的蕴涵基。</p><h3 id="3-2-Introduction"><a href="#3-2-Introduction" class="headerlink" title="3.2 Introduction"></a>3.2 Introduction</h3><p>1.蕴涵式是一种易于理解的逻辑知识表示方式。</p><p>2.如果形式背景是完备的，则能够计算出该形式背景的蕴涵基是完备非冗余的$^{[8]}$。</p><p>3.Ganter$^{[5]}$ 引入了计算主基的算法 NextCloures，并证明了该算法的正确性。</p><p>4.在数据库领域，Maier$^{[12]}$研究了依赖关系的推导，但没有提供明确的依赖关系依赖基的构造。</p><p>5.对于不完备的形式背景的情况，Ganter$^{[4-6]}$和 Stumme$^{[13]}$已经开发了一种称为属性探索的技术。</p><ul><li>该算法是 NextCloure 算法的扩展。</li><li>计算的顺序为线性序即字典序，无法并行。</li><li>在文献$^{[10, 11]}$中，作者引入 NextCloure 算法以非线性序即蕴涵前提基数递增的顺序并行地计算主基。</li></ul><p>6.在文献$^{[9]}$中可以找到编程语言 Java 8 中的 NextClosure 算法实现。</p><p>本文对 NextClosure 进行了扩展，增加了专家交互的可能性。更具体地说，我们假设有一个描述感兴趣的领域的正式上下文，但它是不可访问的，并且有一位专家(或一组专家)可以正确地决定暗示是否在此上下文中成立，如果她反驳，则还提供了一个反例。此外，可能存在观察到的全域上下文的子上下文，其用于减少向专家提出的问题的数量。使用属性探索技术，可以构造域上下文的最小隐含基础。将在以下各节中介绍的算法 Parallel Attribute Exploration 实现了此技术，并进一步允许并行执行。</p><h3 id="3-3-Formal-Concept-Analysis"><a href="#3-3-Formal-Concept-Analysis" class="headerlink" title="3.3 Formal Concept Analysis"></a>3.3 Formal Concept Analysis</h3><h3 id="3-4-Experts"><a href="#3-4-Experts" class="headerlink" title="3.4 Experts"></a>3.4 Experts</h3><p>专家是在某个感兴趣的领域正确回答问题的先知。这些问题是以与蕴涵式的形式表达的，专家可以接受也可以拒绝。如果专家接受一个蕴涵式，那么它必须适用于感兴趣的领域中的所有对象，否则她必须返回一个反例，即作为拒绝的对象。</p><p>基本概念：</p><ul><li>$M$：属性集。</li><li>M上的专家是一个部分映射$\chi: Imp(M) \rightarrow _p {\wp}(M)$。</li><li>$Imp(\chi)$：专家接受的蕴涵式集合。</li><li>$Cex(\chi) := \{ C|\exist X, Y\subseteq M: \chi(X\rightarrow Y) = C \}$：专家$\chi$拒绝的反例构成的集合。</li></ul><p>1.如果$\chi (X \rightarrow Y) = C \Rightarrow X\subseteq C\ and\ Y \subsetneq C$，则将$C$作为$X\rightarrow Y$的一个反例。</p><p>2.如果$\chi (X \rightarrow Y)$未定义，则专家$\chi$给出的反例必须与接受的蕴涵式不冲突即$\chi (U \rightarrow V) = C \Rightarrow X\subsetneq C\ or\ Y \subseteq C$。</p><p>3.如果专家$\chi$接受蕴涵式$X\rightarrow Y$，则记为$\chi \models X\rightarrow Y$。</p><h3 id="3-5-Parallel-Attribute-Exploration"><a href="#3-5-Parallel-Attribute-Exploration" class="headerlink" title="3.5 Parallel Attribute Exploration"></a>3.5 Parallel Attribute Exploration</h3><h3 id="3-6-Discussion"><a href="#3-6-Discussion" class="headerlink" title="3.6 Discussion"></a>3.6 Discussion</h3><h3 id="3-7-Conclusion"><a href="#3-7-Conclusion" class="headerlink" title="3.7 Conclusion"></a>3.7 Conclusion</h3><p>本文考虑并行属性探索的问题，提出的算法 Parallel Attribute Exploration 是算法 NextClosure 的扩展，并提供了一个原型实现$^{[9]}$，计划将其用于协作知识获取平台。</p><p>下一步，该算法将进一步扩展已处理背景知识，正如经典属性探索所做的那样$^{[4, 13]}$。在此基础上，将该算法推广到数据集在分级完备格上用闭包算子描述的情况。</p><p>词汇积累</p><ul><li>straight-forward：直接的。</li><li>quotient：商。</li><li>oracle：先知。</li></ul><h2 id="4-提问题，检验对文章关键信息的了解。"><a href="#4-提问题，检验对文章关键信息的了解。" class="headerlink" title="4 提问题，检验对文章关键信息的了解。"></a>4 提问题，检验对文章关键信息的了解。</h2><blockquote><p>1.这篇论文作者的目标是什么？或者实现了什么？</p></blockquote><p>作者通过引入专家交互和非线性序即以蕴涵前提基数递增的顺序对 NextClosure 算法的扩展实现了在不可访问的形式背景下属性探索的并行化。</p><blockquote><p>2.文中新方法/技术的关键要素是什么？</p></blockquote><p>专家交互:</p><p>蕴涵前提基数递增的顺序:</p><blockquote><p>3.论文中哪些内容对我有用？</p><p>4.我还想关注哪些文献？</p></blockquote><p>[4]: Bernhard Ganter. “Attribute Exploration with Background Knowledge”. In: Theor.Comput. Sci. 217.2 (1999), pp. 215–233.</p><p>[6]: Bernhard Ganter. “Two Basic Algorithms in Concept Analysis”. In: Formal Concept Analysis, 8th International Conference, ICFCA 2010, Agadir, Morocco, March 15-18, Proceedings. 2010, pp. 312–340.</p><p>[9]: Francesco Kriegel. Concept Explorer FX. Software for Formal Concept Analysis. 2010–2016. url: <a href="https://github.com/francesco-kriegel/conexp-fx" target="_blank" rel="noopener">https://github.com/francesco-kriegel/conexp-fx</a>.</p><p>[11]: Francesco Kriegel and Daniel Borchmann. “NextClosures: Parallel Computation of the Canonical Base”. In: Proceedings of the 12th International Conference on Concept Lattices and Their Applications (CLA 2015), Clermont-F errand, France, October 13-16, 2015, pp. 181–192.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>并行算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第二章 线性代数</title>
    <link href="/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    <url>/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="第二章-线性代数"><a href="#第二章-线性代数" class="headerlink" title="第二章 线性代数"></a>第二章 线性代数</h1><p class="note note-info">深度学习、花书、线性代数</p><a id="more"></a><h2 id="2-1-标量、向量、矩阵和张量"><a href="#2-1-标量、向量、矩阵和张量" class="headerlink" title="2.1 标量、向量、矩阵和张量"></a>2.1 标量、向量、矩阵和张量</h2><p>基本概念</p><ul><li><span style="color:red">标量（scalar）</span>：一个标量就是一个单独的数。</li></ul><p style="text-indent:2em">它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用<i>斜体</i>表示标量。标量通常被赋予<i>小写的变量名称</i>。当我们介绍标量时，会明确它们是哪种类型的数。比如，在定义实数标量时，我们可能会说 ‘‘令 <i>s ∈ R</i> 表示一条线的斜率’’；在定义自然数标量时，我们可能会说 ‘‘令 <i>n ∈ N</i> 表示元素的数目’’。</p><ul><li><span style="color:red">向量（vector）</span>：一个向量就是有序排列的一列数。通过次序中的索引，我们可以确定每个单独的数。</li></ul><p style="text-indent:2em">通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量<b>粗体的小写变量名称</b>，比如 <b>x</b>。向量中的元素可以通过带<i>脚标的斜体</i>表示。向量 <b>x</b> 的第一个元素是 <b>x</b><sub><i>1</i></sub>，第二个元素是 <b>x</b><sub><i>2</i></sub>，等等。</p><p>$\mathbb{R}^n$：如果每个元素都属于 $\mathbb{R}$，并且该向量有 $n$ 个元素，那么该向量属于实数集 $\mathbb{R}$ 的 $n$ 次笛卡尔乘积构成的集合，记为 $\mathbb{R}^n$。</p><ul><li><p><span style="color:red">—-</span>：集合补集的索引。比如 $x_{-1}$ 表示 $x$ 中除 $x_1$ 外的所有元素。</p></li><li><p><span style="color:red">矩阵（matrix）</span>：矩阵是一个二维数组，其中的每一个元素被两个索引（而非一个）所确定。</p><ul><li>$A_{i,:}$：$A$ 的第 $i$ 行（row）。</li><li>$A_{:,i}$： $A$的第 $i$ 列（column）。</li><li>$f(A)_{i,j}$：表示函数$f$ 作用在$A$上输出的矩阵的第 $i$行第 $j$ 列元素。</li></ul></li><li><p><span style="color:red">张量（tensor）</span>：一个数组中的元素分布在若干维坐标（坐标超过两维）的规则网格中，我们称之为张量。我们使用字体 <b>A</b> 来表示张量 “A’’。张量 <b>A</b> 中坐标为 (i, j, k) 的元素记作 $\bf{A_{i,j,k} }$。</p></li><li><p><span style="color:red">主对角线（main diagonal）</span>：从左上角到右下角的对角线。</p></li><li><p><span style="color:red">广播（broadcasting）</span>：向量$b$ 和矩阵$A$ 的每一行相加。这个简写方法使我们无需在加法操作前定义一个将向量 $b$ 复制到每一行而生成的矩阵。这种隐式地复制向量 $b$ 到很多位置的方式称为广播。即$C=A+b\Leftrightarrow C_{i,j}=A_{i,j}+b_j$。</p></li></ul><h2 id="2-2-矩阵与向量相乘"><a href="#2-2-矩阵与向量相乘" class="headerlink" title="2.2 矩阵与向量相乘"></a>2.2 矩阵与向量相乘</h2><p>基本概念</p><ul><li><span style="color:red">元素对应乘积（element-wise product）或者 <strong>Hadamard</strong> 乘积（Hadamard product）</span>：两个矩阵中对应元素的乘积，记为$A\odot B$。</li><li><span style="color:red">点积（dot product）</span>：两个相同维数的向量 <strong>x</strong> 和 <strong>y</strong> 的 点积（dot product）可看作是矩阵乘积$x^Ty$。注：$x^Ty=y^Tx$。</li></ul><h2 id="2-3-线性相关与生成子空间"><a href="#2-3-线性相关与生成子空间" class="headerlink" title="2.3 线性相关与生成子空间"></a>2.3 线性相关与生成子空间</h2><p>基本概念</p><ul><li><span style="color:red">线性组合（linear combination）</span>：<br><p style="text-indent:2em">为了分析方程有多少个解，我们可以将 A 的列向量看作从 原点（origin）（元素都是零的向量）出发的不同方向，确定有多少种方法可以到达向量 b。在这个观点下，向量 x 中的每个元素表示我们应该沿着这些方向走多远，即 xi 表示我们需要沿着第 i 个向量的方向走多远：</p><script type="math/tex; mode=display">  Ax=\sum\limits_i x_iA_{:,i}</script><p style="text-indent:2em">一般而言，这种操作被称为 线性组合（linear combination）。形式上，一组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即：</p><script type="math/tex; mode=display">  \sum\limits_i c_iv^{(i)}</script></li><li><span style="color:red">列空间（column space）或值域（range）</span>：确定 <strong>Ax</strong> = <strong>b</strong> 是否有解相当于确定向量 <strong>b</strong> 是否在 <strong>A</strong> 列向量的生成子空间中。这个特殊的生成子空间被称为 <strong>A</strong> 的 列空间（column space）或者 <strong>A</strong> 的 值域（range）。</li><li><span style="color:red">线性无关（linearly independent）</span>：如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为 线性无关（linearly independent）。</li><li><span style="color:red">奇异的（singular）</span>：一个列向量线性相关的方阵。</li></ul><h2 id="2-4-范数"><a href="#2-4-范数" class="headerlink" title="2.4 范数"></a>2.4 范数</h2><p style="text-indent:2em"><span style="color:red">范数（singular）</span>是将向量映射到非负值的函数。直观上来说，向量 x 的范数衡量从原点到点 x 的距离。更严格地说，范数是满足下列性质的任意函数：</p><ul><li>$f(x)=0\Rightarrow x=0$</li><li>$f(x+y)\le f(x)+f(y)$  （三角不等式（triangle inequality））</li><li>$\forall \alpha\in \mathbb{R},\ f(\alpha x)=|\alpha|f(x)$</li></ul><p>$L^p$范数：</p><script type="math/tex; mode=display">\parallel x\parallel_p=\left(\sum\limits_i |x_i|^p \right)^{\frac1p}\qquad 其中p\in\mathbb{R},\ p\ge1。</script><p style="text-indent:2em"><span style="color:red">L<sup>2</sup> 范数（singular）</span>被称为欧几里得范数（Euclidean norm）。它表示从原点出发到向量 x 确定的点的欧几里得距离。</p><script type="math/tex; mode=display">\parallel x\parallel_2=\left(\sum\limits_i |x_i|^2 \right)^{\frac12}</script><p style="text-indent:2em">两个向量的 点积（dot product）可以用范数来表示。具体地：</p><script type="math/tex; mode=display">x^Ty=\parallel x\parallel_2\parallel y\parallel_2 \cos\theta\qquad 其中\theta表示x和y之间的夹角。</script><p><span style="color:red">平方L<sup>2</sup> 范数（singular）</span>：</p><script type="math/tex; mode=display">\parallel x\parallel_2=\sum\limits_i |x_i|^2=x^Tx</script><p style="text-indent:2em">平方 L<sup>2</sup> 范数对x 中每个元素的导数只取决于对应的元素，而 L<sup>2</sup> 范数对每个元素的导数却和整个向量相关。但是在很多情况下，平方 L<sup>2</sup> 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。</p><p><span style="color:red">L<sup>1</sup> 范数（singular）</span>：</p><script type="math/tex; mode=display">\parallel x\parallel_1=\sum\limits_i |x_i|</script><p style="text-indent:2em">当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 L<sup>1</sup> 范数。每当x 中某个元素从 0 增加 ϵ，对应的 L<sup>1</sup> 范数也会增加 ϵ。</p><p><span style="color:red">L<sup>&infin;</sup> 范数（singular）</span>：表示向量中具有最大幅值的元素的绝对值。</p><script type="math/tex; mode=display">\parallel x\parallel_{\infin}=\max\limits_i |x_i|</script><p><span style="color:red"><strong>Frobenius</strong> 范数（Frobenius norm）</span>：衡量矩阵的大小。</p><script type="math/tex; mode=display">\parallel A\parallel_F=\sqrt{\sum\limits_{i,j} A_{i,j}^2}</script><h2 id="2-5-矩阵分解"><a href="#2-5-矩阵分解" class="headerlink" title="2.5 矩阵分解"></a>2.5 矩阵分解</h2><p><span style="color:red">特征分解（eigendecomposition）</span>：将矩阵分解成一组特征向量和特征值。</p><script type="math/tex; mode=display">Av=\lambda v</script><p>标量$\lambda$为特征向量$v$对应的特征值（eigenvalue）。（类似地，我们也可以定义 左特征向量（left eigenvector）$v^⊤A = v^⊤λ$，但是通常我们更关注右特征向量（right eigenvector））。</p><p>假设矩阵 <strong>A</strong> 有 <em>n</em> 个线性无关的特征向量 $\{ v^{(1)}, . . . , v^{(n)} \}$，对应着特征值$\{ λ_1, . . . , λ_n \}$。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$V = \{ v^{(1)}, . . . , v^{(n)} \}$类似地，我们也可以将特征值连接成一个向量 $λ = \{ λ_1, . . . , λ_n \}^T$。因此 <strong>A</strong> 的 特征分解（eigendecomposition）可以记作</p><script type="math/tex; mode=display">A = V diag(λ) V^{-1}</script><p><span style="color:red">正交分解</span>：每个实对称矩阵都可以分解为实特征向量和实特征值。</p><script type="math/tex; mode=display">A = Q \Lambda Q^T</script><p>其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\lambda_i$对应的特征向量是矩阵$Q$的第$i$列，记作$Q_{:,i}$。我们通常按降序排列 的元素。在该约$\Lambda$定下，特征分解唯一当且仅当所有的特征值都是唯一的。</p><p>我们可以将 A 看作沿方向 $v^{(i)}$ 延展 $λ_i$ 倍的空间。如图2.3 所示的例子。</p><img src="/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/image-20200713160411504.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/image-20200713160411504.png" srcset="/img/loading.gif" alt></p><p>实对称矩阵的特征分解也可以用于优化二次方程 $f(x) = x^⊤Ax$，其中限制 $∥x∥_2 = 1$。当 $x$等于 $A$ 的某个特征向量时，$f$ 将返回对应的特征值。在限制条件下，函数  的最大值$f$是最大特征值，最小值是最小特征值。</p><p><span style="color:red">奇异值分解（singular value decomposition, SVD）</span>：将矩阵分解为 奇异向量（singular vector）和 奇异值（singular value）。</p><script type="math/tex; mode=display">A = U D V^T</script><p>其中假设$A$是一个$m \times n$的矩阵，那么$U$将是一个$m \times m$的方阵，$D$是一个$m \times n$的矩阵，$V$是一个$n \times n$的方阵。</p><p>这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵 $U$ 和 $V$ 都定义为正交矩阵，而矩阵 $D$ 定义为对角矩阵。注意，矩阵 $D$ 不一定是方阵。</p><p>对角矩阵 $D$ 对角线上的元素被称为矩阵 $A$ 的 奇异值（singular value）。矩阵$U$ 的列向量被称为 左奇异向量（left singular vector），矩阵 $V$ 的列向量被称 右奇异向量（right singular vector）。</p><p>$A$的<span style="color:red">左奇异向量（left singular vector）</span>是 $AA^⊤$ 的特征向量。</p><p>$A$的<span style="color:red">右奇异向量（right singular vector）</span>是 $A^⊤A$ 的特征向量。</p><p>$A$的<span style="color:red">非零奇异值</span>是 $A^⊤A$ 特征值的平方根，同时也是$AA^⊤$ 特征值的平方根。</p><h2 id="2-6-Moore-Penrose-伪逆、迹运算、行列式"><a href="#2-6-Moore-Penrose-伪逆、迹运算、行列式" class="headerlink" title="2.6 Moore-Penrose 伪逆、迹运算、行列式"></a>2.6 Moore-Penrose 伪逆、迹运算、行列式</h2><p> <span style="color:red">Moore-Penrose 伪逆（Moore-Penrose pseudoinverse）</span>：矩阵$A$的伪逆定义为</p><script type="math/tex; mode=display">A^+ = \lim\limits_{\alpha \searrow 0}(A^{\top} A + \alpha I)^{-1}A^{\top}</script><p>但实际的伪逆算法没有基于这个定义，而是</p><script type="math/tex; mode=display">A^+ = V D^+ U^{\top}</script><p>其中，矩阵$U,\ D,\ V$分别是矩阵$A$奇异值分解后得到的矩阵。对角矩阵$D$的伪逆$D^+$是其非0元素取倒数之后再转置得到的。</p><p>当矩阵$A$的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x = A^+ y$ 是方程所有可行解中欧几里得范数 $∥x∥_2$ 最小的一个。</p><p>当矩阵 $A$ 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的 $x$使得  $Ax$和$y$的欧几里得距离 $∥Ax - y∥_2$ 最小。</p><p><span style="color:red">迹运算</span>：</p><script type="math/tex; mode=display">\begin{align}\parallel A \parallel_F &= \sqrt{Tr(AA^{\top})} \newlineTr(A) &= Tr(A^{\top}) \newlineTr(ABC) &= Tr(BCA) = Tr(CAB) \newlineTr(AB) &= Tr(BA) \end{align}</script><p><span style="color:red">行列式</span>：记作 $det(A)$，是一个将方阵  $A$ 映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。</p><p>如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的体积。如果行列式是 1，那么这个转换保持空间体积不变。</p><h2 id="2-7-PCA（主成分分析，principal-components-analysis）"><a href="#2-7-PCA（主成分分析，principal-components-analysis）" class="headerlink" title="2.7 PCA（主成分分析，principal components analysis）"></a>2.7 PCA（主成分分析，principal components analysis）</h2><hr><p>问题描述：假设我们有 $m$ 个数据点 $x^{(1)}, . . . , x^{(m)}∈\mathbb{R}^n$，对于每个数据点$x^{(i)}$ ，我们希望找到⼀个对应的点 $c^{(i)} \in \mathbb{R}^l,\ l\lt n$去表⽰它 (相当于对它进⾏降维)，并且让损失的信息量尽可能少。</p><hr><p>解题思路</p><p>我们可以将这个过程看作是一个编码解码的过程。</p><p>编码函数：$f(x) = c$；</p><p>解码函数：$x \approx g(f(x))$；</p><p>则 PCA 由我们选择的解码函数确定，而为了简化解码器，我们使用矩阵乘法将编码映射回$\mathbb{R}^n$，即$g(c) = Dc$，其中$D \in \mathbb{R}^{n \times l}$是定义的解码矩阵，且$D$的列向量相互正交。</p><p>此时，问题可能有多个解。而为了获得唯一解，则可假定$D$的所有列向量均具有单位范数。</p><p>对于给定的$x$，我们需要找到信息损失最小的$c^*$，即</p><script type="math/tex; mode=display">c^* = \arg\min\limits_c ||x - g(c)||_2 = \arg\min\limits_c ||x - g(c)||_2^2</script><p>我们用$L^2$范数来衡量它们的距离，又因$L^2$范数是非负的并且其平方运算在非负值上单调递增，则两者在相同的$c$上取得最小值，则可用平方$L^2$范数代替$L^2$范数。而</p><script type="math/tex; mode=display">||x - g(c)||_2 = (x - g(c))^{\top}(x - g(c)) = x^{\top}x - x^{\top}g(c) - {g(c)}^{\top}x + {g(c)}^{\top}{g(c)}</script><p>则可忽略不依赖$c$的$x^{\top}x$，则有</p><script type="math/tex; mode=display">\begin{align}c^* &= \arg\min\limits_c[x^{\top}x - x^{\top}g(c) - {g(c)}^{\top}x + {g(c)}^{\top}{g(c)}] \newline&= \arg\min\limits_c[-2x^{\top}g(c) + {g(c)}^{\top}{g(c)}] \newline代入g(c)=Dc， &= \arg\min\limits_c[-2x^{\top}Dc + {\left(Dc\right)}^{\top}{Dc}] \newline&= \arg\min\limits_c[-2x^{\top}Dc + c^{\top}D^{\top}{Dc}] \newline由于D具有单位正交性，&= \arg\min\limits_c[-2x^{\top}Dc + c^{\top}I_l c] \newline&= \arg\min\limits_c[-2x^{\top}Dc + c^{\top} c] \newline\end{align}</script><p>则对$c$求梯度，并令其为0有</p><script type="math/tex; mode=display">\begin{align}\nabla_c (-2x^{\top}Dc + c^{\top} c) &= 0 \newline-2D^{\top}x + 2c &= 0 \newline则， c &= D^{\top}x\end{align}</script><p>因此，</p><p>编码函数：$f(x) = c = D^{\top}x$；</p><p>编码解码得到的重构函数：$r(x) = g(f(x)) = g(c) = Dc = DD^{\top}x$；</p><p>接下来，我们就需挑选最优的编码矩阵$D$，所以我们必须最小化所有维度和所有点上的误差矩阵的 Frobenius 范数。</p><script type="math/tex; mode=display">D^* = \arg\min\limits_D \sqrt{\sum\limits_{i,j} \left(x_j^{(i)} - r(x^{(i)})_j \right)^2} \qquad subject\ to\ D^{\top}D = I_l</script><p>而为了方便，我们考虑$l = 1$的情况，此时问题简化为</p><script type="math/tex; mode=display">\begin{align}&d^* = \arg\min\limits_d \sum\limits_i \left( x_j^{(i)} - dd^{\top}x^{(i)} \right)^2 \newline&s.t.\ d^{\top}d = 1\end{align}</script><p>考虑标量$d^{\top}x^{(i)}$的转置与自身相等，则可化为</p><script type="math/tex; mode=display">\begin{align}d^* &= \arg\min\limits_d \sum\limits_i \left( x_j^{(i)} - d^{\top}x^{(i)}d \right)^2 \quad s.t.\ d^{\top}d = 1 \newline&= \arg\min\limits_d \sum\limits_i \left( x_j^{(i)} - {x^{(i)}}^{\top}dd \right)^2 \end{align}</script><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-第一章 引言</title>
    <link href="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/"/>
    <url>/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h1><p class="note note-info">深度学习、花书</p><a id="more"></a><h2 id="1-早期问题"><a href="#1-早期问题" class="headerlink" title="1 早期问题"></a>1 早期问题</h2><p style="text-indent:2em">在人工智能的早期，那些对人类智力来说非常困难、但对计算机来说相对简单的问题得到迅速解决。比如，那些可以通过一系列形式化的数学规则来描述的问题。</p><p style="text-indent:2em">人工智能的真正挑战在于解决那些对人来说很容易执行、但很难形式化描述的任务，如识别人们所说的话或图像中的脸。对于这些问题，我们人类往往可以凭借直觉轻易地解决。</p><p>针对这些比较直观的问题，本书讨论一种解决方案<span style="color:red">(AI 深度学习（deep learning）)</span>。</p><ul><li>该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。</li><li>让计算机从经验获取知识，可以避免由人类来给计算机形式化地指定它需要的所有知识。</li><li>层次化的概念让计算机构建较简单的概念来学习复杂概念。</li></ul><p>一个<span style="color:red">关键挑战</span>：如何将这些非形式化的知识传达给计算机。</p><p><span style="color:red">硬编码 (hard code)</span>：计算机可以使用逻辑推理规则来自动地理解这些形式化语言中的声明。这就是众所周知的人工智能的知识库（knowledge base）方法。</p><p><span style="color:red">机器学习（machine learning）</span>：由于依靠硬编码的知识体系面对的困难表明，AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。</p><p>引入机器学习使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决</p><p>策。如：</p><ul><li>一个被称为 逻辑回归（logistic regression）的简单机器学习算法可以决定是否建议剖腹产 (Mor-Yosef <em>et al.</em>, 1990)。</li><li>简单机器学习算法朴素贝叶斯（naive Bayes）则可以区分垃圾电子邮件和合法电子邮件。</li></ul><p>但是这些简单的机器学习算法的性能在很大程度上依赖于给定数据的<span style="color:red">表示（repre sentation）</span>。，表示的选择对机器学习算法的性能产生巨大的影响。</p><p style="text-indent:2em">许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。例如，对于通过声音鉴别说话者的任务来说，一个有用的特征是对其声道大小的估计。这个特征为判断说话者是男性、女性还是儿童提供了有力线索。</p><p style="text-indent:2em">然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所以我们可能会想用车轮的存在与否作为特征。不幸的是，我们难以准确地根据像素值来描述车轮看上去像什么。虽然车轮具有简单的几何形状，但它的图像可能会因场景而异，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等。</p><p style="text-indent:2em">解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为<span style="color:red">表示学习（representation learning）</span>。</p><p style="text-indent:2em">表示学习算法的典型例子是<span style="color:red">自编码器</span>（autoencoder）。自编码器由一个编码器encoder）函数和一个解码器（decoder）函数组合而成。</p><p><span style="color:red">编码器</span>：编码器函数将输入数据转换为一种不同的表示。</p><p><span style="color:red">解码器</span>：解码器函数则将这个新的表示转换到原来的形式。</p><p style="text-indent:2em">我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。为了实现不同的特性，我们可以设计不同形式的自编码器。</p><p style="text-indent:2em">当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的<span style="color:red">变差因素（factors of variation）</span>。在此背景下，‘‘因素’’ 这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。</p><p style="text-indent:2em">显然，从原始数据中提取如此高层次、抽象的特征是非常困难的。许多诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类水平的理解来辨识。这几乎与获得原问题的表示一样困难，因此，乍一看，表示学习似乎并不能帮助我们。</p><p><span style="color:red">深度学习（deep learning）</span>通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。深度学习让计算机通过较简单概念构建复杂的概念。图 1.2 展示了深度学习系统。</p><img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710112001631.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710112001631.png" srcset="/img/loading.gif" alt></p><p><span style="color:red">多层感知机（multilayer perceptron, MLP）</span>：是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。</p><p>种度量模型深度的方式：</p><ol><li>基于评估架构所需执行的顺序指令的数目。<br><p style="text-indent:2em">假设我们将模型表示为给定输入后，计算对应输出的流程图，则可以将这张流程图中的最长路径视为模型的深度。正如两个使用不同语言编写的等价程序将具有不同的长度；相同的函数可以被绘制为具有不同深度的流程图，其深度取决于我们可以用来作为一个步骤的函数。图 1.3 说明了语言的选择如何给相同的架构两个不同的衡量。</p><img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710113610351.png" srcset="/img/loading.gif" class></li></ol><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710113610351.png" srcset="/img/loading.gif" alt></p><ol><li>将描述概念彼此如何关联的图的深度视为模型深度。<p style="text-indent:2em">在这种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图更深。这是因为系统对较简单概念的理解在给出更复杂概念的信息后可以进一步精细化。例如，一个 AI 系统观察其中一只眼睛在阴影中的脸部图像时，它最初可能只看到一只眼睛。但当检测到脸部的存在后，系统可以推断第二只眼睛也可能是存在的。在这种情况下，概念的图仅包括两层（关于眼睛的层和关于脸的层），但如果我们细化每个概念的估计将需要额外的 n 次计算，即计算的图将包含 2n 层。 </p></li></ol><p style="text-indent:2em">深度学习是一种特定类型的机器学习，具有强大的能力和灵活性，它将大千世界表示为嵌套的层次概念体系（由较简单概念间的联系定义复杂概念、从一般抽象概括到高级抽象表示）。</p><p>图1.4 说明了这些不同的 AI 学科之间的关系。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115035883.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115035883.png" srcset="/img/loading.gif" alt></p><p>图 1.5 展示了每个学科如何工作的高层次原理。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115508658.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115508658.png" srcset="/img/loading.gif" alt></p><p>图 1.6 展示了本书的组织架构。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115751350.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115751350.png" srcset="/img/loading.gif" alt></p><h2 id="2-深度学习的历史趋势"><a href="#2-深度学习的历史趋势" class="headerlink" title="2 深度学习的历史趋势"></a>2 深度学习的历史趋势</h2><h3 id="2-1-神经网络的众多名称和命运变迁"><a href="#2-1-神经网络的众多名称和命运变迁" class="headerlink" title="2.1 神经网络的众多名称和命运变迁"></a>2.1 神经网络的众多名称和命运变迁</h3><p>三次发展浪潮：</p><ol><li>20世纪40年代到60年代深度学习的雏形出现在控制论（cybernetics）中。</li><li>20 世纪 80 年代到 90 年代深度学习表现为联结主义（connectionism）。</li><li>直到 2006 年，才真正以深度学习之名复兴。</li></ol><p>图 1.7 给出了定量的展示。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710121022428.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710121022428.png" srcset="/img/loading.gif" alt></p><p><span style="color:red">控制论</span>：</p><ul><li>$n$个输入：$x_1, x_2, \cdots, x_n$。</li><li>一组权重：$w_1, w_2, \cdots, w_n$。</li><li>输出：$f(x, w)=x_1w_1+\cdots+x_nw_n$。</li></ul><p style="text-indent:2em">McCulloch-Pitts 神经元 (McCulloch and Pitts, 1943) 是脑功能的早期模型。该线性模型通过检验函数 f(x, w) 的正负来识别两种不同类别的输入。显然，模型的权重需要正确设置后才能使模型的输出对应于期望的类别。这些权重可以由操作人员设定。在 20 世纪 50 年代，<span style="color:red">感知机</span> (Rosenblatt, 1956, 1958) 成为第一个能根据每个类别的输入样本来学习权重的模型。约在同一时期，<span style="color:red">自适应线性单元 (adaptive linear element, ADALINE)</span> 简单地返回函数 *f*(**x**) 本身的值来预测一个实数 (Widrow and Hoffff, 1960)，并且它还可以学习从数据预测这些数。</p><p><span style="color:red">随机梯度下降（stochastic gradient descent）</span>的一种特例：调节 ADALINE 权重的训练算法。</p><p><span style="color:red">线性模型（linear model）</span>：基于感知机和 ADALINE 中使用的函数 <em>f</em>(x, w) 的模型。</p><p>局限性：无法学习异或（XOR）函数，即$f([0, 1], w)=1和f([1, 0], w)=1，但f([1, 1], w)=0和f([0, 0], w)=0$。</p><p><span style="color:red">计算神经科学</span>：了解大脑是如何在算法层面上工作的尝试。</p><p><span style="color:red">深度学习领域主要关注任务</span>：如何构建计算机系统，从而成功解决需要智能才能解决的任务。</p><p><span style="color:red">计算神经科学领域主要关注任务</span>：构建大脑如何真实工作的比较精确的模型。</p><p><span style="color:red">联结主义（connectionism）或并行分布处理 ( parallel distributed processing) </span>：当网络将大量简单的计算单元连接在一起时可以实现智能行为。</p><p><span style="color:red"> 分布式表示（distributed representation）</span>：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。</p><p style="text-indent:2em">例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统，表示这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或隐藏单元激活。这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。改善这种情况的方法之一是使用分布式表示，即用三个神经元描述颜色，三个神经元描述对象身份。这仅仅需要 6 个神经元而不是 9 个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。</p><p> 相关研究进展：</p><ul><li>在 20 世纪 90 年代，研究人员在使用神经网络进行序列建模的方面取得了重要进展。Hochreiter (1991b) 和 Bengio <em>et al.</em> (1994a) 指出了对长序列进行建模的一些根本性数学难题。</li><li>Hochreiter and Schmidhuber (1997)引入 长短期记忆（long short-term memory, LSTM）网络来解决这些难题。如今，LSTM 在许多序列建模任务中广泛应用，包括 Google 的许多自然语言处理任务。</li><li>神经网络继续在某些任务上获得令人印象深刻的表现 (LeCun <em>et al.</em>,  1998c; Bengio <em>et al.</em>, 2001a)。加拿大高级研究所（CIFAR）通过其神经计算和自适应感知（NCAP）研究计划帮助维持神经网络研究。该计划联合了分别由 Geoffffrey Hinton、Yoshua Bengio和 Yann LeCun 领导的多伦多大学、蒙特利尔大学和纽约大学的机器学习研究小组。这个多学科的 CIFAR NCAP 研究计划还囊括了神经科学家、人类和计算机视觉专家。</li></ul><p style="text-indent:2em">神经网络研究的第三次浪潮始于 2006 年的突破。Geoffffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton *et al.*, 2006a)，神经网络研究的第三次浪潮始于 2006 年的突破。eoffffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton *et al.*, 2006a)，我们将在第 15.1 节中更详细地描述。其他 CIFAR 附属研究小组很快表明，同样的策略可以被用来训练许多其他类型的深度网络 (Bengio andLeCun, 2007a; Ranzato *et al.*, 2007b)，并能系统地帮助提高在测试样例上的泛化能力。神经网络研究的这一次浪潮普及了 “深度学习’’ 这一术语的使用，强调研究者现在有能力训练以前不可能训练的比较深的神经网络，并着力于深度的理论重要性上 (Bengio and LeCun, 2007b; Delalleau and Bengio, 2011; ascanu *et al.*, 2014a; Montufar *et al.*, 2014)。此时，深度神经网络已经优于与之竞争的基于其他机器学习技术以及手工设计功能的 AI 系统。在写这本书的时候，神经网络的第三次发展浪潮仍在继续，尽管深度学习的研究重点在这一段时间内发生了巨大变化。第三次浪潮已开始着眼于新的无监督学习技术和深度模型在小数据集的泛化能力，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据集的能力。</p><h3 id="2-2-与日俱增的数据量"><a href="#2-2-与日俱增的数据量" class="headerlink" title="2.2 与日俱增的数据量"></a>2.2 与日俱增的数据量</h3><p>图 1.8 展示了基准数据集的大小如何随着时间的推移而显著增加。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154348314.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154348314.png" srcset="/img/loading.gif" alt></p><p>图 1.10 展示了神经元连接数如何随着时间的推移而显著增加。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154829899.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154829899.png" srcset="/img/loading.gif" alt></p><p>图 1.11 展示了神经元网络规模如何随着时间的推移而显著增加。</p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710155243539.png" srcset="/img/loading.gif" alt></p><h3 id="2-3-应用领域"><a href="#2-3-应用领域" class="headerlink" title="2.3 应用领域"></a>2.3 应用领域</h3><p>图像识别：</p><ol><li>最早的深度模型被用来识别裁剪紧凑且非常小的图像中的单个对象(Rumelhart <em>et al.</em>, 1986d)。</li><li>此后，神经网络可以处理的图像尺寸逐渐增加。现代对象识别网络能处理丰富的高分辨率照片，并且不需要在被识别的对象附近进行裁剪(Krizhevsky <em>et al.</em>, 2012b)。</li><li>类似地，最早的网络只能识别两种对象（或在某些情况下，单类对象的存在与否），而这些现代网络通常能够识别至少1000个不同类别的对象。对象识别中最大的比赛是每年举行的 ImageNet 大型视觉识别挑战（ILSVRC）。深度学习迅速崛起的激动人心的一幕是卷积网络第一次大幅赢得这一挑战，它将最高水准的前5 错误率从 26.1% 降到 15.3% (Krizhevsky <em>et al.</em>, 2012b)，这意味着该卷积网络针对每个图像的可能类别生成一个顺序列表，除了 15.3% 的测试样本，其他测试样本的正确类标都出现在此列表中的前 5 项里。此后，深度卷积网络连续地赢得这些比赛，截至写本书时，深度学习的最新结果将这个比赛中的前 5 错误率降到了 3.6%，如图 1.12 所示。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710165940221.png" srcset="/img/loading.gif" class><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710165940221.png" srcset="/img/loading.gif" alt></li></ol><p>语音识别：</p><ul><li>语音识别在 20 世纪 90 年代得到提高后，直到约 2000 年都停滞不前。深度学习的引入 (Dahl <em>et al.</em>, 2010; Deng <em>et al.</em>, 2010b; Seide <em>et al.</em>, 2011; Hinton <em>et al.</em>, 2012a) 使得语音识别错误率陡然下降，有些错误率甚至降低了一半。</li></ul><p>行人检测和图像分割:</p><ul><li>(Sermanet <em>et al.</em>, 2013; Farabet <em>et al.</em>, 2013; Couprie <em>et al.</em>, 2013)，并且在交通标志分类上取得了超越人类的表现 (Ciresan <em>et al.</em>, 2012)</li></ul><p style="text-indent:2em">在深度网络的规模和精度有所提高的同时，它们可以解决的任务也日益复杂。Goodfellow *et al.* (2014d) 表明，神经网络可以学习输出描述图像的整个字符序列，而不是仅仅识别单个对象。此前，人们普遍认为，这种学习需要对序列中的单个元素进行标注 (Gulcehre and Bengio, 2013)。循环神经网络，如之前提到的LSTM 序列模型，现在用于对序列和其他序列之间的关系进行建模，而不是仅仅固定输入之间的关系。这种序列到序列的学习似乎引领着另一个应用的颠覆性发展，即机器翻译 (Sutskever et al., 2014; Bahdanau et al., 2015)。</p><p>这种复杂性日益增加的趋势已将其推向逻辑结论，即神经图灵机 (Graves <em>et al.</em>, 2014) 的引入。</p><p><span style="color:red">神经图灵机 </span>：能学习读取存储单元和向存储单元写入任意内容。这样的神经网络可以从期望行为的样本中学习简单的程序。</p><p style="text-indent:2em">例如，从杂乱和排好序的样本中学习对一系列数进行排序。这种自我编程技术正处于起步阶段，但原则上未来可以适用于几乎所有的任务。</p><p>深度学习的另一个最大的成就是其在 强化学习（reinforcement learning）领域的扩展。</p><p><span style="color:red">强化学习 </span>：在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。</p><p style="text-indent:2em">DeepMind 表明，基于深度学习的强化学习系统能够学会玩Atari 视频游戏，并在多种任务中可与人类匹敌 (Mnih et al., 2015)。深度学习也显著改善了机器人强化学习的性能 (Finn et al., 2015)。</p><p>技术公司：Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflflix、NVIDIA和 NEC 等。</p><p>软件基础架构的进展：软件库如 Theano (Bergstra <em>et al.</em>, 2010a; Bastien <em>et al.</em>, 2012a)、PyLearn2 (Goodfellow <em>et al.</em>, 2013e)、Torch (Col lobert <em>et al.</em>, 2011b)、DistBelief (Dean <em>et al.</em>, 2012)、Caffffe (Jia, 2013)、MXNet (Chen <em>et al.</em>, 2015) 和 TensorFlow (Abadi <em>et al.</em>, 2015) 都能支持重要的研究项目或商业产品。</p><p style="text-indent:2em">深度学习也为其他科学做出了贡献。用于对象识别的现代卷积网络为神经科学家们提供了可以研究的视觉处理模型 (DiCarlo, 2013)。深度学习也为处理海量数据以及在科学领域作出有效的预测提供了非常有用的工具。它已成功地用于预测分子如何相互作用从而帮助制药公司设计新的药物 (Dahl et al., 2014)，搜索亚原子粒子 (Baldi et al., 2014)，以及自动解析用于构建人脑三维图的显微镜图像(Knowles-Barley et al., 2014) 等。我们期待深度学习未来能够出现在越来越多的科学领域中。</p><p style="text-indent:2em">总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的普及性和实用性都有了极大的发展。未来几年充满了进一步提高深度学习并将它带到新领域的挑战和机遇。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">deep learning</a>.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/MingchaoZhu/DeepLearning" target="_blank" rel="noopener">深度学习</a>。<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2014-Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</title>
    <link href="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/"/>
    <url>/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/</url>
    
    <content type="html"><![CDATA[<h1 id="Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises"><a href="#Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises" class="headerlink" title="Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises"></a>Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</h1><p class="note note-info">属性探索、快速算法</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Uwe Ryssel、Felix Distel、Daniel Borchmann。</p><p>​            2. 期刊：Annals of Mathematics and Artificial Intelligence。</p><p>​            3. 时间：2014。</p><p>​            4. DOI：10.1007/s10472-013-9355-9 。 </p><p style="display:none">​    @article{Ryssel2014Fast,​      title={Fast algorithms for implication bases and attribute exploration using proper premises},​      author={Ryssel, Uwe and Distel, Felix and Borchmann, Daniel},​      journal={Annals of Mathematics & Artificial Intelligence},​      volume={70},​      number={1-2},​      pages={25-53},​      year={2014},​    },​    @inproceedings{Ganter2010Two,​      title={Two Basic Algorithms in Concept Analysis},​      author={Ganter, Bernhard},​      booktitle={International Conference on Formal Concept Analysis},​      year={2010},​      },​      @article{Obiedkov2007Attribute,​      title={Attribute-incremental construction of the canonical implication basis},​      author={Obiedkov, S. and Duquenne, V.},​      journal={Annals of Mathematics & Artificial Intelligence},​      volume={49},​      number={1-4},​      pages={p.77-99},​      year={2007},​      }  ​    </p><p></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>形式概念分析的中心任务是枚举形式背景的最小蕴涵基。</li><li>本文提出了一种快速计算适当前提的新算法。<ul><li>减少了多次获得适当前提的数量。</li><li>减少了在适当前提集合内的冗余。</li></ul></li></ul><p>词汇积累：</p><ul><li>minimal hypergraph transversals：最小超图横断面。</li><li>refactoring：重构。</li><li>heuristic：启发式。</li><li>refactoring of model variants：模型变体重构。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>计算主基存在2种方法$^{[17, 26]}$，由于这两种方法均计算了概念内涵，则无法改变其指数级的复杂度$^{[1, 10]}$。</p></li><li><p>形式概念分析早期提出了一种具有适当前提基的算法，该算法避免了计算概念内涵。</p></li><li><p>有一些方法可以在多项式时间内将适当前提基转换为主基$^{[24, 28]}$。</p></li><li><p><span style="color:red">本文提出了一种快速计算适当前提的算法。基于以下三个思想：</span></p><ol><li>在适当前提和最小超图横断面之间使用一个简单的联系。</li><li>对最小超图断面的枚举问题进行了深入的研究。</li><li>可对现有算法可使用与适当前提的联系。</li></ol><p>1.首先，使用原算法遍历所有属性，并使用黑盒超图算法来计算每个属性的适当前提。<br>2.为了避免多次计算相同的适当前提，本文引入了一个候选过滤器：对属性集中的每个属性进行过滤，并且只在候选属性集中搜素合适的前提。本文表明，这种过滤方法在保持完备性的同时，大大减少了多次计算的适当前提的数量。<br>3.通过仅在交不可约属性集中搜索适当的前提来移除适当前提内的冗余。<br>4.本文认为该算法对于并行化来说是微不足道的，从而导致进一步的加速。由于其增量性质，基于主基的算法的并行化版本至今尚不为人所知。<br>5.本文将给出使用适当前提的属性探索的另一种变体。它使用与本文对主基的枚举算法相同的方法。</p></li><li><p>考虑不使用伪意图的属性探索的替代公式。在$^{[27]}$中首次尝试使用适当的前提来制定属性探索。&lt;/div</p></li></ol><p>词汇积累：</p><ul><li>well-researched：深入研究。</li><li>artifacts：史前古器物；人工产品。</li><li>negated：否定的。</li><li>fractions of a second：几分之一秒。</li><li>arguments：论点。</li></ul><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>1.$g\swarrow m$：在不包含$m$的对象内涵中，关于子集的顺序$g’$是最大的。</p><p>2.蕴涵式$\mathscr{L}$的形式化描述：</p><script type="math/tex; mode=display">\begin{align} \mathscr{L}^1(A)&=A\cup \bigcup\{Y|(X\rightarrow Y)\in \mathscr{L}, X\subseteq A\}  \newline\mathscr{L}^i(A)&=\mathscr{L}^1(\mathscr{L}^{i-1}(A) )\quad for\ i>1  \newline\mathscr{L}(A)&=\bigcup\limits_{i\in N>0}\mathscr{L}^i(A)\end{align}</script><p>原文：Then an implication X →Y follows from the set $\mathscr{L}$ of implications if and only if $Y ⊆ \mathscr{L}$(X). We write $\mathscr{L}\models(X →Y)$ if and only if X →Y follows from $\mathscr{L}$.</p><p>解释：一个蕴涵式$X\rightarrow Y$可从蕴涵式集合$\mathscr{L}$中推出来$，当且仅当$$Y\subseteq \mathscr{L}(X)$成立。可写作$\mathscr{L}\models(X →Y)$，当且仅当$X\rightarrow Y$可由$\mathscr{L}$推出来成立。</p><p><span style="color:red">sound(非冗余的、无噪声的)</span>：$\mathscr{L}$中的所有蕴涵式对于形式背景$\mathbb{K}$均成立。</p><p><span style="color:red">complete(完备的)</span>：形式背景$\mathbb{K}$中成立的所有蕴涵式均来自于$\mathscr{L}$。</p><p>如果$\mathscr{L}$对于形式背景$\mathbb{K}$是sound and complete，则$\mathscr{L}$就称为形式背景$\mathbb{K}$的一个基。进一步地，$\mathscr{L}^1(A)=\mathscr{L}(A)\ holds\ for\ all\ A\subseteq M$，则$\mathscr{L}$称为直接基。</p><p><span style="color:red">伪内涵</span>：$P\neq P^{“}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{“}\subseteq P$。则形式背景的主基为$\{P\rightarrow P^{“}|P: pseudo-intent\ of\ \mathbb{K}\}$。</p><p><span style="color:red">前提</span>：B is called a premise for m if $m\in B^{“}\backslash B$，则形式背景的一个基为：$\mathscr{L}=\{B\rightarrow B^{“}|B: B\subseteq M, premise\ for\ some\ m\in M\}$。</p><p><span style="color:red">适当前提</span>：B  is called a proper premise if $B^{\bullet}$ is not empty，其中$B^{\bullet}=B^{“}\backslash \left(B\cup\bigcup\limits_{S\subsetneq B}S^{“}\right)$。如果$m\in B^{\bullet}$，则称$m$在$m\in M$中是一个适当前提。可以得到$B$对于$m$是一个适当前提，当且仅当$B$在$m$的前提中是$\subseteq-minimal$。</p><p><span style="color:red">适当前提基</span>：进一步地有{$B\rightarrow B^{\bullet}|B: proper\ premise$}是形式背景$\mathbb{K}$的一个complete and sound 直接基。则该集合称为形式背景$\mathbb{K}$的适当前提基。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" alt></p><h2 id="3-Proper-Premises-as-Minimal-Hypergraph-Transversals"><a href="#3-Proper-Premises-as-Minimal-Hypergraph-Transversals" class="headerlink" title="3 Proper Premises as Minimal Hypergraph Transversals"></a>3 Proper Premises as Minimal Hypergraph Transversals</h2><h3 id="3-1-基本定义"><a href="#3-1-基本定义" class="headerlink" title="3.1 基本定义"></a>3.1 基本定义</h3><p><span style="color:red">minimal hypergraph transversals</span>：用于从关系型数据库中挖掘函数依赖。</p><p>超图以前已经用于关联规则挖掘的相关任务$^{[33]}$。超图如何应用于数据挖掘的概述可以在$^{[20]}$中找到。 </p><p><span style="color:red">hypergraph</span>：$V$是有限的顶点集，$V$上的一个超图$\mathscr{H}$是幂集$2^V$的一个子集。每一个集合$E\in \mathscr{H}$为超图的一条边，与经典图论不同的是，这条边可以关联到两个以上的顶点，也可以关联到两个以下的顶点。</p><p><span style="color:red">hypergraph transversal</span>：一个集合$S\subseteq V$被称为$\mathscr{H}$的 hypergraph transversal，当它与每条边$E\in \mathscr{H}$都相交。 即$\forall E\in \mathscr{H}, S\cap E\neq \varnothing$。</p><p><span style="color:red">minimal hypergraph transversal</span>：集合$S\subseteq V$被称为最小超图横断面，当S在$\mathscr{H}$的所有超图横断面中的子集序下是最小的。</p><p><span style="color:red">transversal hypergraph</span>：$\mathscr{H}$的所有 minimal hypergraph transversal构成的集合。记为$Tr(\mathscr{H})$。</p><p>TRANSHYP：判定超图$\mathscr{H}$是否是超图$\mathscr{G}$的 transversal hypergraph 的问题。</p><p>TRANSENUM：枚举出超图$\mathscr{G}$的所有minal hypergraph transversal 的问题。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" alt></p><p>解释：$P\subseteq M\backslash\{m\}对于m\in M$是一个前提，当且仅当对于所有的$g\in G\ with\ g\swarrow m$成立。$P$对于$m$是一个适当前提，当且仅当在$m$的所有前提中，按子集序$P$是最小的。则有推论</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li><p>implicitly：含蓄地；暗中地；间接的；不言而喻。</p></li><li><p>in contrast to：与…不同。</p></li><li><p>be incident to：关联到…。</p></li><li><p>quasi-polynomial time：准多项式时间。</p></li></ul><h2 id="4-Improvements-to-the-Algorithm"><a href="#4-Improvements-to-the-Algorithm" class="headerlink" title="4 Improvements to the Algorithm"></a>4 Improvements to the Algorithm</h2><h3 id="4-1-Avoiding-Duplicates-using-Candidate-Sets"><a href="#4-1-Avoiding-Duplicates-using-Candidate-Sets" class="headerlink" title="4.1 Avoiding Duplicates using Candidate Sets"></a>4.1 Avoiding Duplicates using Candidate Sets</h3><p>原因：Algorithm1会多次计算适当前提，因为它们可以是多个属性的适当前提。如{c, e}是属性a, b, d 的适当前提，则其会计算3次。</p><p>第一种思想：根据当前属性，引入相关属性的候选集。则只需在候选集集合$C$中搜索$\mathscr{H}_{\mathbb{K}, m}^{\swarrow}$最小超图横断面即可。</p><p>冗余条件：$\mu w\wedge\mu m\le\mu v\lt\mu m$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" alt></p><p>Algorithm2的正确性证明。</p><h3 id="4-2-Irreducible-Attributes"><a href="#4-2-Irreducible-Attributes" class="headerlink" title="4.2 Irreducible Attributes"></a>4.2 Irreducible Attributes</h3><p>从候选集合C中移除属性m，属性m满足条件：$\mu m=\wedge_{i=1}^n\mu x_i\ for\ i=1,…, n$。这样的属性称为$\wedge$可约属性，其集合为$N$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708174500270.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708174500270.png" srcset="/img/loading.gif" alt></p><p>Algorithm3的正确性证明。</p><p>词汇积累</p><ul><li>intuition：直觉的。</li><li>snippet：片段。</li><li>identify：确定；识别。</li></ul><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h2><p>算法：</p><ul><li>SB：基于Next-Closure算法计算主基的实现算法。</li><li>HT：和Algorithm1一样，在所有适当前提中计算超图横断面的算法。</li><li>PP：Algorithm3的实现算法。 </li></ul><p>数据集：</p><ol><li>SPECT [9], which describes Single Proton Emission Computed Tomography (SPECT) images. This data set is given as a dyadic formal context with 187 objects, 23 attributes, and an approximate density of 0.38.</li><li>Congressional V oting Records of the U.S. House of Representatives from 1984 [31]. It contains 435 objects, 16 attributes and is given as many valued context. It has been nominally scaled, resulting in a context with 50 attributes and an approximate density of 0.34.</li><li>The third structured data set originates from the application described in Section 5.2 and [30]. It has 26 objects, 79 attributes and an approximate density of 0.35.<img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191106203.png" srcset="/img/loading.gif" class><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191106203.png" srcset="/img/loading.gif" alt></li></ol><p>实验结果：<img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191311319.png" srcset="/img/loading.gif" class> <img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191311319.png" srcset="/img/loading.gif" alt></p><p>词汇积累</p><ul><li>contranominal：相反的。</li><li>trade-off：权衡。</li><li>explicit：显式化的。</li></ul><h2 id="6-Attribute-Exploration"><a href="#6-Attribute-Exploration" class="headerlink" title="6 Attribute Exploration"></a>6 Attribute Exploration</h2><p>基本概念</p><p><span style="color:red">属性探索</span>：一种交互式的形式化算法，即使原形式背景是不完整的，也能够获得sound和complete的蕴涵基集合。其特点为：</p><ul><li>一个不完备的形式背景。</li><li>一个拥有全部领域知识的专家。</li><li>每次迭代，将蕴涵式向专家询问。<ul><li>接受，则将蕴涵式加入蕴涵基集合。</li><li>拒绝，专家提供一个反例加入当前工作形式背景。</li></ul></li></ul><p><span style="color:red">原始形式背景（initial context）</span>：初始探索的形式背景。</p><p><span style="color:red">背景知识（background knowledge）</span>：初始蕴涵式集合。</p><p><span style="color:red">当前工作形式背景（current working context）</span>：每次探索时的形式背景。</p><p><span style="color:red">已知蕴涵式集合（the set of known implications）</span>：探索过程中专家接受的蕴涵式构成的集合。</p><p><span style="color:red">最终形式背景（final context）</span>：整个探索过程结束后的形式背景。</p><p><span style="color:red">背后的形式背景（background context）</span>$\mathbb{K}_{BG}$：隐式的已知的形式背景。</p><p>可以将属性探索看作是使$\mathbb{K}_{BG}$中知识显式化的过程。</p><p>主要挑战:避免重新计算已经被专家接受的蕴涵式。</p><p>词汇积累</p><ul><li>interactive formalism：交互式的形式主义。</li><li>facilitate：便于，促进，帮助；使容易。</li><li>implicit：隐式的。</li></ul><h3 id="6-1-Incremental-Computation-of-Proper-Premises-Using-Berge-Multiplication"><a href="#6-1-Incremental-Computation-of-Proper-Premises-Using-Berge-Multiplication" class="headerlink" title="6.1 Incremental Computation of Proper Premises Using Berge Multiplication"></a>6.1 Incremental Computation of Proper Premises Using Berge Multiplication</h3><h4 id="6-1-1-Berge-Multiplication-and-its-Offspring"><a href="#6-1-1-Berge-Multiplication-and-its-Offspring" class="headerlink" title="6.1.1 Berge Multiplication and its Offspring"></a>6.1.1 Berge Multiplication and its Offspring</h4><p>定义两个超图$\mathscr{G}$和$\mathscr{H}$以边进行合并为</p><script type="math/tex; mode=display">\mathscr{G}\vee \mathscr{H}:=\{ g\cup h|g\in \mathscr{G},\ h\in \mathscr{H} \}</script><p>集合$S$中$\subseteq-minimal$集（子集序）为</p><script type="math/tex; mode=display">min(S):=\{ X\in S |not\ \exists Y\in S:Y\subsetneq X \}</script><p>则对于两个有限的超图$\mathscr{G}$和$\mathscr{H}$有</p><script type="math/tex; mode=display">Tr(\mathscr{G} \cup \mathscr{H})=\min(Tr(\mathscr{G}) \vee Tr(\mathscr{H}))</script><p>Berge Multiplication算法的相关研究：</p><ul><li>Takata$^{[32]}$给出了一个超图族的例子，使得Berge Multiplication算法的最小运行时间为$n^{\Omega(\log\log n)}$，$n$为相应的输出规模。</li><li>Boros$^{[8]}$证明了Berge Multiplication算法在$n^{\sqrt{n} }$的时间内能够获得超图$\mathscr{H}$的所有最小超图横截面，$n$为相应的输出规模。</li><li>所有的Berge Multiplication的变体算法最坏情况的运行时间至少为$n^{\Omega(\log\log n)}$$^{[21]}$，$n$为相应的输出规模。</li></ul><p>词汇积累</p><ul><li>divide and conquer：分而治之。</li><li>hypergraph transversal algorithms：超图遍历算法。</li><li>open question：悬而未决的问题。</li><li>permutation：排列。</li></ul><h4 id="6-1-2-A-Naive-Exploration-Algorithm-using-Proper-Premises"><a href="#6-1-2-A-Naive-Exploration-Algorithm-using-Proper-Premises" class="headerlink" title="6.1.2 A Naive Exploration Algorithm using Proper Premises"></a>6.1.2 A Naive Exploration Algorithm using Proper Premises</h4><p>第一个形式化的带有前提的属性探索算法，令$\mathbb{K}$为原始形式背景，$\mathscr{L}$为背景知识并且$\mathscr{E}=\mathscr{H}_{\mathbb{K}, m}^{\notin}$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712111542675.png" srcset="/img/loading.gif" class> <p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712111542675.png" srcset="/img/loading.gif" alt></p><p>解释：</p><ol><li>该算法遍历所有的属性$m\in M$，并通过Berge Multiplication不断考虑$\mathscr{E}$中的边来计算属性$m$的适当前提。</li><li>如果$\mathscr{L} \nvDash P \rightarrow P^{“}$，则向专家询问蕴涵式$P \rightarrow P^{“}$。<ol><li>接受，将其$P \rightarrow P^{“}$加入$\mathscr{L}$。</li><li>拒绝，$g$作为$P \rightarrow \{ m \}$的反例。<ol><li>如果$m \notin g’$，则将集合$M \backslash g’$作为$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一条边加入$\mathscr{E}$中。</li></ol></li></ol></li><li>2步骤一直持续到$\mathscr{E}$中不存在边剩余，此时就可得到$\mathbb{K}_{B, G}$中所有属性$m$的适当前提。</li><li>最后考虑$M$中的剩余属性。</li></ol><p>这一过程在算法6中正式给出。请注意，在此算法中，我们仅非正式地将专家交互描述为“expert confirms $Q→Q^{“}$”或“ask expert for valid counterexample ”，这在文献中是常见的。但也有可能更正式地描述这种相互作用，就像在$^{[6]}$中所做的那样。然而，我们不会在这里这样做，因为我们没有必要进行进一步的考虑。</p><p>在算法6终止时的形式背景为$\mathbb{K}$，$\mathscr{L}$是<script type="math/tex">\mathbb{K}_{B, G}</script>的一个基，并且$\mathscr{L}$仅包含形式$P \rightarrow P^{“}$的蕴涵式，其中$P$是$\mathbb{K}_{B, G}$的适当前提。</p><p>证明$\mathbb{K}$的适当前提就是$\mathbb{K}_{B, G}$的适当前提。$\Leftarrow$Lemma 3。</p><p>假设对于某些属性$m$，$P$是$\mathbb{K}_{BG}$中的一个真前提。由Lemma 2 知这等价于$P$是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的极小超图横截面。由于所有的反例均取自于$\mathbb{K}_{BG}$，则有$\mathscr{H}_{\mathbb{K}, m}^{\notin} \subseteq  \mathscr{H}_{BG, m}^{\notin}$,因此$P$也是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一个超图横截面。这证明了在$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的超图横截面中$P$的极小性。</p><p>现在假设$Q \subseteq P$是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一个极小超图，那么$Q$是$\mathbb{K}$的一个适当前提，并且有$\mathscr{L} \vDash Q \rightarrow Q^{“}$。而由Lemma 3 可知，$Q$也是$\mathbb{K}_{BG}$的一个适当前提，是$\mathscr{H}_{BG, m}^{\notin}$的一个极小超图横截面。则$P=Q$。这证明了$P$也是$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的极小超图横截面即$P$是$\mathbb{K}$的一个适当前提。</p><p>$\Rightarrow$ 算法6终止的形式背景$\mathbb{K}$的前提基与$\mathbb{K}_{BG}$的前提基相同。</p><p>词汇积累</p><ul><li>whereupon：因此。</li></ul><h3 id="6-2-Using-the-improvements"><a href="#6-2-Using-the-improvements" class="headerlink" title="6.2 Using the improvements"></a>6.2 Using the improvements</h3><h4 id="6-2-1-Querying-the-hierarchy-of-attribute-concepts"><a href="#6-2-1-Querying-the-hierarchy-of-attribute-concepts" class="headerlink" title="6.2.1 Querying the hierarchy of attribute concepts"></a>6.2.1 Querying the hierarchy of attribute concepts</h4><p><span style="color:red">find single proper premises</span>:</p><p>算法2和算法3都要求在第一步中计算所有的单一适当前提。在属性探索中，形式背景$\mathbb{K}$最初是不完整的。因此，简单地计算$\mathbb{K}$的所有单个适当前提是不够的，因为我们不知道当形式背景$\mathbb{K}$扩张时它们是否仍然是适当前提。</p><p>然而，如果属性$\{ m \}$是$\mathbb{K}$的一个适当前提，并且专家确认蕴涵式$\{ m \} \rightarrow \{ m \}^{“}$成立，则 Lemma 3 确保了当形式背景$\mathbb{K}$扩张时$\{ m \}$仍然是一个前提。</p><p><span style="color:red">find irreducibles</span>:</p><p>在算法3中，需要知道哪些属性概念是交不可约的。不幸的是，在工作形式背景$\mathbb{K}$中满足可约的属性概念$\mu m$在$\mathbb{K}$的扩展中可能不再满足可约。</p><p>我们可以向专家提出蕴涵式$\{ n \in \{ m \}^{“} | \mu m &lt; \mu n \} \rightarrow \{ m \}$来确定是否可以移除可约属性$\{ m \}$。如果专家接受该蕴涵式，则属性$m$在形式背景$\mathbb{K}$扩张时仍然是交可约的。</p><p>算法7显示了这两个初始查询是如何执行的。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712154529970.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712154529970.png" srcset="/img/loading.gif" alt></p><h4 id="6-2-2-Candidate-Sets"><a href="#6-2-2-Candidate-Sets" class="headerlink" title="6.2.2 Candidate Sets"></a>6.2.2 Candidate Sets</h4><p>将对候选集的限制调整到属性探索设置并非易事。</p><p>问题是，一个属性$u$不是$\mathbb{K}_{init}$中的属性m的候选的属性，但$u$仍然可以是$\mathbb{K}_{B,G}$中的候选属性。</p><p>例子：考虑 Table 7 中的形式背景$\mathbb{K}_{B,G}$，在经过算法7的预处理后，我们得到 Table 8 中的形式背景$\mathbb{K}_{init}$以及蕴涵集$\mathscr{L}_{init}=\{ \{v\} \rightarrow \{m\} \}$。假设我们想获得属性$m$的适当前提，则可计算候选集为：$C_{init}=\{ u \in M \backslash \{m\} |not\ \exists v\in M: \mu u \wedge \mu m \le _{init} \mu v \lt _{init} \mu m \}$。</p><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/images/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712225021028.png" srcset="/img/loading.gif" alt></p><p>很容易从 Fig. 7 的概念格中看出，它是空集，因此无法计算出属性$m$的前提$\{ u, w \}$。然而从 Fig. 6 的概念格中可以看出，$\{ u, w \}$确实是属性$m$的前提。</p><p>但是请注意，如果我们先计算属性$v$的适当前提 ，再计算属性$m$的适当前提，则不会出现上述问题。然后向专家询问蕴涵式$\{m,u\} \rightarrow \{v\}$是否成立，使得专家添加$E$作为反例。</p><p>显然，这个问题可以通过固定处理属性的顺序来避免，我们可以使$M$中的&lt;序满足$\mu m \lt _{init} \mu n \Rightarrow m \lt n$。算法8中使用了候选集和这种顺序。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712231347704.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712231347704.png" srcset="/img/loading.gif" alt></p><p>如果属性$m$是这中顺序的第一个属性，则$\mu m$是最小的属性概念，因此属性$m$的候选集为$M \backslash \{m\}$。这意味着在第一次迭代后可以获得属性$m$的所有适当前提。而在算法2中，属性$m$的所有适当前提在属性$m$的迭代或属性$v (\mu v \lt \mu m)$的迭代后获得。因此，由归纳法可知，对于所有的属性$n$，以下条件成立：</p><p>在属性$n$处理后，</p><ul><li>对于属性$n$的每一个适当前提$P$，$\mathscr{L}$包含有蕴涵式$P \rightarrow P^{‘’}$。</li><li>对于在$\mathbb{K}_{B,G}$不成立的每个蕴涵式$S \rightarrow \{n\}$，均向$\mathbb{K}$中添加了相应的反例。</li></ul><p>这保证了在$\mathbb{K}_{B,G}$计算的候选集恰好是$\mathbb{K}$中的候选集。下面的 Lemma 7 可以说明这一点。</p><hr><blockquote><p>Lemma 7 假设我们允许算法8运行到属性$m$的迭代，记$\mathbb{K}_m$为此次迭代中获得的形式背景，$\mathbb{K}_{B,G}$为背后的形式背景，$\le_{m}$和$\le_{B,G}$分别为相应概念格的序。</p></blockquote><p>那么</p><script type="math/tex; mode=display">\begin{align}C_{B,G}=& \{ u\in M \backslash \{m\} | not\ \exists v\in M: \mu u \wedge \mu m \le _{BG}{\mu v} \lt _{BG}{\mu m} \} \newline =& \{ u\in M \backslash \{m\} | not\ \exists v\in M: \mu u \wedge \mu m \le _m{\mu v} \lt _m{\mu m} \} \end{align}</script><p>Proof 从 Lemma 6 可知</p><script type="math/tex; mode=display">\mu v \lt _{BG}\mu m \Leftrightarrow \mu v \lt _{init}\mu m \Leftrightarrow \mu v \lt _m \mu m</script><p>而且，由FCA可知一个事实——$\mu u \wedge \mu m \le _{BG}\mu v\ iff\ \{u,m\}^{‘BG} \subseteq \{v\}^{‘BG}$即有$\{u,m\} \rightarrow \{v\}$在$\mathbb{K}_{BG}$中成立。如果有$\mu v \lt _m{\mu m}$，则可知属性$v$在之前的迭代过程中已处理，因此所有在$\mathbb{K}_m$中成立的蕴涵式$S \rightarrow \{v\}\ iff\ they\ hold\ in\ \mathbb{K}_{BG}$。所以可以得到$all\ v\in M\ with\ \mu v \lt _m{\mu m}$。</p><script type="math/tex; mode=display">\begin{align}\mu u \wedge \mu m \le _{BG}\mu v &\Leftrightarrow \{u,m\} \rightarrow \{v\}\ holds\ in\ \mathbb{K}_{BG} \newline&\Leftrightarrow \{u,m\} \rightarrow \{v\}\ holds\ in\ \mathbb{K}_m\ \Leftrightarrow \mu u \wedge \mu m \le _m{\mu v} \end{align}</script><p>从上式三式（9-11）可得以下结论等价：</p><script type="math/tex; mode=display">\begin{align}u\notin C_m &\Leftrightarrow \exists v\in M: \mu u \wedge \mu m \le _m{\mu v} \lt _m{\mu m} \newline&\Leftrightarrow \exists v\in M: \mu u \wedge \mu m \le _{BG}{\mu v} \lt _{BG}{\mu m} \newline&\Leftrightarrow u\notin C_{BG}\end{align}</script><p>这证明了$C_m = C_{BG}$。</p><hr><p>Lemma 7 说明了：如果我们在 background context 中运行算法2， 获得的候选集与以 subcontext 作为算法8的输入获得的候选集相同。而算法8的正确性可由算法2的正确性直接获得（Lemma 1）。</p><h4 id="6-2-3-Removing-Reducibles"><a href="#6-2-3-Removing-Reducibles" class="headerlink" title="6.2.3 Removing Reducibles"></a>6.2.3 Removing Reducibles</h4><p>从 Lemma 6 中，我们可以知道，一旦$\mathbb{K}_{init}$被计算，则在探索过程中，不可约属性集将不会改变，并且实际上与$\mathbb{K}_{BG}$中的不可约属性集相同。因此，可直接将搜索空间限制为不可约属性集来直接扩展算法8获得算法9，而算法9的正确性是算法3和算法9的直接结果。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200713231141823.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200713231141823.png" srcset="/img/loading.gif" alt></p><p>词汇积累</p><ul><li>induction：归纳法。</li></ul><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h2><p>本文</p><ol><li>证明了，当主基具有最小基数时，适当前提的基通常可以更有效地计算。</li><li>在基于超图的算法中进行非常简单的优化会产生很大的性能提升。</li><li>我们已经在随机的、人工生成的形式背景以及来自实际应用的形式背景上对它进行了评估。我们的评估表明，在这些特定的数据集上，我们的算法比现有算法更快。这表明我们的方法可能也更适用于其他数据集，特别是当这些数据集包含大量内涵时。</li><li>至于最小基数，则还需要额外的最小化步骤。在这种情况下，性能收益可能较小。然而，在像模型重构这样的应用程序中，最小基数只是次要的。</li><li>展示了如何执行基于适当前提的探索。并提供了详细的算法描述，并展示了如何将本文第一部分中的改进应用于属性探索设置。</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>快速算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW2</title>
    <link href="/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/"/>
    <url>/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）"><a href="#李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）" class="headerlink" title="李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）"></a>李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）</h1><p>本文主要参考了博文<a href="https://www.cnblogs.com/HL-space/p/10785225.html" target="_blank" rel="noopener">https://www.cnblogs.com/HL-space/p/10785225.html</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是根据每个ID的各种属性值去判断该ID对应角色是Winner还是Losser（收入是否大于50K）。显然这是一个二分类问题，可以运用Logistic Regression来实现。</p><ul><li>spam_train.csv：形状为4000×59，4000行数据对应4000个角色，ID编号从1到4001，59列属性中，第一列为角色ID，最后一列为分类结果（label为0/1），中间57列为角色对应的57种属性值。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><ol><li>本次任务先对数据做线性回归，得出每个样本的回归值，计算公式为：</li></ol><script type="math/tex; mode=display">y^n=\sum\limits_{i=1}^{57}w_ix_i^n+b</script><p>其中，$x_i^n$为第$n$个样本值，$y^n$为对应的回归结果。</p><ol><li><p>将回归结果送入$sigmod$函数，得到对应的概率值。计算公式为：</p><script type="math/tex; mode=display"> p^n=\frac{1}{1+e^{-y^n} }</script></li><li><p>众所周知，不管线性回归还是Logistic回归，其关键和核心就在于通过误差的反向传播来更新参数，进而使模型不断优化。因此，损失函数的确定及对各参数的求导就成了重中之重。在分类问题中，模型一般针对各类别输出一个概率分布，因此常用交叉熵作为损失函数。交叉熵可用于衡量两个概率分布之间的相似、统一程度，两个概率分布越相似、越统一，则交叉熵越小；反之，两概率分布之间差异越大、越混乱，则交叉熵越大。</p></li></ol><p>下式表示k分类问题的交叉熵，P为label，是一个概率分布，常用one_hot编码。例如针对3分类问题而言，若样本属于第一类，则P为(1,0,0)，若属于第二类，则P为(0,1,0)，若属于第三类，则为(0,0,1)。即所属的类概率值为1，其他类概率值为0。Q为模型得出的概率分布，可以是(0.1,0.8,0.1)等。具体计算公式为：</p><script type="math/tex; mode=display">Loss^n=-\sum\limits_1^kp^n\log Q^n</script><p>在实际应用中，为求导方便，常使用以e为底的对数。则上式可化为：</p><script type="math/tex; mode=display">Loss^n=-\sum\limits_1^kp^n\ln Q^n</script><p>针对本次作业而言，虽然模型只输出了一个概率值p，但由于处理的是二分类问题，因此可以很快求出另一概率值为1-p，即可视为模型输出的概率分布为Q(p，1-p)。将本次的label视为概率分布P(y,1-y)，即Winner(label为1)的概率分布为(1,0)，分类为Losser(label为0)的概率分布为(0,1)。则其==损失函数==为：</p><script type="math/tex; mode=display">Loss^n=-[\hat{y}^n\ln p^n+(1-\hat{y}^n)\ln(1-p^n)]</script><p>Loss对权重$w$求偏导，有：</p><script type="math/tex; mode=display">\begin{align}因为&\frac{\partial\ln p^n}{\partial p^n}=\frac{1}{p^n}=1+e^{-y^n} \newline同理&\frac{\partial\ln (1-p^n)}{\partial p^n}=\frac{-1}{1-p^n}=\frac{-e^{-y^n} }{1+e^{-y^n} } \newline而且&\frac{\partial p^n}{\partial y^n}=-\frac{1}{(1+e^{-y^n})^2}(e^{-y^n})(-1)=\frac{-e^{-y^n} }{(1+e^{-y^n})^2} \newline和&\frac{\partial y^n}{\partial w_i}=x_i \newline\newline\end{align}</script><script type="math/tex; mode=display">\begin{align}所以有\frac{\partial Loss^n}{\partial w_i}&=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}]  \newline&=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i]  \newline&=-x_i(\hat{y}^n-p^n)  \end{align}</script><p>同理Loss对偏置$b$求偏导，有：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial Loss^n}{\partial b}&=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}]  \newline&=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})]  \newline&=-(\hat{y}^n-p^n)  \end{align}</script><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>首先将所有空值以0填充，然后对数据进行标准化处理。</p><p>上述操作完成后，将表格的第2列至58列取出为x(shape为4000×57)，将最后一列取出做label y(shape为4000×1)。</p><p>进一步划分训练集和验证集，分别取x、y中前3000个样本为训练集x_train(shape为3000×57)，y_train(shape为3000×1)，后1000个样本为验证集x_val(shape为1000×57)，y_val(shape为1000×1)。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"spam_train.csv"</span>)<span class="hljs-comment"># 空值填0</span>data = data.fillna(<span class="hljs-number">0</span>)<span class="hljs-comment"># 转换为array类型</span>data = np.array(data)<span class="hljs-comment"># 取样本x(4000*57)</span>x = data[:, <span class="hljs-number">1</span>:<span class="hljs-number">-1</span>]<span class="hljs-comment"># 标准化</span>x[<span class="hljs-number">-1</span>] /= np.mean(x[<span class="hljs-number">-1</span>])x[<span class="hljs-number">-2</span>] /= np.mean(x[<span class="hljs-number">-2</span>])<span class="hljs-comment"># 取目标值y</span>y = data[:, <span class="hljs-number">-1</span>]<span class="hljs-comment"># 划分训练集与验证集</span>x_train, x_val = x[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>, :], x[<span class="hljs-number">3000</span>:, :]y_train, y_val = y[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y[<span class="hljs-number">3000</span>:]</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练模型，更新参数。</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(x_train, y_train, epoch)</span>:</span>    num = x_train.shape[<span class="hljs-number">0</span>]    dim = x_train.shape[<span class="hljs-number">1</span>]    bias = <span class="hljs-number">0</span>    weights = np.ones(dim)    learning_rate = <span class="hljs-number">1</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 偏置梯度平方和</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 权重梯度平方和</span>    weight_sum = np.zeros(dim)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        b_g = <span class="hljs-number">0</span>        w_g = np.zeros(dim)                <span class="hljs-comment"># 在所有数据上计算梯度，梯度计算时针对损失函数求导</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):            y_pre = weights.dot(x_train[j, :]) + bias            sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))            b_g += (<span class="hljs-number">-1</span>) * (y_train[j] - sig)            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(dim):                w_g[k] += (<span class="hljs-number">-1</span>) * (y_train[j] - sig) * x_train[j, k] + <span class="hljs-number">2</span> * reg_rate * weights[k]        b_g /= num        w_g /= num        <span class="hljs-comment"># adagrad</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>         <span class="hljs-comment"># 更新权重和偏置</span>        bias -= learning_rate / bias_sum ** <span class="hljs-number">0.5</span> * b_g        weights -= learning_rate / weight_sum ** <span class="hljs-number">0.5</span> * w_g        <span class="hljs-comment"># 每训练100轮，输出一次在训练集上的正确率。</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            acc = <span class="hljs-number">0</span>            result = np.zeros(num)            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):                y_pre = weights.dot(x_train[j, :]) + bias                sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))                <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:                    result[j] = <span class="hljs-number">1</span>                <span class="hljs-keyword">else</span>:                    result[j] = <span class="hljs-number">0</span>                <span class="hljs-keyword">if</span> result[j] == y_train[j]:                    acc += <span class="hljs-number">1.0</span>                loss += (<span class="hljs-number">-1</span>) * (y_train * np.log(sig) + (<span class="hljs-number">1</span> - y_train[j]) * np.log(<span class="hljs-number">1</span> - sig))            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为'</span>.format(i), loss / num)            print(<span class="hljs-string">'&#123;&#125; 次训练后, 训练集的精度为&#123;:.2f&#125;'</span>.format(i, acc / num))    <span class="hljs-keyword">return</span> weights, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 验证模型效果</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate</span><span class="hljs-params">(x_val, y_val, weights, bias)</span>:</span>    num = <span class="hljs-number">1000</span>    loss = <span class="hljs-number">0</span>    acc = <span class="hljs-number">0</span>    result = np.zeros(num)    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):        y_pre = weights.dot(x_val[j, :]) + bias        sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))        <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:            result[j] = <span class="hljs-number">1</span>        <span class="hljs-keyword">else</span>:            result[j] = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> result[j] == y_val[j]:            acc += <span class="hljs-number">1.0</span>        loss += (<span class="hljs-number">-1</span>) * (y_val * np.log(sig) + (<span class="hljs-number">1</span> - y_val[j]) * np.log(<span class="hljs-number">1</span> - sig))    <span class="hljs-keyword">return</span> loss / num, acc / num</code></pre><h3 id="3-4-测试模型"><a href="#3-4-测试模型" class="headerlink" title="3.4 测试模型"></a>3.4 测试模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练轮数</span>epoch = <span class="hljs-number">4000</span><span class="hljs-comment"># 开始训练</span>w, b = train(x_train, y_train, epoch)<span class="hljs-comment"># 在验证集上看效果</span>loss, acc = validate(x_val, y_val, w, b)print(<span class="hljs-string">'验证集的损失值为:'</span>, loss)print(<span class="hljs-string">'验证集的精度值为:'</span>, acc)</code></pre><h3 id="3-5-实验结果"><a href="#3-5-实验结果" class="headerlink" title="3.5 实验结果"></a>3.5 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为 [<span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span>  <span class="hljs-number">1.50623089</span> ... <span class="hljs-number">1.50623089</span> <span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span> ]<span class="hljs-number">0</span> 次训练后, 训练集的精度为<span class="hljs-number">0.62</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.90</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">2000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.86</span> <span class="hljs-number">2400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span> <span class="hljs-number">2800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span>验证集的损失值为: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]验证集的精度值为: <span class="hljs-number">0.864</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率基本能达到90%，验证集分类的正确率能达到86.4%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW1</title>
    <link href="/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/"/>
    <url>/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）"><a href="#李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）" class="headerlink" title="李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）"></a>李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）</h1><p>本文主要参考了博文<a href="https://blog.csdn.net/m123_45n/article/details/106560274" target="_blank" rel="noopener">https://blog.csdn.net/m123_45n/article/details/106560274</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是利用Liner Regression去预测丰原市PM2.5的数值。对丰原市一年的观测记录分为了train set和test set，其中train set 是丰原市每个月前20天的所有记录。test set则是从丰原市剩下的记录取样出来。train set和test set都是以csv格式的文件进行保存的。</p><ul><li>train.csv：每个月前20天的完整数据。</li><li>test.csv：从剩下的数据中取样出连续的10小时为一组，前九个小时的所有观测数据当作==特征值==，第十个小时的PM2.5当作==目标值==。共取出240组不重复的test data，然后根据feature去预测出第十小时的PM2.5的值。</li><li>Data（中含有18项观测数据）：AMB_TEMP，CH4，CO，NHMC，NO，NO2，NOx，O3，PM10，PM2.5，RAINFALL，RH，SO2，THC，WD_HR，WIND_DIREC，WIND_SPEED，WS_HR。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><p>本次任务选择的模型为线性回归模型：</p><script type="math/tex; mode=display">y=\sum\limits_{i=0}^8w_ix_i+b。</script><ul><li><p>$i$从0到8是因为选取前9个小时作为==特征值==输入，每个输入都有一个==权重值==$w$与之相乘，再加上一个偏置$b$，即为线性回归模型。通过这个模型去预测第十个小时的PM2.5的值。</p></li><li><p>此外，可将该模型的运算转换为向量运算：</p><script type="math/tex; mode=display">  \begin{align}  y=  \begin{bmatrix}  w_0\ \dots\ w_8   \end{bmatrix}  \begin{bmatrix}  x_0\newline \vdots\newline x_8   \end{bmatrix}  +b\end{align}</script></li><li><p>选择的<strong>Loss Function</strong>为：</p></li></ul><script type="math/tex; mode=display">Loss=\frac{1}{2m}\sum\limits_{i=0}^{m-1}(\hat{y}^i-y^i)^2+\frac12\lambda\sum\limits_{j=0}^8w_j^2</script><ul><li><p>Loss对$w_j$求导为：</p><script type="math/tex; mode=display">  \frac{\partial L}{\partial w_j}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-x_j)+\lambda w_j</script></li><li></li></ul><script type="math/tex; mode=display">\frac{\partial L}{\partial b}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-1)</script><ul><li><p>则参数更新为：</p><script type="math/tex; mode=display">  \begin{align}  w_j:=w_j-\eta \frac{\partial L}{\partial w_j} \newline  b:=b-\eta \frac{\partial L}{\partial b}  \end{align}</script><p>  其中将$w_j$的更新转换为向量运算：</p><script type="math/tex; mode=display">  \begin{bmatrix}  w_0 \newline \vdots \newline w_8   \end{bmatrix}  :=  \begin{bmatrix}  w_0 \newline \vdots \newline w_8   \end{bmatrix}  -\eta  \begin{bmatrix}  \frac{\partial L}{\partial w_0} \newline \vdots \newline \frac{\partial L}{\partial w_8}   \end{bmatrix}</script></li><li><p>Optimizer的选择：Adagrad。更新方式为：</p></li></ul><script type="math/tex; mode=display">\begin{align}w^1=&w^0-\frac{\eta^0}{\sigma^0}g^0 \newline \vdots \newlinew^{t+1}=&w^t-\frac{\eta^t}{\sigma^t}g^t\end{align}</script><p>​                其中$g^t$为$w^t$的梯度值，而且有：</p><script type="math/tex; mode=display">\begin{align}\sigma^t=&\sqrt{\frac{1}{t+1}\sum\limits_{i=0}^t(g^i)^2} \newline\eta^t=&\frac{\eta}{t+1}\end{align}</script><p>​                带入后，则有：</p><script type="math/tex; mode=display">w^{t+1}=w^t-\frac{\eta}{\sqrt{\sum\limits_{i=0}^t(g^i)^2} }</script><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率能达到90%，验证集分类的正确率能达到87.2%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, header=<span class="hljs-number">0</span>)<span class="hljs-comment"># 删除无关特征</span>data.drop([<span class="hljs-string">"日期"</span>,<span class="hljs-string">"測站"</span>,<span class="hljs-string">"測項"</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataProcess</span><span class="hljs-params">(data)</span>:</span>    <span class="hljs-string">''' 数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数：</span><span class="hljs-string">        data: 原始训练集数据</span><span class="hljs-string">         </span><span class="hljs-string">    返回：</span><span class="hljs-string">        x: 一天中PM2.5前9项特征构成的训练集。</span><span class="hljs-string">        y: 一天中PM2.5第10项特征构成的目标集。</span><span class="hljs-string">        data: 将原始数据集的每个数据值均转换为float类型后的数据集。</span><span class="hljs-string">    '''</span>    x_list, y_list = [], []    data = data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    <span class="hljs-comment"># 将所有数据转换成float类型</span>    data = np.array(data).astype(float)    <span class="hljs-comment"># 将数据集拆分为多个数据帧</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>): <span class="hljs-comment"># 18 * 240 = 4320</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">15</span>): <span class="hljs-comment"># 24-9=15(组)</span>            <span class="hljs-comment"># 截取PM2.5前9项作为训练数据</span>            sample = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j:j+<span class="hljs-number">9</span>]            <span class="hljs-comment"># 截取PM2.5第10项作为目标数据</span>            label = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j+<span class="hljs-number">9</span>]            x_list.append(sample)            y_list.append(label)    x = np.array(x_list)    y = np.array(y_list)    <span class="hljs-keyword">return</span> x, y, data</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainModel</span><span class="hljs-params">(x_data, y_data, epoch)</span>:</span>    <span class="hljs-string">''' 训练模型，从训练集中拿出3000个数据用来训练，剩余600个数据用于验证。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_data: </span><span class="hljs-string">        y_data:</span><span class="hljs-string">        epoch:</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        weight: 权重</span><span class="hljs-string">        bias：偏置</span><span class="hljs-string">        </span><span class="hljs-string">    '''</span>    <span class="hljs-comment"># 初始化偏置</span>    bias = <span class="hljs-number">0</span>    <span class="hljs-comment"># 初始化权重,生成一个9列的行向量，并全部初始化为1。</span>    weight = np.ones(<span class="hljs-number">9</span>)    <span class="hljs-comment"># 初始化学习率为1。</span>    learning_rate = <span class="hljs-number">1</span>    <span class="hljs-comment"># 初始化正则项系数为0.001。</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 用于存放偏置的梯度平方和。</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 用于存放权重的梯度平方和。</span>    weight_sum = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        <span class="hljs-comment"># 偏置梯度平均值。</span>        b_g = <span class="hljs-number">0</span>        <span class="hljs-comment"># 权重梯度平均值</span>        w_g = np.zeros(<span class="hljs-number">9</span>)        <span class="hljs-comment"># 在所有数据上计算w和b的梯度。</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):            b_g += (y_data[j] - weight.dot(x_data[j]) - bias) * (<span class="hljs-number">-1</span>) <span class="hljs-comment"># 如果2个一维向量dot，则结果为它们的内积。</span>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                w_g[k] += (y_data[j] - weight.dot(x_data[j]) - bias) * (-x_data[j, k])        <span class="hljs-comment"># 求平均值</span>        b_g /= <span class="hljs-number">3000</span>        w_g /= <span class="hljs-number">3000</span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):            w_g[k] += reg_rate * weight[k]                <span class="hljs-comment"># adagrad优化</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>        bias -= learning_rate / (bias_sum ** <span class="hljs-number">0.5</span>) * b_g        weight -= learning_rate / (weight_sum ** <span class="hljs-number">0.5</span>) * w_g        <span class="hljs-comment"># 每训练200次输出一次误差</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):                loss += (y_data[j] - weight.dot(x_data[j]) - bias) ** <span class="hljs-number">2</span>            loss /= <span class="hljs-number">3000</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                loss += reg_rate * (weight[j] ** <span class="hljs-number">2</span>)            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为&#123;:.2f&#125;'</span>.format(i, loss / <span class="hljs-number">2</span>))        <span class="hljs-keyword">return</span> weight, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validateModel</span><span class="hljs-params">(x_val, y_val, weight, bias)</span>:</span>    <span class="hljs-string">''' 验证模型，返回损失值。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_val: </span><span class="hljs-string">        y_val: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        loss: 平均损失值。</span><span class="hljs-string">    '''</span>    loss = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">600</span>):        loss += (y_val[i] - weight.dot(x_val[i]) - bias) ** <span class="hljs-number">2</span>        <span class="hljs-keyword">return</span> loss / <span class="hljs-number">600</span></code></pre><h3 id="3-4-测试数据预处理"><a href="#3-4-测试数据预处理" class="headerlink" title="3.4 测试数据预处理"></a>3.4 测试数据预处理</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testDataProcess</span><span class="hljs-params">(test_data)</span>:</span>    <span class="hljs-string">''' 测试数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        test_data: 测试数据集。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        testList: 测试数据集。</span><span class="hljs-string">    '''</span>    testList = []    test_data = test_data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    test_data = np.array(test_data).astype(float)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>):        testList.append(test_data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>])    <span class="hljs-keyword">return</span> np.array(testList)</code></pre><h3 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 训练 '''</span>    <span class="hljs-comment"># 读取训练数据，并将其转换为列表形式。</span>    data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, usecols=np.arange(<span class="hljs-number">3</span>, <span class="hljs-number">27</span>).tolist())    x_data, y_data, data = dataProcess(data)        x_train, y_train = x_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>]    x_val, y_val = x_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>], y_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>]    weight, bias = trainModel(x_train, y_train, <span class="hljs-number">2000</span>)    savePre(weight, bias)    print(<span class="hljs-string">"训练得到的模型的weight为&#123;&#125;"</span>.format(weight))    print(<span class="hljs-string">"训练得到的模型的bias为&#123;&#125;"</span>.format(bias))    loss = validateModel(x_val, y_val, weight, bias)    print(<span class="hljs-string">"模型模型在验证集的loss为&#123;:.2f&#125;"</span>.format(loss))</code></pre><h3 id="3-6-测试模型"><a href="#3-6-测试模型" class="headerlink" title="3.6 测试模型"></a>3.6 测试模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testModel</span><span class="hljs-params">(x_test, weight, bias)</span>:</span>    <span class="hljs-string">''' 用测试数据集测试模型，并将结果保存到output.csv中</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        x_test: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    '''</span>    f = open(<span class="hljs-string">"output.csv"</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>, newline=<span class="hljs-string">""</span>)    csv_write = csv.writer(f)    csv_write.writerow([<span class="hljs-string">"id"</span>, <span class="hljs-string">"value"</span>])    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x_test)):        output = weight.dot(x_test[i]) + bias        csv_write.writerow([<span class="hljs-string">"id_"</span> + str(i), str(output)])    f.close()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 测试 '''</span>    <span class="hljs-comment"># 读取测试数据集</span>    pre = pd.read_csv(<span class="hljs-string">"pre.csv"</span>)    preList = list(pre.replace(<span class="hljs-string">","</span>, <span class="hljs-string">" "</span>))    weight = []    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list(preList[<span class="hljs-number">0</span>].split(<span class="hljs-string">","</span>)):        weight.append(float(i))    bias = float(preList[<span class="hljs-number">1</span>])    test_data = pd.read_csv(<span class="hljs-string">"test.csv"</span>,header=<span class="hljs-literal">None</span>,usecols=np.arange(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>).tolist())    x_test = testDataProcess(test_data)    print(x_test.shape)    print(x_test)    testModel(x_test, np.array(weight), bias)</code></pre><h3 id="3-7-实验结果"><a href="#3-7-实验结果" class="headerlink" title="3.7 实验结果"></a>3.7 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为<span class="hljs-number">477.36</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为<span class="hljs-number">25.08</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为<span class="hljs-number">23.30</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.67</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.37</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为<span class="hljs-number">22.22</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为<span class="hljs-number">22.14</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为<span class="hljs-number">22.10</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.08</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.07</span>训练得到的模型的weight为[ <span class="hljs-number">0.00194352</span> <span class="hljs-number">-0.02562139</span>  <span class="hljs-number">0.18138721</span> <span class="hljs-number">-0.19572767</span> <span class="hljs-number">-0.0230436</span>   <span class="hljs-number">0.40959797</span> <span class="hljs-number">-0.51985702</span>  <span class="hljs-number">0.06835844</span>  <span class="hljs-number">1.03413381</span>]训练得到的模型的bias为<span class="hljs-number">1.948721373892966</span>模型模型在验证集的loss为<span class="hljs-number">39.17</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">在这个模型当中，可以看到在经过2000次训练后，模型在验证集上的损失已达到了39.17。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后600项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2015-CLA-NextCloures Parallel Computation of the Canonical Base</title>
    <link href="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/"/>
    <url>/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/</url>
    
    <content type="html"><![CDATA[<h1 id="NextClosures-Parallel-Computation-of-the-Canonical-Base"><a href="#NextClosures-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="NextClosures: Parallel Computation of the Canonical Base"></a>NextClosures: Parallel Computation of the Canonical Base</h1><div class="note note-primary">            <p>属性探索、并行算法、NextClosures</p>          </div> <a id="more"></a><!-- <p class='note note-info'>论文、属性探索</p> --><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel 、Daniel Borchmann。</p><p>​            2. 期刊：International Journal of General Systems。</p><p>​            3. 会议：CLA。</p><p>​            4. 时间：2015。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>形式背景的规范基在形式概念分析中起着举足轻重的作用。</li><li>目前所提的计算主基的算法都是线性序的，即一次只计算一个伪内涵。</li><li>本文介绍一种允许以并行方式计算主基的方法。</li><li>实验表明，对于足够大的数据集，加速比与CPU的数量成正比。</li></ul><p>词汇积累：</p><ul><li>canonical base: 主基，正则基，规范基。</li><li>remedies: 救济方法，弥补手段。</li><li>deficit: 缺陷。</li><li>is proportional to: 与之成正比。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>有两种已知的线性序的算法可以计算形式背景的主基$^{[6, 12]}$，即一次只计算一个蕴涵式。这两种算法还计算了给定形式背景的所有形式概念。</p></li><li><p>目前为止，还不知道能否在多项式时间内计算主基。</p></li><li><p>实际上，对于以字典序来计算伪内涵的算法，都表明了它无法在多项式时间内计算出主基。</p></li><li><p>已有并行式计算给定形式背景的概念格的方法$^{[5, 13]}$。</p></li><li><p>本文想探索出一种并行式计算主基的算法，该思想已经被Lindg[11]用来顺序计算给定形式背景的概念格。</p><p> 思想：为了计算主基，则需要计算出形式背景的所有内涵和伪内涵的概念格。这个格需要按级别顺序自下而上的计算，并且基于该格特定层上有一定的“宽度”在该计算可以并行完成。</p><p> 现在有一个事实：对于计算内涵/伪内涵$B$的上邻域，则只需要迭代所有$m\notin B$的属性并计算出$B\cup \{m\}$的闭包。</p><p> 而在lindg中计算该闭包的方法为：$B\rightarrow B^{II}$。</p><p> 而且实验结果表明，对于合适的大的形式背景，主基的计算速度与CPU的数量成正比。</p></li></ol><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>1.A model of $X\rightarrow Y$ is a set $T\subseteq M\Leftrightarrow X\subseteq T\Rightarrow Y\subseteq T$.</p><p>2.A model of $L$ is a model of all implications in $L$, and $X^L$ is the superset of $X$ that is a model of $L$.</p><p>3.$X^L$的计算方式如下：</p><script type="math/tex; mode=display">\begin{align}X^L:=\bigcup_{n\geq 1}X^{L_n}\ where\ X^{L_1}:&=X\cup \bigcup \{B\mid A\rightarrow B\in L\ and\ A\subseteq X\} \newlineand\ X^{L_{n+1} }:&=(X^{L_n})^{L_1} for\ all\ n\in N\end{align}</script><p>当且仅当$X，X\subseteq M$是$\mathbb{K}^<em>$闭包时，很容易判定$X$是形式背景$K$的内涵/伪内涵。$\mathbb{K}^</em>$闭包定义如下：</p><script type="math/tex; mode=display">\begin{align}X^{\mathbb{K}^*}:=\bigcup_{n\geq 1}X^{\mathbb{K}_n^*}\ where\ X^{\mathbb{K}_1^*}:&=X\cup\bigcup \{P^{II}\mid P\in PsInt(\mathbb{K})\ and\ P\subsetneq X\} \newlineand\ X^{\mathbb{K}_{n+1}^*}:&=(X^{\mathbb{K}_n^*})^{\mathbb{K}_1^*} for\ all\ n\in N\end{align}</script><p>如果$L$是形式背景$\mathbb{K}$的主基，则$L^<em>$与$\mathbb{K}^</em>$重合。$L^*$闭包定义如下：</p><script type="math/tex; mode=display">\begin{align}X^{L^*}:=\bigcup_{n\geq 1}X^{L_n^*}\ where\ X^{L_1^*}:&=X\cup \bigcup \{B\mid A\rightarrow B\in L\ and\ A\subsetneq X\} \newlineand\ X^{L_{n+1}^*}:&=(X^{L_n^*})^{L_1^*} for\ all\ n\in N\end{align}</script><p>4.内涵：$B=B^{II}$。</p><p>伪内涵：$P\neq P^{II}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{II}\subseteq P$。将形式背景$K$上的所有伪内涵集合记为$PsInt(K)$。</p><p>蕴涵式：$\{P\rightarrow P^{II}\mid P\in PsInt(K)\}$。</p><h2 id="3-Parallel-Computation-of-the-Canonical-Base"><a href="#3-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="3 Parallel Computation of the Canonical Base"></a>3 Parallel Computation of the Canonical Base</h2><p>1.$NextCloures$算法由Ganter$^{[6]}$提出，用于枚举主基。</p><ul><li>算法思想：以一定的线性顺序(即字典序)计算形式背景K的所有内涵和伪内涵。</li><li>优势：下一个(伪)意图是唯一确定的，但我们可能需要回溯才能找到它。</li><li>该算法本质上是顺序的，即不可能将其并行化。</li></ul><p>2.我们的方法按照子集序枚举所有的内涵和伪内涵。</p><ul><li>易于并行化枚举。</li><li>多线程实现中，不同线程之间不需要通信。</li></ul><p>原因：检测$P$是否为伪内涵时，只需判断$P$的伪内涵子集$Q\subsetneq P$，是否满足满足$Q^{II}\subseteq P$。即可按基数递增来判断伪内涵。</p><p>算法思想的基本工作流程：</p><p>​            1. 判断$\varnothing=\varnothing^{II}\Rightarrow \varnothing$是内涵/伪内涵。</p><ol><li>假设基数$&lt;k$的所有伪内涵均已确定，然后就可正确判断出属性集$P，P\subseteq M，|P|=k$是内涵/伪内涵。</li><li>设定一个候选集合储存当前层次的内涵/伪内涵，而无论何时才找到伪内涵$P$，它的$\subseteq-next\ cloure$都是由$P^{II}$唯一确定的。</li><li>如果$B$是一个内涵则它的$\subseteq-next\ cloure$为$(B\cup \{m\})^{\mathbb{K}^*},\ m\notin B$。</li></ol><p>具体的工作流程：</p><p>基本定义：</p><ul><li>$K$：有限的形式背景。</li><li>$k$：当前候选集的基数。候选集为储存当前层级的内涵/伪内涵 。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </li><li>$C$：候选集合。</li><li>$\mathfrak{B}$：形式概念集合。</li><li>$L$：蕴涵式集合。</li></ul><p>具体流程：</p><p>1.$k:=0，C:=\{\varnothing\}，\mathfrak{B}:=\varnothing，L:=\varnothing$.</p><p>2.并行化：对于每个基数$|C|=k$的候选集合C $\in C$，确定它是否是$L^<em>-closed$。如果不是，则将它的$L^</em>-closure$加入候选集合$C$，跳到5。如果是$L^*-closed$，则跳到3。</p><p>3.如果C是形式背景$K$的内涵，那么将形式概念$(C^I，C)$加入集合$\mathfrak{B}$中。否则C必为伪内涵，则将蕴涵式$C\rightarrow C^{II}$加入集合$L$，并且将形式概念$(C^I，C^{II})$                                                                                                                                                                                                                                                     加入集合$\mathfrak{B}$中。</p><p>4.对得到的每一个内涵$C^{II}$，将它的上层邻居$C^{II}\bigcup \{m\}且m\notin C^{II}$加入候选集合$C$中。</p><p>5.等待基数为$k$的所有候选集合都处理完毕，如果$k&lt;M$则递增$k$，并且跳到2；否则算法结束，返回形式概念集合$\mathfrak{B}$和蕴涵式集合$L$。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335.jpg" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335-1593786388801.jpg" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li>backtracking：回溯。</li><li>level-wise：逐级。</li><li>coincide：一致。</li><li>inductive：归纳的；感应的，诱导的。</li><li>suffices：满足。</li></ul><h2 id="4-Benchmarks"><a href="#4-Benchmarks" class="headerlink" title="4 Benchmarks"></a>4 Benchmarks</h2><p style="text-indent:2em">本节主要目的在于将我们并行式的算法与线性序的NextCloures算法在计算主基时进行定性与定量的比较分析。 </p><p style="text-indent:2em">结果：并行式算法在一定的极限下，算法的运行时间与可用CPU的数量成正比的减少。 </p><p>​        特点：同一层的候选集不能彼此影响，即线程之间不需要通信。</p><p>​        异常情况：在某些情况下，当使用所有可用的CPU时，计算时间会增加。（原因未知）—-可能是由于平台或操作系统的一些技术细节，比如基准测试过程中执行的一些后台任务，或者线程维护带来的开销。</p><p>   数据集：FCA Data Repository$^{[4]}$。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003519115.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003519115-1595918426558.png" srcset="/img/loading.gif" alt></p><p>基准测试结果：计算主基所用的时间基本与CPU数量成线性负相关。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003803863.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003803863-1595918434190.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003944602.png" srcset="/img/loading.gif" class><p>仅一个CPU的时候，本文方法与原方法所用时间基本相同，但当增加CPU的数量时，本文方法明显优于原方法，大致减少了$\frac13$到3比例的时间。</p><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/image-20200723003944602-1595918438926.png" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li>benchmarks：基准测试。</li><li>overhead caused：导致的开销。</li></ul><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p style="text-indent:2em">本文介绍了计算正则基的并行式算法NextCloures。该算法以层次化的顺序（即增加基数），自下而上的构造给定形式背景的所有内涵和伪内涵的概念格。</p><p style="text-indent:2em">由于概念格中某一层的元素可以独立计算，也可以并行枚举，从而产生了计算正则基的并行算法。</p><p>可能的扩展：</p><ul><li>处理一组蕴涵式或约束闭包算子给出的具有背景知识的形式背景。</li><li>属性探索：包含专家交互，以探索部分已知形式背景的规范基。<ul><li>可以让几位专家同时回答问题。</li><li>由于前提基数的增加即问题的难度与经典属性探索线性序提出的问题相比不断增加。</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>并行算法</tag>
      
      <tag>NextCloures</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学-并行计算学科发展历程</title>
    <link href="/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/"/>
    <url>/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="并行计算学科发展历程"><a href="#并行计算学科发展历程" class="headerlink" title="并行计算学科发展历程"></a>并行计算学科发展历程</h1><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：陈国良、张玉杰</p><p>​            2.期刊：计算机科学</p><p>​            3.时间：2020/06/11</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>回顾在并行计算学科发展所做的工作。</li><li>对非数值计算的计算方法进行介绍。</li><li>新型非冯诺依曼结构计算机体系结构的介绍。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>并行计算课程发展的5个阶段：</p><p>非数值计算的并行算法$\rightarrow$新型非冯诺依曼计算机结构$\rightarrow$改革计算机基础课程的计算思维$\rightarrow$数据科学$\rightarrow$大数据计算理论研究。</p><p>完整的并行算法学科体系：算法理论-算法设计-算法实现-算法应用。</p><p>一体化的并行计算研究方法：并行机结构-并行算法-并行编程。</p><h2 id="2-非数值计算中的计算方法"><a href="#2-非数值计算中的计算方法" class="headerlink" title="2 非数值计算中的计算方法"></a>2 非数值计算中的计算方法</h2><p>从计算科学角度数值计算内容主要有：</p><ul><li>矩阵运算。</li><li>线性方程组的求解。</li><li>快速傅里叶变换等。</li></ul><p>非数值计算中的并行算法基本设计策略包括：</p><ul><li>串行算法的直接并行化。</li><li>从问题描述开始设计全新的并行算法。</li><li>借用已有的算法。</li><li>利用已求解问题与待求解问题两者之间的内在相似性来求解新问题。</li></ul><h2 id="3-新型非冯诺依曼计算机体系结构"><a href="#3-新型非冯诺依曼计算机体系结构" class="headerlink" title="3 新型非冯诺依曼计算机体系结构"></a>3 新型非冯诺依曼计算机体系结构</h2><p style="text-indent:2em">传统的冯诺依曼体系结构：第一代计算机（电子管计算机）、第二代计算机（晶体管计算机）、第三代计算机（集成电路计算机）、第四代计算机（大规模超大规模集成电路）</p><p>一些先进新型计算机系统结构：</p><ul><li>微程序控制器设计：<ul><li>设计了“八位运控模型”，并采用了自行提出的“寄存器传输操作语言”进行形式化描述。</li></ul></li><li>直接执行高级语言的计算机：<ul><li>研究了“直接执行的高级语言FORTRAN”机      器，介绍了对标准FORTRAN语言所作的一些限制和补充，简述了该计算机体系结构，列举典型的FORTEAN语言的直接执行过程，并自行提出了可重组结构与之配合。</li></ul></li><li>数据库计算机：<ul><li>研究了数据库计算机，实现RDF查询语言和SQL语言的转换并在此基础上实现一个对用户透明的、建立在关系数据库之上的RDF搜索引擎，以提高其海量存储和查找效率。</li></ul></li><li>光计算机：<ul><li>通过垂直偏振光、水平偏振光和无强光3个稳定的光状态表示信息的三值光计算机原理$^{[7]}$，提出基于光原理三值逻辑计算机。</li></ul></li><li>生物计算机：<ul><li>研究了基于字符串匹配原理的生物序列比对的生物计算机，在纳米计算模型上实现了DNA序列模体发现算法$^{[8]}$。</li></ul></li><li>可重构可变计算机：<ul><li>研究了“可变结构计算机系统”及其结构中的资源间相互通信问题$^{[9]}$。</li></ul></li><li>数据流计算机：<ul><li>根据MIT提出的数据流计算机概念，分析了曼彻斯特大学的数据流计算机，在国内分布式计算计算会议上发表了“数据流计算机体系结构解析”$^{[10]}$一文。</li></ul></li><li>神经计算机：<ul><li>研究神经网络在组合优化中的应用的同时，自行构建了基于Transputer阵列的“通用并行神经网络模拟系统（GP2N2S2）”$^{[10]}$，提供了高级神经网络描述语言及其编辑和编译器的执行环境，实现了程序的自动化执行。</li></ul></li><li>Transpute阵列机：<ul><li>搭建了Transputer阵列机，在中科院计算所进行了组装、调试和运行，并在其上实现了通用的Rohoman应用平台。</li></ul></li><li>量子计算机：<ul><li>在中国科大开展了量子计算研究，讨论量子计算机模型及其物理实现方案、量子计算过程、量子计算模型和量子并行算法，分析量子指数级存储容量和指数加速特征等，并在保密通信、密码安全等领域对量子信息技术进行研究$^{[12, 13]}$。</li></ul></li></ul><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p style="text-indent:2em">大数据、物联网、云计算和区块链是新一代信息技术发展中的华彩乐章。物联网使成千上万的网络传感器嵌入到现实世界中，云计算为物联网产生的海量数据提供了存储空间和在线处理模式，而大数据则让海量数据产生了价值，区块链促进海量信息可靠交互保障生产要素在区域内有序高效地流通。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2018-CS-A General Form of Attribute Exploration </title>
    <link href="/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/"/>
    <url>/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/</url>
    
    <content type="html"><![CDATA[<!-- * @Author: peerless * @Date: 2020-06-28 18:25:20 * @LastEditTime: 2020-06-30 21:27:52 * @LastEditors: Please set LastEditors * @Description: A General Form of Attribute Exploration * @FilePath: \peer-less.github.io\source\_posts\2012CS-A General Form of Attribute Exploration .md   title:         A General Form of Attribute Exploration # 标题   subtitle:                                              # 副标题   date:          2020-06-30                              # 时间   author         peerless                                # 作者   heaeder-img:   img/post-bg-.jpg                        # 这篇文章标题背景图片   catalog:       true                                    # 是否归档   tags:                                                  # 标签      - 学术      - 属性探索         --> <h1 id="A-General-Form-of-Attribute-Exploration"><a href="#A-General-Form-of-Attribute-Exploration" class="headerlink" title="A General Form of Attribute Exploration"></a>A General Form of Attribute Exploration</h1><h2 id="行文思路简要总结"><a href="#行文思路简要总结" class="headerlink" title="行文思路简要总结"></a>行文思路简要总结</h2><p>问题：如何从经典属性探索出发获得通用的属性探索？</p><p>​            1.回顾经典属性探索算法。</p><p>​            2.通过引入3个条件扩展属性探索算法：</p><ul><li>引入两个闭包算子$c_{cert}(A)$与$c_{univ}(A)$。</li><li>不明确指定提供的反例。</li><li>只向专家询问满足$c_{cert}(A)\subsetneq B\subset c_{univ}(A)$的蕴涵式$A\rightarrow B$。</li></ul><p>​            3.对通用属性探索算法进行非冗余性与完备性验证。</p><p>​            4.对通用算法询问蕴涵式时进行改进，使算法向专家询问的蕴涵式的数量是最小的。</p><ul><li>改进方法：若属性集$A=c_{cert}(A)\subsetneq c_{univ}(A)$，则向专家询问蕴涵式$A\rightarrow c_{univ}(A)$。</li></ul><p>总结：</p><p>​            1. 该通用属性算法首先保留了经典属性探索算法的大多性质：完备性、非冗余性和向专家询问蕴涵式的数量最少等。</p><p>​            2. 该通用属性探索算法能够处理抽象给定的闭包算子和部分形式背景下给出的反例。</p><p>问题：</p><ol><li>理论层面：该通用属性探索算法理论上的时间复杂度仍为指数级，无法在多项式时间内计算主基。</li><li>应用层面：该通用属性探索算法并未说明在实际的应用领域中效果如何。</li></ol><h2 id="论文基本信息："><a href="#论文基本信息：" class="headerlink" title="论文基本信息："></a>论文基本信息：</h2><p>​            1.作者：Daniel Borchmann（德累斯顿州立大学数学与科学学院代数研究所丹尼尔·博尔赫曼）</p><p>​            2.期刊：Computer Science</p><p>​            3.时间：2018/11/15</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>提出一种属性探索的一般形式。</li><li>扩展属性探索的适用性。</li><li>将属性探索的现有变种转换为一般形式，简化理论。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>1.属性探索的变种：</p><ul><li>部分形式背景的属性探索$^{[3]}$。</li><li>描述逻辑模型的探索$^{[1, 2]}$。</li></ul><font color="red">研究问题：寻找一种将所有变种都包含在内的一般属性探索。</font><p>可行原因：</p><pre><code>     1.属性探索算法的整体结构均保持不变。     2.属性探索的所有重要属性均保留了下来。</code></pre><p>单词积累:</p><pre><code>     discourse: 论述、谈话、演讲。</code></pre><p>​         have a close look: 仔细研究。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>蕴涵式</p><script type="math/tex; mode=display">A\rightarrow B \Leftrightarrow A^{'}\subseteq B^{'} \Leftrightarrow B\subseteq A^{''}</script><p>一些声明：</p><ul><li><p>$Imp(M)$：$M$上的所有蕴涵式集合。</p></li><li><p>$Imp(K)$：形式背景$K$上的所有蕴涵式集合。</p></li><li><p>$Th(K)$：形式背景$K$上的所有成立的蕴涵式集合。</p></li><li><p>$L\subseteq Imp(K)$，$A\subseteq M$。如果对于所有的蕴涵式$(X\rightarrow Y)\in L$，都有$X\subsetneq A或Y\subseteq A$成立，则集合$A$在$L$下为封闭集合。则可做如下定义：</p><script type="math/tex; mode=display">  \begin{align}  L^0(A):&=A \newline  L^1(A):&=\bigcup{ \{Y\mid (X\rightarrow Y)\in L, X\subseteq A\} } \newline  L^i(A):&=L^1(L^{i-1}(A))\ for\ i > 1 \newline  L(A):&=\bigcup\limits_{i\in N}{L^i(A)}  \end{align}</script><p>  $L(A)$是基于$L$下比集合A大的最小集合。</p><p>  由上可知$L^N(A)$表示集合$A$的子集所能推出来的属性集合的子集所能推出来的属性集合。</p><p>  $Cn(L)$：在$L$下成立的所有蕴涵式集合。</p><ul><li>$B\subseteq Imp(K)$在$L$下是非冗余的。$\Leftrightarrow B\subseteq Cn(L)$。</li><li>$B\subseteq Imp(K)$在$L$下是完备的。$\Leftrightarrow Cn(B)\supseteq L$。</li><li><p>$B$是主基。$\Leftrightarrow Cn(B)=Cn(L)$。</p><p>$P$在$L$下是伪闭集，如果以下条件成立：</p></li><li><p>$P\neq L(P)$。</p></li><li><p>对于所有的伪闭集$Q\subsetneq P$有$L(Q)\subseteq P$成立。</p><p>特别的，若$L=Th(K)$，则$P$称为形式背景$K$的伪内涵。则主基可定义为：$Can(L):=\{P\rightarrow L(P)\mid P 在L下是伪闭集\}$。</p><p>由于不知道对象$g$是否具有属性$m$，因此需要引入部分形式背景：存在一个属性集的有序对$(A, B), A, B\in M且A\bigcap B=\varnothing$所构成的集合即为部分形式背景。</p></li><li><p>若$A\bigcup B=M$，则集合称为全局对象描述，即相应对象明确具有的属性集。</p></li><li>若$A\bigcup B\neq M$，则集合称为部分对象描述，即相应对象明确不具有的属性集。</li></ul></li></ul><h2 id="3-Classical-Attribute-Exploration"><a href="#3-Classical-Attribute-Exploration" class="headerlink" title="3 Classical Attribute Exploration"></a>3 Classical Attribute Exploration</h2><p>$M$是一个有限的属性集合，$K$是基于$M$的形式背景，$p$是基于$M$的领域专家。</p><p>1.对属性集通过字典序进行初始化，并获得第一个属性$\varnothing / P$.</p><p>2.如果$P^{‘}=P$，跳到第5步；否则，令$r:=(P\rightarrow P^{“})$</p><p>3.如果专家认为$r$成立，把$r$加入蕴涵集$Imp(K)$中。</p><p>4.如果专家认为$r$不成立，给出相应的一个反例$C$，将$C$（及其相应的属性）作为新对象加入当前工作形式背景$K$中。</p><p>5.找到字典序$P$的下一个属性集$Q$，若不存在下一个，则算法终止，否则，将$P$设置为$Q$。</p><h2 id="4-Generalizing-Attribute-Exploration"><a href="#4-Generalizing-Attribute-Exploration" class="headerlink" title="4 Generalizing Attribute Exploration"></a>4 Generalizing Attribute Exploration</h2><font color="red">推广目的：用更抽象的术语来描述属性探索，以允许该算法在经典属性探索算法之外的应用。</font><p>3个扩展：</p><p>1.提出两个闭包操作符：$c_{univ}、c_{cert}$。</p><ul><li><p>$c_{univ}$：我们已知的全部领域知识。$c_{univ}(A)$可以从$A$推出的属性集。</p></li><li><p>$c_{cert}$：我们已知的某些知识。$c_{cert}(A)$确定可以从$A$推出的属性集。</p></li></ul><p>2.采用如下方法对算法进行扩展：</p><ul><li><p>提供反例时，不需要完全指定。</p></li><li><p>只需要所提反例所拥有的信息与所给的蕴涵式相矛盾即可。</p></li><li><p>提供关于该对象具有哪些属性以及不具有的属性信息即可。</p></li></ul><p>3.我们向专家提出的蕴涵式是一种特殊的形式：</p><ul><li>搜索关于$c_{cert}$和$c_{univ}$未确定的蕴涵式$A\rightarrow B$，即$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$。对于这样的蕴涵式，我们不能从$c_{cert}$和$c_{univ}$推断出属性集$c_{univ}(A) \backslash B$是否能从$A$推出或不能推出，因此，我们需要向专家询问。</li></ul><p>Algorithm(General Attribute Exploration)</p><p>   $q$为部分领域专家。  </p><p>   ​    1. $K = \varnothing$<br>   ​    2. 对于有限属性集$A\subseteq M$，若存在有限属性集$B$，有$c_{cert}(A)\subsetneq B\subseteq c_{univ}(A)$成立，则考虑蕴涵式$A\rightarrow B$；如果不存在，则算法终止，输出$K$与$c_{cert}$。</p><p>   ​    3. 若$q$认为$A\rightarrow B$成立，那么更新$c_{cert}^{‘}=X\longmapsto c_{cert}(L(c_{cert}(X) ) )$。</p><p>​        4. 否则，$(C, D)=q(A\rightarrow B)$作为反例，加入形式背景$K$。</p><p>​        5. 对所有的反例$(C, D)$有：</p><script type="math/tex; mode=display">\begin{align}C':&=c_{cert}(C) \newlineD':&=D\bigcup \{ {m\in M\backslash D\mid c_{cert}(C\bigcup \{m\})\bigcap D\neq \varnothing}\}\end{align}</script><p>​        6. 对所有的$X\subseteq M，X\longmapsto c_{univ}(X)\bigcap K(X)$，即将$c_{univ}$更新为$c_{univ}^{‘}(X)=c_{univ}(X)\bigcap K(X)$。</p><p>​        7. 跳转到2。</p><p>两条性质：非冗余性与完备性。</p><h2 id="5-Computing-Undecided-Implications"><a href="#5-Computing-Undecided-Implications" class="headerlink" title="5 Computing Undecided Implications"></a>5 Computing Undecided Implications</h2><p>目的：使向专家询问蕴涵式的数量是最小的。</p><p>原因：该通用属性探索算法对询问蕴涵式的顺序未做约束。（可能询问以确定的蕴涵式）</p><p>经典属性探索下计算未确定的蕴涵式：</p><p>​        在基于已知蕴涵式$k$的条件下，计算属性集$P$字典序之后的最小属性集$Q(Q\subseteq M，且Q不是当前工作形式背景的内涵)$。那么就需要向专家询问蕴涵式$Q\rightarrow Q^{“}$。</p><p>通用属性探索下计算未确定的蕴涵式：</p><p>​        为了保证向专家询问的蕴涵式的数量是最小的，将通用属性探索算法的第二步更改为：$对于有限属性集A\subseteq M$，有$A=c_{cert}(A)\subsetneq c_{univ}(A)$并且$A$是$\subseteq-minimal$成立，则考虑蕴涵式$A\rightarrow c_{univ}(A)$。</p><p>在给出算法总是能得到最小数量的询问蕴涵式前，先给出如下定义：</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h2><p style="text-indent:2em">从使用领域专家的经典的属性探索中推导出一种更加通用的属性探索算法。它能够处理抽象给定的闭包算子，并且可以处理部分给定的反例。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
