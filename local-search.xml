<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>第二章 线性代数</title>
    <link href="/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    <url>/2020/07/10/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="第二章-线性代数"><a href="#第二章-线性代数" class="headerlink" title="第二章 线性代数"></a>第二章 线性代数</h1><p class="note note-info">深度学习、花书、线性代数</p><a id="more"></a><h2 id="2-1-标量、向量、矩阵和张量"><a href="#2-1-标量、向量、矩阵和张量" class="headerlink" title="2.1 标量、向量、矩阵和张量"></a>2.1 标量、向量、矩阵和张量</h2><p>基本概念</p><ul><li><span style="color:red">标量（scalar）</span>：一个标量就是一个单独的数。</li></ul><p style="text-indent:2em">它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用<i>斜体</i>表示标量。标量通常被赋予<i>小写的变量名称</i>。当我们介绍标量时，会明确它们是哪种类型的数。比如，在定义实数标量时，我们可能会说 ‘‘令 <i>s ∈ R</i> 表示一条线的斜率’’；在定义自然数标量时，我们可能会说 ‘‘令 <i>n ∈ N</i> 表示元素的数目’’。</p><ul><li><span style="color:red">向量（vector）</span>：一个向量就是有序排列的一列数。通过次序中的索引，我们可以确定每个单独的数。</li></ul><p style="text-indent:2em">通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量<b>粗体的小写变量名称</b>，比如 <b>x</b>。向量中的元素可以通过带<i>脚标的斜体</i>表示。向量 <b>x</b> 的第一个元素是 <b>x</b><sub><i>1</i></sub>，第二个元素是 <b>x</b><sub><i>2</i></sub>，等等。</p><p>$\mathbb{R}^n$：如果每个元素都属于 $\mathbb{R}$，并且该向量有 $n$ 个元素，那么该向量属于实数集 $\mathbb{R}$ 的 $n$ 次笛卡尔乘积构成的集合，记为 $\mathbb{R}^n$。</p><ul><li><p><span style="color:red">—-</span>：集合补集的索引。比如 $x_{-1}$ 表示 $x$ 中除 $x_1$ 外的所有元素。</p></li><li><p><span style="color:red">矩阵（matrix）</span>：矩阵是一个二维数组，其中的每一个元素被两个索引（而非一个）所确定。</p><ul><li>$A_{i,:}$：$A$ 的第 $i$ 行（row）。</li><li>$A_{:,i}$： $A$的第 $i$ 列（column）。</li><li>$f(A)_{i,j}$：表示函数$f$ 作用在$A$上输出的矩阵的第 $i$行第 $j$ 列元素。</li></ul></li><li><p><span style="color:red">张量（tensor）</span>：一个数组中的元素分布在若干维坐标（坐标超过两维）的规则网格中，我们称之为张量。我们使用字体 <b>A</b> 来表示张量 “A’’。张量 <b>A</b> 中坐标为 (i, j, k) 的元素记作 $\bf{A_{i,j,k} }$。</p></li><li><p><span style="color:red">主对角线（main diagonal）</span>：从左上角到右下角的对角线。</p></li><li><p><span style="color:red">广播（broadcasting）</span>：向量$b$ 和矩阵$A$ 的每一行相加。这个简写方法使我们无需在加法操作前定义一个将向量 $b$ 复制到每一行而生成的矩阵。这种隐式地复制向量 $b$ 到很多位置的方式称为广播。即$C=A+b\Leftrightarrow C<em>{i,j}=A</em>{i,j}+b_j$。</p></li></ul><h2 id="2-2-矩阵与向量相乘"><a href="#2-2-矩阵与向量相乘" class="headerlink" title="2.2 矩阵与向量相乘"></a>2.2 矩阵与向量相乘</h2><p>基本概念</p><ul><li><span style="color:red">元素对应乘积（element-wise product）或者 <strong>Hadamard</strong> 乘积（Hadamard product）</span>：两个矩阵中对应元素的乘积，记为$A\odot B$。</li><li><span style="color:red">点积（dot product）</span>：两个相同维数的向量 <strong>x</strong> 和 <strong>y</strong> 的 点积（dot product）可看作是矩阵乘积$x^Ty$。注：$x^Ty=y^Tx$。</li></ul><h2 id="2-3-线性相关与生成子空间"><a href="#2-3-线性相关与生成子空间" class="headerlink" title="2.3 线性相关与生成子空间"></a>2.3 线性相关与生成子空间</h2><p>基本概念</p><ul><li><span style="color:red">线性组合（linear combination）</span>：<br><p style="text-indent:2em">为了分析方程有多少个解，我们可以将 A 的列向量看作从 原点（origin）（元素都是零的向量）出发的不同方向，确定有多少种方法可以到达向量 b。在这个观点下，向量 x 中的每个元素表示我们应该沿着这些方向走多远，即 xi 表示我们需要沿着第 i 个向量的方向走多远：</p><script type="math/tex; mode=display">  Ax=\sum\limits_i x_iA_{:,i}</script><p style="text-indent:2em">一般而言，这种操作被称为 线性组合（linear combination）。形式上，一组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即：</p><script type="math/tex; mode=display">  \sum\limits_i c_iv^{(i)}</script></li><li><span style="color:red">列空间（column space）或值域（range）</span>：确定 <strong>Ax</strong> = <strong>b</strong> 是否有解相当于确定向量 <strong>b</strong> 是否在 <strong>A</strong> 列向量的生成子空间中。这个特殊的生成子空间被称为 <strong>A</strong> 的 列空间（column space）或者 <strong>A</strong> 的 值域（range）。</li><li><span style="color:red">线性无关（linearly independent）</span>：如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为 线性无关（linearly independent）。</li><li><span style="color:red">奇异的（singular）</span>：一个列向量线性相关的方阵。</li></ul><h2 id="2-4-范数"><a href="#2-4-范数" class="headerlink" title="2.4 范数"></a>2.4 范数</h2><p style="text-indent:2em"><span style="color:red">范数（singular）</span>是将向量映射到非负值的函数。。直观上来说，向量 x 的范数衡量从原点到点 x 的距离。更严格地说，范数是满足下列性质的任意函数：</p><ul><li>$f(x)=0\Rightarrow x=0$</li><li>$f(x+y)\le f(x)+f(y)$  （三角不等式（triangle inequality））</li><li>$\forall \alpha\in \mathbb{R},\ f(\alpha x)=|\alpha|f(x)$</li></ul><p>$L^p$范数：</p><script type="math/tex; mode=display">\parallel x\parallel_p=\left(\sum\limits_i |x_i|^p \right)^{\frac1p}\qquad 其中p\in\mathbb{R},\ p\ge1。</script><p style="text-indent:2em"><span style="color:red">L<sup>2</sup> 范数（singular）</span>被称为欧几里得范数（Euclidean norm）。它表示从原点出发到向量 x 确定的点的欧几里得距离。</p><script type="math/tex; mode=display">\parallel x\parallel_2=\left(\sum\limits_i |x_i|^2 \right)^{\frac12}</script><p style="text-indent:2em">两个向量的 点积（dot product）可以用范数来表示。具体地：</p><script type="math/tex; mode=display">x^Ty=\parallel x\parallel_2\parallel y\parallel_2 \cos\theta\qquad 其中\theta表示x和y之间的夹角。</script><p><span style="color:red">平方L<sup>2</sup> 范数（singular）</span>：</p><script type="math/tex; mode=display">\parallel x\parallel_2=\sum\limits_i |x_i|^2=x^Tx</script><p style="text-indent:2em">平方 L<sup>2</sup> 范数对x 中每个元素的导数只取决于对应的元素，而 L<sup>2</sup> 范数对每个元素的导数却和整个向量相关。但是在很多情况下，平方 L<sup>2</sup> 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。</p><p><span style="color:red">L<sup>1</sup> 范数（singular）</span>：</p><script type="math/tex; mode=display">\parallel x\parallel_1=\sum\limits_i |x_i|</script><p style="text-indent:2em">当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 L<sup>1</sup> 范数。每当x 中某个元素从 0 增加 ϵ，对应的 L<sup>1</sup> 范数也会增加 ϵ。</p><p><span style="color:red">L<sup>&infin;</sup> 范数（singular）</span>：表示向量中具有最大幅值的元素的绝对值。</p><script type="math/tex; mode=display">\parallel x\parallel_{\infin}=\max\limits_i |x_i|</script><p><span style="color:red"><strong>Frobenius</strong> 范数（Frobenius norm）</span>：衡量矩阵的大小。</p><script type="math/tex; mode=display">\parallel A\parallel_F=\sqrt{\sum\limits_{i,j} A_{i,j}^2}</script><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-第一章 引言</title>
    <link href="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/"/>
    <url>/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h1><p class="note note-info">深度学习、花书</p><a id="more"></a><h2 id="1-早期问题"><a href="#1-早期问题" class="headerlink" title="1 早期问题"></a>1 早期问题</h2><p style="text-indent:2em">在人工智能的早期，那些对人类智力来说非常困难、但对计算机来说相对简单的问题得到迅速解决。比如，那些可以通过一系列形式化的数学规则来描述的问题。</p><p style="text-indent:2em">人工智能的真正挑战在于解决那些对人来说很容易执行、但很难形式化描述的任务，如识别人们所说的话或图像中的脸。对于这些问题，我们人类往往可以凭借直觉轻易地解决。</p><p>针对这些比较直观的问题，本书讨论一种解决方案<span style="color:red">(AI 深度学习（deep learning）)</span>。</p><ul><li>该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。</li><li>让计算机从经验获取知识，可以避免由人类来给计算机形式化地指定它需要的所有知识。</li><li>层次化的概念让计算机构建较简单的概念来学习复杂概念。</li></ul><p>一个<span style="color:red">关键挑战</span>：如何将这些非形式化的知识传达给计算机。</p><p><span style="color:red">硬编码 (hard code)</span>：计算机可以使用逻辑推理规则来自动地理解这些形式化语言中的声明。这就是众所周知的人工智能的知识库（knowledge base）方法。</p><p><span style="color:red">机器学习（machine learning）</span>：由于依靠硬编码的知识体系面对的困难表明，AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。</p><p>引入机器学习使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决</p><p>策。如：</p><ul><li>一个被称为 逻辑回归（logistic regression）的简单机器学习算法可以决定是否建议剖腹产 (Mor-Yosef <em>et al.</em>, 1990)。</li><li>简单机器学习算法朴素贝叶斯（naive Bayes）则可以区分垃圾电子邮件和合法电子邮件。</li></ul><p>但是这些简单的机器学习算法的性能在很大程度上依赖于给定数据的<span style="color:red">表示（repre sentation）</span>。，表示的选择对机器学习算法的性能产生巨大的影响。</p><p style="text-indent:2em">许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。例如，对于通过声音鉴别说话者的任务来说，一个有用的特征是对其声道大小的估计。这个特征为判断说话者是男性、女性还是儿童提供了有力线索。</p><p style="text-indent:2em">然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所以我们可能会想用车轮的存在与否作为特征。不幸的是，我们难以准确地根据像素值来描述车轮看上去像什么。虽然车轮具有简单的几何形状，但它的图像可能会因场景而异，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等。</p><p style="text-indent:2em">解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为<span style="color:red">表示学习（representation learning）</span>。</p><p style="text-indent:2em">表示学习算法的典型例子是<span style="color:red">自编码器</span>（autoencoder）。自编码器由一个编码器encoder）函数和一个解码器（decoder）函数组合而成。</p><p><span style="color:red">编码器</span>：编码器函数将输入数据转换为一种不同的表示。</p><p><span style="color:red">解码器</span>：解码器函数则将这个新的表示转换到原来的形式。</p><p style="text-indent:2em">我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。为了实现不同的特性，我们可以设计不同形式的自编码器。</p><p style="text-indent:2em">当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的<span style="color:red">变差因素（factors of variation）</span>。在此背景下，‘‘因素’’ 这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。</p><p style="text-indent:2em">显然，从原始数据中提取如此高层次、抽象的特征是非常困难的。许多诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类水平的理解来辨识。这几乎与获得原问题的表示一样困难，因此，乍一看，表示学习似乎并不能帮助我们。</p><p><span style="color:red">深度学习（deep learning）</span>通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。深度学习让计算机通过较简单概念构建复杂的概念。图 1.2 展示了深度学习系统。</p><img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710112001631.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710112001631.png" srcset="/img/loading.gif" alt></p><p><span style="color:red">多层感知机（multilayer perceptron, MLP）</span>：是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。</p><p>种度量模型深度的方式：</p><ol><li>基于评估架构所需执行的顺序指令的数目。<br><p style="text-indent:2em">假设我们将模型表示为给定输入后，计算对应输出的流程图，则可以将这张流程图中的最长路径视为模型的深度。正如两个使用不同语言编写的等价程序将具有不同的长度；相同的函数可以被绘制为具有不同深度的流程图，其深度取决于我们可以用来作为一个步骤的函数。图 1.3 说明了语言的选择如何给相同的架构两个不同的衡量。</p><img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710113610351.png" srcset="/img/loading.gif" class></li></ol><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710113610351.png" srcset="/img/loading.gif" alt></p><ol><li>将描述概念彼此如何关联的图的深度视为模型深度。<p style="text-indent:2em">在这种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图更深。这是因为系统对较简单概念的理解在给出更复杂概念的信息后可以进一步精细化。例如，一个 AI 系统观察其中一只眼睛在阴影中的脸部图像时，它最初可能只看到一只眼睛。但当检测到脸部的存在后，系统可以推断第二只眼睛也可能是存在的。在这种情况下，概念的图仅包括两层（关于眼睛的层和关于脸的层），但如果我们细化每个概念的估计将需要额外的 n 次计算，即计算的图将包含 2n 层。 </p></li></ol><p style="text-indent:2em">深度学习是一种特定类型的机器学习，具有强大的能力和灵活性，它将大千世界表示为嵌套的层次概念体系（由较简单概念间的联系定义复杂概念、从一般抽象概括到高级抽象表示）。</p><p>图1.4 说明了这些不同的 AI 学科之间的关系。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115035883.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115035883.png" srcset="/img/loading.gif" alt></p><p>图 1.5 展示了每个学科如何工作的高层次原理。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115508658.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115508658.png" srcset="/img/loading.gif" alt></p><p>图 1.6 展示了本书的组织架构。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115751350.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710115751350.png" srcset="/img/loading.gif" alt></p><h2 id="2-深度学习的历史趋势"><a href="#2-深度学习的历史趋势" class="headerlink" title="2 深度学习的历史趋势"></a>2 深度学习的历史趋势</h2><h3 id="2-1-神经网络的众多名称和命运变迁"><a href="#2-1-神经网络的众多名称和命运变迁" class="headerlink" title="2.1 神经网络的众多名称和命运变迁"></a>2.1 神经网络的众多名称和命运变迁</h3><p>三次发展浪潮：</p><ol><li>20世纪40年代到60年代深度学习的雏形出现在控制论（cybernetics）中。</li><li>20 世纪 80 年代到 90 年代深度学习表现为联结主义（connectionism）。</li><li>直到 2006 年，才真正以深度学习之名复兴。</li></ol><p>图 1.7 给出了定量的展示。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710121022428.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710121022428.png" srcset="/img/loading.gif" alt></p><p><span style="color:red">控制论</span>：</p><ul><li>$n$个输入：$x_1, x_2, \cdots, x_n$。</li><li>一组权重：$w_1, w_2, \cdots, w_n$。</li><li>输出：$f(x, w)=x_1w_1+\cdots+x_nw_n$。</li></ul><p style="text-indent:2em">McCulloch-Pitts 神经元 (McCulloch and Pitts, 1943) 是脑功能的早期模型。该线性模型通过检验函数 f(x, w) 的正负来识别两种不同类别的输入。显然，模型的权重需要正确设置后才能使模型的输出对应于期望的类别。这些权重可以由操作人员设定。在 20 世纪 50 年代，<span style="color:red">感知机</span> (Rosenblatt, 1956, 1958) 成为第一个能根据每个类别的输入样本来学习权重的模型。约在同一时期，<span style="color:red">自适应线性单元 (adaptive linear element, ADALINE)</span> 简单地返回函数 *f*(**x**) 本身的值来预测一个实数 (Widrow and Hoffff, 1960)，并且它还可以学习从数据预测这些数。</p><p><span style="color:red">随机梯度下降（stochastic gradient descent）</span>的一种特例：调节 ADALINE 权重的训练算法。</p><p><span style="color:red">线性模型（linear model）</span>：基于感知机和 ADALINE 中使用的函数 <em>f</em>(x, w) 的模型。</p><p>局限性：无法学习异或（XOR）函数，即$f([0, 1], w)=1和f([1, 0], w)=1，但f([1, 1], w)=0和f([0, 0], w)=0$。</p><p><span style="color:red">计算神经科学</span>：了解大脑是如何在算法层面上工作的尝试。</p><p><span style="color:red">深度学习领域主要关注任务</span>：如何构建计算机系统，从而成功解决需要智能才能解决的任务。</p><p><span style="color:red">计算神经科学领域主要关注任务</span>：构建大脑如何真实工作的比较精确的模型。</p><p><span style="color:red">联结主义（connectionism）或并行分布处理 ( parallel distributed processing) </span>：当网络将大量简单的计算单元连接在一起时可以实现智能行为。</p><p><span style="color:red"> 分布式表示（distributed representation）</span>：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。</p><p style="text-indent:2em">例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统，表示这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或隐藏单元激活。这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。改善这种情况的方法之一是使用分布式表示，即用三个神经元描述颜色，三个神经元描述对象身份。这仅仅需要 6 个神经元而不是 9 个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。</p><p> 相关研究进展：</p><ul><li>在 20 世纪 90 年代，研究人员在使用神经网络进行序列建模的方面取得了重要进展。Hochreiter (1991b) 和 Bengio <em>et al.</em> (1994a) 指出了对长序列进行建模的一些根本性数学难题。</li><li>Hochreiter and Schmidhuber (1997)引入 长短期记忆（long short-term memory, LSTM）网络来解决这些难题。如今，LSTM 在许多序列建模任务中广泛应用，包括 Google 的许多自然语言处理任务。</li><li>神经网络继续在某些任务上获得令人印象深刻的表现 (LeCun <em>et al.</em>,  1998c; Bengio <em>et al.</em>, 2001a)。加拿大高级研究所（CIFAR）通过其神经计算和自适应感知（NCAP）研究计划帮助维持神经网络研究。该计划联合了分别由 Geoffffrey Hinton、Yoshua Bengio和 Yann LeCun 领导的多伦多大学、蒙特利尔大学和纽约大学的机器学习研究小组。这个多学科的 CIFAR NCAP 研究计划还囊括了神经科学家、人类和计算机视觉专家。</li></ul><p style="text-indent:2em">神经网络研究的第三次浪潮始于 2006 年的突破。Geoffffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton *et al.*, 2006a)，神经网络研究的第三次浪潮始于 2006 年的突破。eoffffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton *et al.*, 2006a)，我们将在第 15.1 节中更详细地描述。其他 CIFAR 附属研究小组很快表明，同样的策略可以被用来训练许多其他类型的深度网络 (Bengio andLeCun, 2007a; Ranzato *et al.*, 2007b)，并能系统地帮助提高在测试样例上的泛化能力。神经网络研究的这一次浪潮普及了 “深度学习’’ 这一术语的使用，强调研究者现在有能力训练以前不可能训练的比较深的神经网络，并着力于深度的理论重要性上 (Bengio and LeCun, 2007b; Delalleau and Bengio, 2011; ascanu *et al.*, 2014a; Montufar *et al.*, 2014)。此时，深度神经网络已经优于与之竞争的基于其他机器学习技术以及手工设计功能的 AI 系统。在写这本书的时候，神经网络的第三次发展浪潮仍在继续，尽管深度学习的研究重点在这一段时间内发生了巨大变化。第三次浪潮已开始着眼于新的无监督学习技术和深度模型在小数据集的泛化能力，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据集的能力。</p><h3 id="2-2-与日俱增的数据量"><a href="#2-2-与日俱增的数据量" class="headerlink" title="2.2 与日俱增的数据量"></a>2.2 与日俱增的数据量</h3><p>图 1.8 展示了基准数据集的大小如何随着时间的推移而显著增加。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154348314.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154348314.png" srcset="/img/loading.gif" alt></p><p>图 1.10 展示了神经元连接数如何随着时间的推移而显著增加。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154829899.png" srcset="/img/loading.gif" class></p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710154829899.png" srcset="/img/loading.gif" alt></p><p>图 1.11 展示了神经元网络规模如何随着时间的推移而显著增加。</p><p><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710155243539.png" srcset="/img/loading.gif" alt></p><h3 id="2-3-应用领域"><a href="#2-3-应用领域" class="headerlink" title="2.3 应用领域"></a>2.3 应用领域</h3><p>图像识别：</p><ol><li>最早的深度模型被用来识别裁剪紧凑且非常小的图像中的单个对象(Rumelhart <em>et al.</em>, 1986d)。</li><li>此后，神经网络可以处理的图像尺寸逐渐增加。现代对象识别网络能处理丰富的高分辨率照片，并且不需要在被识别的对象附近进行裁剪(Krizhevsky <em>et al.</em>, 2012b)。</li><li>类似地，最早的网络只能识别两种对象（或在某些情况下，单类对象的存在与否），而这些现代网络通常能够识别至少1000个不同类别的对象。对象识别中最大的比赛是每年举行的 ImageNet 大型视觉识别挑战（ILSVRC）。深度学习迅速崛起的激动人心的一幕是卷积网络第一次大幅赢得这一挑战，它将最高水准的前5 错误率从 26.1% 降到 15.3% (Krizhevsky <em>et al.</em>, 2012b)，这意味着该卷积网络针对每个图像的可能类别生成一个顺序列表，除了 15.3% 的测试样本，其他测试样本的正确类标都出现在此列表中的前 5 项里。此后，深度卷积网络连续地赢得这些比赛，截至写本书时，深度学习的最新结果将这个比赛中的前 5 错误率降到了 3.6%，如图 1.12 所示。<img src="/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710165940221.png" srcset="/img/loading.gif" class><img src="/peerless.github.io/2020/07/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%BC%95%E8%A8%80/image-20200710165940221.png" srcset="/img/loading.gif" alt></li></ol><p>语音识别：</p><ul><li>语音识别在 20 世纪 90 年代得到提高后，直到约 2000 年都停滞不前。深度学习的引入 (Dahl <em>et al.</em>, 2010; Deng <em>et al.</em>, 2010b; Seide <em>et al.</em>, 2011; Hinton <em>et al.</em>, 2012a) 使得语音识别错误率陡然下降，有些错误率甚至降低了一半。</li></ul><p>行人检测和图像分割:</p><ul><li>(Sermanet <em>et al.</em>, 2013; Farabet <em>et al.</em>, 2013; Couprie <em>et al.</em>, 2013)，并且在交通标志分类上取得了超越人类的表现 (Ciresan <em>et al.</em>, 2012)</li></ul><p style="text-indent:2em">在深度网络的规模和精度有所提高的同时，它们可以解决的任务也日益复杂。Goodfellow *et al.* (2014d) 表明，神经网络可以学习输出描述图像的整个字符序列，而不是仅仅识别单个对象。此前，人们普遍认为，这种学习需要对序列中的单个元素进行标注 (Gulcehre and Bengio, 2013)。循环神经网络，如之前提到的LSTM 序列模型，现在用于对序列和其他序列之间的关系进行建模，而不是仅仅固定输入之间的关系。这种序列到序列的学习似乎引领着另一个应用的颠覆性发展，即机器翻译 (Sutskever et al., 2014; Bahdanau et al., 2015)。</p><p>这种复杂性日益增加的趋势已将其推向逻辑结论，即神经图灵机 (Graves <em>et al.</em>, 2014) 的引入。</p><p><span style="color:red">神经图灵机 </span>：能学习读取存储单元和向存储单元写入任意内容。这样的神经网络可以从期望行为的样本中学习简单的程序。</p><p style="text-indent:2em">例如，从杂乱和排好序的样本中学习对一系列数进行排序。这种自我编程技术正处于起步阶段，但原则上未来可以适用于几乎所有的任务。</p><p>深度学习的另一个最大的成就是其在 强化学习（reinforcement learning）领域的扩展。</p><p><span style="color:red">强化学习 </span>：在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。</p><p style="text-indent:2em">DeepMind 表明，基于深度学习的强化学习系统能够学会玩Atari 视频游戏，并在多种任务中可与人类匹敌 (Mnih et al., 2015)。深度学习也显著改善了机器人强化学习的性能 (Finn et al., 2015)。</p><p>技术公司：Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflflix、NVIDIA和 NEC 等。</p><p>软件基础架构的进展：软件库如 Theano (Bergstra <em>et al.</em>, 2010a; Bastien <em>et al.</em>, 2012a)、PyLearn2 (Goodfellow <em>et al.</em>, 2013e)、Torch (Col lobert <em>et al.</em>, 2011b)、DistBelief (Dean <em>et al.</em>, 2012)、Caffffe (Jia, 2013)、MXNet (Chen <em>et al.</em>, 2015) 和 TensorFlow (Abadi <em>et al.</em>, 2015) 都能支持重要的研究项目或商业产品。</p><p style="text-indent:2em">深度学习也为其他科学做出了贡献。用于对象识别的现代卷积网络为神经科学家们提供了可以研究的视觉处理模型 (DiCarlo, 2013)。深度学习也为处理海量数据以及在科学领域作出有效的预测提供了非常有用的工具。它已成功地用于预测分子如何相互作用从而帮助制药公司设计新的药物 (Dahl et al., 2014)，搜索亚原子粒子 (Baldi et al., 2014)，以及自动解析用于构建人脑三维图的显微镜图像(Knowles-Barley et al., 2014) 等。我们期待深度学习未来能够出现在越来越多的科学领域中。</p><p style="text-indent:2em">总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的普及性和实用性都有了极大的发展。未来几年充满了进一步提高深度学习并将它带到新领域的挑战和机遇。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">deep learning</a>.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/MingchaoZhu/DeepLearning" target="_blank" rel="noopener">深度学习</a>。<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
      <category>深度学习(AI圣经)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>花书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2014-Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</title>
    <link href="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/"/>
    <url>/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/</url>
    
    <content type="html"><![CDATA[<h1 id="Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises"><a href="#Fast-Algorithms-for-Implication-Bases-and-Attribute-Exploration-Using-Proper-Premises" class="headerlink" title="Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises"></a>Fast Algorithms for Implication Bases and Attribute Exploration Using Proper Premises</h1><p class="note note-info">属性探索、快速算法</p><a id="more"></a><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Uwe Ryssel、Felix Distel、Daniel Borchmann。</p><p>​            2. 期刊：Annals of Mathematics and Artificial Intelligence。</p><p>​            3. 时间：2014。</p><p>​            4. DOI：10.1007/s10472-013-9355-9 。 </p><p style="display:none">​    @article{Ryssel2014Fast,​      title={Fast algorithms for implication bases and attribute exploration using proper premises},​      author={Ryssel, Uwe and Distel, Felix and Borchmann, Daniel},​      journal={Annals of Mathematics & Artificial Intelligence},​      volume={70},​      number={1-2},​      pages={25-53},​      year={2014},​    },​    @inproceedings{Ganter2010Two,​      title={Two Basic Algorithms in Concept Analysis},​      author={Ganter, Bernhard},​      booktitle={International Conference on Formal Concept Analysis},​      year={2010},​      },​      @article{Obiedkov2007Attribute,​      title={Attribute-incremental construction of the canonical implication basis},​      author={Obiedkov, S. and Duquenne, V.},​      journal={Annals of Mathematics & Artificial Intelligence},​      volume={49},​      number={1-4},​      pages={p.77-99},​      year={2007},​      }  ​    </p><p></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>形式概念分析的中心任务是枚举形式背景的最小蕴涵基。</li><li>本文提出了一种快速计算适当前提的新算法。<ul><li>减少了多次获得适当前提的数量。</li><li>减少了在适当前提集合内的冗余。</li></ul></li></ul><p>词汇积累：</p><ul><li>minimal hypergraph transversals：最小超图横断面。</li><li>refactoring：重构。</li><li>heuristic：启发式。</li><li>refactoring of model variants：模型变体重构。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>计算主基存在2种方法$^{[17, 26]}$，由于这两种方法均计算了概念内涵，则无法改变其指数级的复杂度$^{[1, 10]}$。</p></li><li><p>形式概念分析早期提出了一种具有适当前提基的算法，该算法避免了计算概念内涵。</p></li><li><p>有一些方法可以在多项式时间内将适当前提基转换为主基$^{[24, 28]}$。</p></li><li><p><span style="color:red">本文提出了一种快速计算适当前提的算法。基于以下三个思想：</span></p><ol><li>在适当前提和最小超图横断面之间使用一个简单的联系。</li><li>对最小超图断面的枚举问题进行了深入的研究。</li><li>可对现有算法可使用与适当前提的联系。</li></ol><p>1.首先，使用原算法遍历所有属性，并使用黑盒超图算法来计算每个属性的适当前提。<br>2.为了避免多次计算相同的适当前提，本文引入了一个候选过滤器：对属性集中的每个属性进行过滤，并且只在候选属性集中搜素合适的前提。本文表明，这种过滤方法在保持完备性的同时，大大减少了多次计算的适当前提的数量。<br>3.通过仅在交不可约属性集中搜索适当的前提来移除适当前提内的冗余。<br>4.本文认为该算法对于并行化来说是微不足道的，从而导致进一步的加速。由于其增量性质，基于主基的算法的并行化版本至今尚不为人所知。<br>5.本文将给出使用适当前提的属性探索的另一种变体。它使用与本文对主基的枚举算法相同的方法。</p></li><li><p>考虑不使用伪意图的属性探索的替代公式。在$^{[27]}$中首次尝试使用适当的前提来制定属性探索。&lt;/div</p></li></ol><p>词汇积累：</p><ul><li>well-researched：深入研究。</li><li>artifacts：史前古器物；人工产品。</li><li>negated：否定的。</li><li>fractions of a second：几分之一秒。</li><li>arguments：论点。</li></ul><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>1.$g\swarrow m$：在不包含$m$的对象内涵中，关于子集的顺序$g’$是最大的。</p><p>2.蕴涵式$\mathscr{L}$的形式化描述：</p><script type="math/tex; mode=display">\begin{align} \mathscr{L}^1(A)&=A\cup \bigcup\{Y|(X\rightarrow Y)\in \mathscr{L}, X\subseteq A\}  \newline\mathscr{L}^i(A)&=\mathscr{L}^1(\mathscr{L}^{i-1}(A) )\quad for\ i>1  \newline\mathscr{L}(A)&=\bigcup\limits_{i\in N>0}\mathscr{L}^i(A)\end{align}</script><p>原文：Then an implication X →Y follows from the set $\mathscr{L}$ of implications if and only if $Y ⊆ \mathscr{L}$(X). We write $\mathscr{L}\models(X →Y)$ if and only if X →Y follows from $\mathscr{L}$.</p><p>解释：一个蕴涵式$X\rightarrow Y$可从蕴涵式集合$\mathscr{L}$中推出来$，当且仅当$$Y\subseteq \mathscr{L}(X)$成立。可写作$\mathscr{L}\models(X →Y)$，当且仅当$X\rightarrow Y$可由$\mathscr{L}$推出来成立。</p><p><span style="color:red">sound(非冗余的、无噪声的)</span>：$\mathscr{L}$中的所有蕴涵式对于形式背景$\mathbb{K}$均成立。</p><p><span style="color:red">complete(完备的)</span>：形式背景$\mathbb{K}$中成立的所有蕴涵式均来自于$\mathscr{L}$。</p><p>如果$\mathscr{L}$对于形式背景$\mathbb{K}$是sound and complete，则$\mathscr{L}$就称为形式背景$\mathbb{K}$的一个基。进一步地，$\mathscr{L}^1(A)=\mathscr{L}(A)\ holds\ for\ all\ A\subseteq M$，则$\mathscr{L}$称为直接基。</p><p><span style="color:red">伪内涵</span>：$P\neq P^{“}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{“}\subseteq P$。则形式背景的主基为${P\rightarrow P^{“}|P: pseudo-intent\ of\ \mathbb{K}}$。</p><p><span style="color:red">前提</span>：B is called a premise for m if $m\in B^{“}\backslash B$，则形式背景的一个基为：$\mathscr{L}={B\rightarrow B^{“}|B: B\subseteq M, premise\ for\ some\ m\in M}$。</p><p><span style="color:red">适当前提</span>：B  is called a proper premise if $B^{\bullet}$ is not empty，其中$B^{\bullet}=B^{“}\backslash \left(B\cup\bigcup\limits_{S\subsetneq B}S^{“}\right)$。如果$m\in B^{\bullet}$，则称$m$在$m\in M$中是一个适当前提。可以得到$B$对于$m$是一个适当前提，当且仅当$B$在$m$的前提中是$\subseteq-minimal$。</p><p><span style="color:red">适当前提基</span>：进一步地有{$B\rightarrow B^{\bullet}|B: proper\ premise$}是形式背景$\mathbb{K}$的一个complete and sound 直接基。则该集合称为形式背景$\mathbb{K}$的适当前提基。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/18fa1d96ab86c4723e998f61ddc13aa-1594124738433.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/c0c630c6145b2877b6a0a06c71ad0af-1594124834463.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/ea0112b26a45b674667e8603afc5dea-1594124814483.png" srcset="/img/loading.gif" alt></p><h2 id="3-Proper-Premises-as-Minimal-Hypergraph-Transversals"><a href="#3-Proper-Premises-as-Minimal-Hypergraph-Transversals" class="headerlink" title="3 Proper Premises as Minimal Hypergraph Transversals"></a>3 Proper Premises as Minimal Hypergraph Transversals</h2><h3 id="3-1-基本定义"><a href="#3-1-基本定义" class="headerlink" title="3.1 基本定义"></a>3.1 基本定义</h3><p><span style="color:red">minimal hypergraph transversals</span>：用于从关系型数据库中挖掘函数依赖。</p><p>超图以前已经用于关联规则挖掘的相关任务$^{[33]}$。超图如何应用于数据挖掘的概述可以在$^{[20]}$中找到。 </p><p><span style="color:red">hypergraph</span>：$V$是有限的顶点集，$V$上的一个超图$\mathscr{H}$是幂集$2^V$的一个子集。每一个集合$E\in \mathscr{H}$为超图的一条边，与经典图论不同的是，这条边可以关联到两个以上的顶点，也可以关联到两个以下的顶点。</p><p><span style="color:red">hypergraph transversal</span>：一个集合$S\subseteq V$被称为$\mathscr{H}$的 hypergraph transversal，当它与每条边$E\in \mathscr{H}$都相交。 即$\forall E\in \mathscr{H}, S\cap E\neq \varnothing$。</p><p><span style="color:red">minimal hypergraph transversal</span>：集合$S\subseteq V$被称为最小超图横断面，当S在$\mathscr{H}$的所有超图横断面中的子集序下是最小的。</p><p><span style="color:red">transversal hypergraph</span>：$\mathscr{H}$的所有 minimal hypergraph transversal构成的集合。记为$Tr(\mathscr{H})$。</p><p>TRANSHYP：判定超图$\mathscr{H}$是否是超图$\mathscr{G}$的 transversal hypergraph 的问题。</p><p>TRANSENUM：枚举出超图$\mathscr{G}$的所有minal hypergraph transversal 的问题。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224053977.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707224159024.png" srcset="/img/loading.gif" alt></p><p>解释：$P\subseteq M\backslash{m}对于m\in M$是一个前提，当且仅当对于所有的$g\in G\ with\ g\swarrow m$成立。$P$对于$m$是一个适当前提，当且仅当在$m$的所有前提中，按子集序$P$是最小的。则有推论</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707225613699.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230432218.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708154757386.png" srcset="/img/loading.gif" alt></p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" class><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230713322.png" srcset="/img/loading.gif" alt><br><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200707230531420.png" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li><p>implicitly：含蓄地；暗中地；间接的；不言而喻。</p></li><li><p>in contrast to：与…不同。</p></li><li><p>be incident to：关联到…。</p></li><li><p>quasi-polynomial time：准多项式时间。</p></li></ul><h2 id="4-Improvements-to-the-Algorithm"><a href="#4-Improvements-to-the-Algorithm" class="headerlink" title="4 Improvements to the Algorithm"></a>4 Improvements to the Algorithm</h2><h3 id="4-1-Avoiding-Duplicates-using-Candidate-Sets"><a href="#4-1-Avoiding-Duplicates-using-Candidate-Sets" class="headerlink" title="4.1 Avoiding Duplicates using Candidate Sets"></a>4.1 Avoiding Duplicates using Candidate Sets</h3><p>原因：Algorithm1会多次计算适当前提，因为它们可以是多个属性的适当前提。如{c, e}是属性a, b, d 的适当前提，则其会计算3次。</p><p>第一种思想：根据当前属性，引入相关属性的候选集。则只需在候选集集合$C$中搜索$\mathscr{H}_{\mathbb{K}, m}^{\swarrow}$最小超图横断面即可。</p><p>冗余条件：$\mu w\wedge\mu m\le\mu v\lt\mu m$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708170224965.png" srcset="/img/loading.gif" alt></p><p>Algorithm2的正确性证明。</p><h3 id="4-2-Irreducible-Attributes"><a href="#4-2-Irreducible-Attributes" class="headerlink" title="4.2 Irreducible Attributes"></a>4.2 Irreducible Attributes</h3><p>从候选集合C中移除属性m，属性m满足条件：$\mu m=\wedge_{i=1}^n\mu x_i\ for\ i=1,…, n$。这样的属性称为$\wedge$可约属性，其集合为$N$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708174500270.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708174500270.png" srcset="/img/loading.gif" alt></p><p>Algorithm3的正确性证明。</p><p>词汇积累</p><ul><li>intuition：直觉的。</li><li>snippet：片段。</li><li>identify：确定；识别。</li></ul><h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h2><p>算法：</p><ul><li>SB：基于Next-Closure算法计算主基的实现算法。</li><li>HT：和Algorithm1一样，在所有适当前提中计算超图横断面的算法。</li><li>PP：Algorithm3的实现算法。 </li></ul><p>数据集：</p><ol><li>SPECT [9], which describes Single Proton Emission Computed Tomography (SPECT) images. This data set is given as a dyadic formal context with 187 objects, 23 attributes, and an approximate density of 0.38.</li><li>Congressional V oting Records of the U.S. House of Representatives from 1984 [31]. It contains 435 objects, 16 attributes and is given as many valued context. It has been nominally scaled, resulting in a context with 50 attributes and an approximate density of 0.34.</li><li>The third structured data set originates from the application described in Section 5.2 and [30]. It has 26 objects, 79 attributes and an approximate density of 0.35.<img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191106203.png" srcset="/img/loading.gif" class><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191106203.png" srcset="/img/loading.gif" alt></li></ol><p>实验结果：<img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191311319.png" srcset="/img/loading.gif" class> <img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200708191311319.png" srcset="/img/loading.gif" alt></p><p>词汇积累</p><ul><li>contranominal：相反的。</li><li>trade-off：权衡。</li><li>explicit：显式化的。</li></ul><h2 id="6-Attribute-Exploration"><a href="#6-Attribute-Exploration" class="headerlink" title="6 Attribute Exploration"></a>6 Attribute Exploration</h2><p>基本概念</p><p><span style="color:red">属性探索</span>：一种交互式的形式化算法，即使原形式背景是不完整的，也能够获得sound和complete的蕴涵基集合。其特点为：</p><ul><li>一个不完备的形式背景。</li><li>一个拥有全部领域知识的专家。</li><li>每次迭代，将蕴涵式向专家询问。<ul><li>接受，则将蕴涵式加入蕴涵基集合。</li><li>拒绝，专家提供一个反例加入当前工作形式背景。</li></ul></li></ul><p><span style="color:red">原始形式背景（initial context）</span>：初始探索的形式背景。</p><p><span style="color:red">背景知识（background knowledge）</span>：初始蕴涵式集合。</p><p><span style="color:red">当前工作形式背景（current working context）</span>：每次探索时的形式背景。</p><p><span style="color:red">已知蕴涵式集合（the set of known implications）</span>：探索过程中专家接受的蕴涵式构成的集合。</p><p><span style="color:red">最终形式背景（final context）</span>：整个探索过程结束后的形式背景。</p><p><span style="color:red">背后的形式背景（background context）</span>$\mathbb{K}_{BG}$：隐式的已知的形式背景。</p><p>可以将属性探索看作是使$\mathbb{K}_{BG}$中知识显式化的过程。</p><p>主要挑战:避免重新计算已经被专家接受的蕴涵式。</p><p>词汇积累</p><ul><li>interactive formalism：交互式的形式主义。</li><li>facilitate：便于，促进，帮助；使容易。</li><li>implicit：隐式的。</li></ul><h3 id="6-1-Incremental-Computation-of-Proper-Premises-Using-Berge-Multiplication"><a href="#6-1-Incremental-Computation-of-Proper-Premises-Using-Berge-Multiplication" class="headerlink" title="6.1 Incremental Computation of Proper Premises Using Berge Multiplication"></a>6.1 Incremental Computation of Proper Premises Using Berge Multiplication</h3><p>定义两个超图$\mathscr{G}$和$\mathscr{H}$以边进行合并为</p><script type="math/tex; mode=display">\mathscr{G}\vee \mathscr{H}:=\{ g\cup h|g\in \mathscr{G},\ h\in \mathscr{H} \}</script><p>集合$S$中$\subseteq-minimal$集（子集序）为</p><script type="math/tex; mode=display">min(S):=\{ X\in S |not\ \exists Y\in S:Y\subsetneq X \}</script><p>则对于两个有限的超图$\mathscr{G}$和$\mathscr{H}$有</p><script type="math/tex; mode=display">Tr(\mathscr{G} \cup \mathscr{H})=\min(Tr(\mathscr{G}) \vee Tr(\mathscr{H}))</script><p>Berge Multiplication算法的相关研究：</p><ul><li>Takata$^{[32]}$给出了一个超图族的例子，使得Berge Multiplication算法的最小运行时间为$n^{\Omega(\log\log n)}$，$n$为相应的输出规模。</li><li>Boros$^{[8]}$证明了Berge Multiplication算法在$n^{\sqrt{n} }$的时间内能够获得超图$\mathscr{H}$的所有最小超图横截面，$n$为相应的输出规模。</li><li>所有的Berge Multiplication的变体算法最坏情况的运行时间至少为$n^{\Omega(\log\log n)}$$^{[21]}$，$n$为相应的输出规模。</li></ul><p>词汇积累</p><ul><li>divide and conquer：分而治之。</li><li>hypergraph transversal algorithms：超图遍历算法。</li><li>open question：悬而未决的问题。</li><li>permutation：排列。</li></ul><h4 id="6-1-2-A-Naive-Exploration-Algorithm-using-Proper-Premises"><a href="#6-1-2-A-Naive-Exploration-Algorithm-using-Proper-Premises" class="headerlink" title="6.1.2 A Naive Exploration Algorithm using Proper Premises"></a>6.1.2 A Naive Exploration Algorithm using Proper Premises</h4><p>第一个形式化的带有前提的属性探索算法，令$\mathbb{K}$为原始形式背景，$\mathscr{L}$为背景知识并且$\mathscr{E}=\mathscr{H}_{\mathbb{K}, m}^{\notin}$。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712111542675.png" srcset="/img/loading.gif" class> <p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712111542675.png" srcset="/img/loading.gif" alt></p><p>解释：</p><ol><li>该算法遍历所有的属性$m\in M$，并通过Berge Multiplication不断考虑$\mathscr{E}$中的边来计算属性$m$的适当前提。</li><li>如果$\mathscr{L} \nvDash P \rightarrow P^{“}$，则向专家询问蕴涵式$P \rightarrow P^{“}$。<ol><li>接受，将其$P \rightarrow P^{“}$加入$\mathscr{L}$。</li><li>拒绝，$g$作为$P \rightarrow { m }$的反例。<ol><li>如果$m \notin g’$，则将集合$M \backslash g’$作为$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的一条边加入$\mathscr{E}$中。</li></ol></li></ol></li><li>2步骤一直持续到$\mathscr{E}$中不存在边剩余，此时就可得到$\mathbb{K}_{B, G}$中所有属性$m$的适当前提。</li><li>最后考虑$M$中的剩余属性。</li></ol><p>这一过程在算法6中正式给出。请注意，在此算法中，我们仅非正式地将专家交互描述为“expert confirms $Q→Q^{“}$”或“ask expert for valid counterexample ”，这在文献中是常见的。但也有可能更正式地描述这种相互作用，就像在$^{[6]}$中所做的那样。然而，我们不会在这里这样做，因为我们没有必要进行进一步的考虑。</p><p>在算法6终止时的形式背景为$\mathbb{K}$，$\mathscr{L}$是$\mathbb{K}<em>{B, G}$的一个基，并且$\mathscr{L}$仅包含形式$P \rightarrow P^{“}$的蕴涵式，其中$P$是$\mathbb{K}</em>{B, G}$的适当前提。</p><p>证明$\mathbb{K}$的适当前提就是$\mathbb{K}_{B, G}$的适当前提。$\Leftarrow$Lemma 3。</p><p>假设对于某些属性$m$，$P$是$\mathbb{K}<em>{BG}$中的一个真前提。由Lemma 2 知这等价于$P$是$\mathscr{H}</em>{\mathbb{K}, m}^{\notin}$的极小超图横截面。由于所有的反例均取自于$\mathbb{K}<em>{BG}$，则有$\mathscr{H}</em>{\mathbb{K}, m}^{\notin} \subseteq  \mathscr{H}<em>{BG, m}^{\notin}$,因此$P$也是$\mathscr{H}</em>{\mathbb{K}, m}^{\notin}$的一个超图横截面。这证明了在$\mathscr{H}_{\mathbb{K}, m}^{\notin}$的超图横截面中$P$的极小性。</p><p>现在假设$Q \subseteq P$是$\mathscr{H}<em>{\mathbb{K}, m}^{\notin}$的一个极小超图，那么$Q$是$\mathbb{K}$的一个适当前提，并且有$\mathscr{L} \vDash Q \rightarrow Q^{“}$。而由Lemma 3 可知，$Q$也是$\mathbb{K}</em>{BG}$的一个适当前提，是$\mathscr{H}<em>{BG, m}^{\notin}$的一个极小超图横截面。则$P=Q$。这证明了$P$也是$\mathscr{H}</em>{\mathbb{K}, m}^{\notin}$的极小超图横截面即$P$是$\mathbb{K}$的一个适当前提。</p><p>$\Rightarrow$ 算法6终止的形式背景$\mathbb{K}$的前提基与$\mathbb{K}_{BG}$的前提基相同。</p><p>词汇积累</p><ul><li>whereupon：因此。</li></ul><h3 id="6-2-Using-the-improvements"><a href="#6-2-Using-the-improvements" class="headerlink" title="6.2 Using the improvements"></a>6.2 Using the improvements</h3><h4 id="6-2-1-Querying-the-hierarchy-of-attribute-concepts"><a href="#6-2-1-Querying-the-hierarchy-of-attribute-concepts" class="headerlink" title="6.2.1 Querying the hierarchy of attribute concepts"></a>6.2.1 Querying the hierarchy of attribute concepts</h4><p><span style="color:red">find single proper premises</span>:</p><p>算法2和算法3都要求在第一步中计算所有的单一适当前提。在属性探索中，形式背景$\mathbb{K}$最初是不完整的。因此，简单地计算$\mathbb{K}$的所有单个适当前提是不够的，因为我们不知道当形式背景$\mathbb{K}$扩张时它们是否仍然是适当前提。</p><p>然而，如果属性${ m }$是$\mathbb{K}$的一个适当前提，并且专家确认蕴涵式${ m } \rightarrow { m }^{“}$成立，则 Lemma 3 确保了当形式背景$\mathbb{K}$扩张时${ m }$仍然是一个前提。</p><p><span style="color:red">find irreducibles</span>:</p><p>在算法3中，需要知道哪些属性概念是交不可约的。不幸的是，在工作形式背景$\mathbb{K}$中满足可约的属性概念$\mu m$在$\mathbb{K}$的扩展中可能不再满足可约。</p><p>我们可以向专家提出蕴涵式${ n \in { m }^{“} | \mu m &lt; \mu n } \rightarrow { m }$来确定是否可以移除可约属性${ m }$。如果专家接受该蕴涵式，则属性$m$在形式背景$\mathbb{K}$扩张时仍然是交可约的。</p><p>算法7显示了这两个初始查询是如何执行的。</p><img src="/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712154529970.png" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/04/2014-Fast%20Algorithms%20for%20Implication%20Bases%20and%20Attribute%20Exploration%20Using%20Proper%20Premises/image-20200712154529970.png" srcset="/img/loading.gif" alt></p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h2><p>本文</p><ol><li>证明了，当主基具有最小基数时，适当前提的基通常可以更有效地计算。</li><li>在基于超图的算法中进行非常简单的优化会产生很大的性能提升。</li><li>我们已经在随机的、人工生成的形式背景以及来自实际应用的形式背景上对它进行了评估。我们的评估表明，在这些特定的数据集上，我们的算法比现有算法更快。这表明我们的方法可能也更适用于其他数据集，特别是当这些数据集包含大量内涵时。</li><li>至于最小基数，则还需要额外的最小化步骤。在这种情况下，性能收益可能较小。然而，在像模型重构这样的应用程序中，最小基数只是次要的。</li><li>展示了如何执行基于适当前提的探索。并提供了详细的算法描述，并展示了如何将本文第一部分中的改进应用于属性探索设置。</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>快速算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW2</title>
    <link href="/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/"/>
    <url>/2020/07/03/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW2/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）"><a href="#李宏毅Machine-Learning-HW2（利用逻辑回归预测收入是否大于50K）" class="headerlink" title="李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）"></a>李宏毅Machine Learning HW2（利用逻辑回归预测收入是否大于50K）</h1><p>本文主要参考了博文<a href="https://www.cnblogs.com/HL-space/p/10785225.html" target="_blank" rel="noopener">https://www.cnblogs.com/HL-space/p/10785225.html</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是根据每个ID的各种属性值去判断该ID对应角色是Winner还是Losser（收入是否大于50K）。显然这是一个二分类问题，可以运用Logistic Regression来实现。</p><ul><li>spam_train.csv：形状为4000×59，4000行数据对应4000个角色，ID编号从1到4001，59列属性中，第一列为角色ID，最后一列为分类结果（label为0/1），中间57列为角色对应的57种属性值。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><ol><li>本次任务先对数据做线性回归，得出每个样本的回归值，计算公式为：</li></ol><script type="math/tex; mode=display">y^n=\sum\limits_{i=1}^{57}w_ix_i^n+b</script><p>其中，$x_i^n$为第$n$个样本值，$y^n$为对应的回归结果。</p><ol><li><p>将回归结果送入$sigmod$函数，得到对应的概率值。计算公式为：</p><script type="math/tex; mode=display"> p^n=\frac{1}{1+e^{-y^n} }</script></li><li><p>众所周知，不管线性回归还是Logistic回归，其关键和核心就在于通过误差的反向传播来更新参数，进而使模型不断优化。因此，损失函数的确定及对各参数的求导就成了重中之重。在分类问题中，模型一般针对各类别输出一个概率分布，因此常用交叉熵作为损失函数。交叉熵可用于衡量两个概率分布之间的相似、统一程度，两个概率分布越相似、越统一，则交叉熵越小；反之，两概率分布之间差异越大、越混乱，则交叉熵越大。</p></li></ol><p>下式表示k分类问题的交叉熵，P为label，是一个概率分布，常用one_hot编码。例如针对3分类问题而言，若样本属于第一类，则P为(1,0,0)，若属于第二类，则P为(0,1,0)，若属于第三类，则为(0,0,1)。即所属的类概率值为1，其他类概率值为0。Q为模型得出的概率分布，可以是(0.1,0.8,0.1)等。具体计算公式为：</p><script type="math/tex; mode=display">Loss^n=-\sum\limits_1^kp^n\log Q^n</script><p>在实际应用中，为求导方便，常使用以e为底的对数。则上式可化为：</p><script type="math/tex; mode=display">Loss^n=-\sum\limits_1^kp^n\ln Q^n</script><p>针对本次作业而言，虽然模型只输出了一个概率值p，但由于处理的是二分类问题，因此可以很快求出另一概率值为1-p，即可视为模型输出的概率分布为Q(p，1-p)。将本次的label视为概率分布P(y,1-y)，即Winner(label为1)的概率分布为(1,0)，分类为Losser(label为0)的概率分布为(0,1)。则其==损失函数==为：</p><script type="math/tex; mode=display">Loss^n=-[\hat{y}^n\ln p^n+(1-\hat{y}^n)\ln(1-p^n)]</script><p>Loss对权重$w$求偏导，有：</p><script type="math/tex; mode=display">\begin{align}因为&\frac{\partial\ln p^n}{\partial p^n}=\frac{1}{p^n}=1+e^{-y^n} \newline同理&\frac{\partial\ln (1-p^n)}{\partial p^n}=\frac{-1}{1-p^n}=\frac{-e^{-y^n} }{1+e^{-y^n} } \newline而且&\frac{\partial p^n}{\partial y^n}=-\frac{1}{(1+e^{-y^n})^2}(e^{-y^n})(-1)=\frac{-e^{-y^n} }{(1+e^{-y^n})^2} \newline和&\frac{\partial y^n}{\partial w_i}=x_i \newline\newline\end{align}</script><script type="math/tex; mode=display">\begin{align}所以有\frac{\partial Loss^n}{\partial w_i}&=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial w_i}]  \newline&=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})x_i]  \newline&=-x_i(\hat{y}^n-p^n)  \end{align}</script><p>同理Loss对偏置$b$求偏导，有：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial Loss^n}{\partial b}&=-[\hat{y}^n\frac{\partial\ln p^n}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}+(1-\hat{y}^n)\frac{\partial\ln (1-p^n)}{\partial p^n}\frac{\partial p^n}{\partial y^n}\frac{\partial y^n}{\partial b}]  \newline&=-[\hat{y}^n(1+e^{-y^n})(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})+(1-\hat{y}^n)(\frac{-e^{-y^n} }{1+e^{-y^n} })(\frac{-e^{-y^n} }{(1+e^{-y^n})^2})]  \newline&=-(\hat{y}^n-p^n)  \end{align}</script><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>首先将所有空值以0填充，然后对数据进行标准化处理。</p><p>上述操作完成后，将表格的第2列至58列取出为x(shape为4000×57)，将最后一列取出做label y(shape为4000×1)。</p><p>进一步划分训练集和验证集，分别取x、y中前3000个样本为训练集x_train(shape为3000×57)，y_train(shape为3000×1)，后1000个样本为验证集x_val(shape为1000×57)，y_val(shape为1000×1)。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"spam_train.csv"</span>)<span class="hljs-comment"># 空值填0</span>data = data.fillna(<span class="hljs-number">0</span>)<span class="hljs-comment"># 转换为array类型</span>data = np.array(data)<span class="hljs-comment"># 取样本x(4000*57)</span>x = data[:, <span class="hljs-number">1</span>:<span class="hljs-number">-1</span>]<span class="hljs-comment"># 标准化</span>x[<span class="hljs-number">-1</span>] /= np.mean(x[<span class="hljs-number">-1</span>])x[<span class="hljs-number">-2</span>] /= np.mean(x[<span class="hljs-number">-2</span>])<span class="hljs-comment"># 取目标值y</span>y = data[:, <span class="hljs-number">-1</span>]<span class="hljs-comment"># 划分训练集与验证集</span>x_train, x_val = x[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>, :], x[<span class="hljs-number">3000</span>:, :]y_train, y_val = y[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y[<span class="hljs-number">3000</span>:]</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练模型，更新参数。</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(x_train, y_train, epoch)</span>:</span>    num = x_train.shape[<span class="hljs-number">0</span>]    dim = x_train.shape[<span class="hljs-number">1</span>]    bias = <span class="hljs-number">0</span>    weights = np.ones(dim)    learning_rate = <span class="hljs-number">1</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 偏置梯度平方和</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 权重梯度平方和</span>    weight_sum = np.zeros(dim)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        b_g = <span class="hljs-number">0</span>        w_g = np.zeros(dim)                <span class="hljs-comment"># 在所有数据上计算梯度，梯度计算时针对损失函数求导</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):            y_pre = weights.dot(x_train[j, :]) + bias            sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))            b_g += (<span class="hljs-number">-1</span>) * (y_train[j] - sig)            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(dim):                w_g[k] += (<span class="hljs-number">-1</span>) * (y_train[j] - sig) * x_train[j, k] + <span class="hljs-number">2</span> * reg_rate * weights[k]        b_g /= num        w_g /= num        <span class="hljs-comment"># adagrad</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>         <span class="hljs-comment"># 更新权重和偏置</span>        bias -= learning_rate / bias_sum ** <span class="hljs-number">0.5</span> * b_g        weights -= learning_rate / weight_sum ** <span class="hljs-number">0.5</span> * w_g        <span class="hljs-comment"># 每训练100轮，输出一次在训练集上的正确率。</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            acc = <span class="hljs-number">0</span>            result = np.zeros(num)            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):                y_pre = weights.dot(x_train[j, :]) + bias                sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))                <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:                    result[j] = <span class="hljs-number">1</span>                <span class="hljs-keyword">else</span>:                    result[j] = <span class="hljs-number">0</span>                <span class="hljs-keyword">if</span> result[j] == y_train[j]:                    acc += <span class="hljs-number">1.0</span>                loss += (<span class="hljs-number">-1</span>) * (y_train * np.log(sig) + (<span class="hljs-number">1</span> - y_train[j]) * np.log(<span class="hljs-number">1</span> - sig))            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为'</span>.format(i), loss / num)            print(<span class="hljs-string">'&#123;&#125; 次训练后, 训练集的精度为&#123;:.2f&#125;'</span>.format(i, acc / num))    <span class="hljs-keyword">return</span> weights, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 验证模型效果</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate</span><span class="hljs-params">(x_val, y_val, weights, bias)</span>:</span>    num = <span class="hljs-number">1000</span>    loss = <span class="hljs-number">0</span>    acc = <span class="hljs-number">0</span>    result = np.zeros(num)    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num):        y_pre = weights.dot(x_val[j, :]) + bias        sig = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-y_pre))        <span class="hljs-keyword">if</span> sig &gt;= <span class="hljs-number">0.5</span>:            result[j] = <span class="hljs-number">1</span>        <span class="hljs-keyword">else</span>:            result[j] = <span class="hljs-number">0</span>        <span class="hljs-keyword">if</span> result[j] == y_val[j]:            acc += <span class="hljs-number">1.0</span>        loss += (<span class="hljs-number">-1</span>) * (y_val * np.log(sig) + (<span class="hljs-number">1</span> - y_val[j]) * np.log(<span class="hljs-number">1</span> - sig))    <span class="hljs-keyword">return</span> loss / num, acc / num</code></pre><h3 id="3-4-测试模型"><a href="#3-4-测试模型" class="headerlink" title="3.4 测试模型"></a>3.4 测试模型</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练轮数</span>epoch = <span class="hljs-number">4000</span><span class="hljs-comment"># 开始训练</span>w, b = train(x_train, y_train, epoch)<span class="hljs-comment"># 在验证集上看效果</span>loss, acc = validate(x_val, y_val, w, b)print(<span class="hljs-string">'验证集的损失值为:'</span>, loss)print(<span class="hljs-string">'验证集的精度值为:'</span>, acc)</code></pre><h3 id="3-5-实验结果"><a href="#3-5-实验结果" class="headerlink" title="3.5 实验结果"></a>3.5 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为 [<span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span>  <span class="hljs-number">1.50623089</span> ... <span class="hljs-number">1.50623089</span> <span class="hljs-number">1.50623089</span> <span class="hljs-number">0.1929692</span> ]<span class="hljs-number">0</span> 次训练后, 训练集的精度为<span class="hljs-number">0.62</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.82</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.90</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">1800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">2000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.86</span> <span class="hljs-number">2400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.85</span> <span class="hljs-number">2600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span> <span class="hljs-number">2800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">2800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3000</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3000</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3200</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3200</span> 次训练后, 训练集的精度为<span class="hljs-number">0.91</span> <span class="hljs-number">3400</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3400</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3600</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3600</span> 次训练后, 训练集的精度为<span class="hljs-number">0.92</span> <span class="hljs-number">3800</span> 次训练后, 训练集的损失为 [nan nan nan ... nan nan nan]<span class="hljs-number">3800</span> 次训练后, 训练集的精度为<span class="hljs-number">0.84</span>验证集的损失值为: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]验证集的精度值为: <span class="hljs-number">0.864</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率基本能达到90%，验证集分类的正确率能达到86.4%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>台大李宏毅机器学习-HW1</title>
    <link href="/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/"/>
    <url>/2020/07/02/%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-HW1/</url>
    
    <content type="html"><![CDATA[<h1 id="李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）"><a href="#李宏毅Machine-Learning-HW1（利用线性回归预测PM2-5的数值）" class="headerlink" title="李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）"></a>李宏毅Machine Learning HW1（利用线性回归预测PM2.5的数值）</h1><p>本文主要参考了博文<a href="https://blog.csdn.net/m123_45n/article/details/106560274" target="_blank" rel="noopener">https://blog.csdn.net/m123_45n/article/details/106560274</a>，本人对其进行了推导和实现。</p><h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p style="text-indent:2em">本次任务的目的是利用Liner Regression去预测丰原市PM2.5的数值。对丰原市一年的观测记录分为了train set和test set，其中train set 是丰原市每个月前20天的所有记录。test set则是从丰原市剩下的记录取样出来。train set和test set都是以csv格式的文件进行保存的。</p><ul><li>train.csv：每个月前20天的完整数据。</li><li>test.csv：从剩下的数据中取样出连续的10小时为一组，前九个小时的所有观测数据当作==特征值==，第十个小时的PM2.5当作==目标值==。共取出240组不重复的test data，然后根据feature去预测出第十小时的PM2.5的值。</li><li>Data（中含有18项观测数据）：AMB_TEMP，CH4，CO，NHMC，NO，NO2，NOx，O3，PM10，PM2.5，RAINFALL，RH，SO2，THC，WD_HR，WIND_DIREC，WIND_SPEED，WS_HR。</li></ul><h2 id="2-模型选择"><a href="#2-模型选择" class="headerlink" title="2 模型选择"></a>2 模型选择</h2><p>本次任务选择的模型为线性回归模型：</p><script type="math/tex; mode=display">y=\sum\limits_{i=0}^8w_ix_i+b。</script><ul><li><p>$i$从0到8是因为选取前9个小时作为==特征值==输入，每个输入都有一个==权重值==$w$与之相乘，再加上一个偏置$b$，即为线性回归模型。通过这个模型去预测第十个小时的PM2.5的值。</p></li><li><p>此外，可将该模型的运算转换为向量运算：</p><script type="math/tex; mode=display">  \begin{align}  y=  \begin{bmatrix}  w_0\ \dots\ w_8   \end{bmatrix}  \begin{bmatrix}  x_0\newline \vdots\newline x_8   \end{bmatrix}  +b\end{align}</script></li><li><p>选择的<strong>Loss Function</strong>为：</p></li></ul><script type="math/tex; mode=display">Loss=\frac{1}{2m}\sum\limits_{i=0}^{m-1}(\hat{y}^i-y^i)^2+\frac12\lambda\sum\limits_{j=0}^8w_j^2</script><ul><li><p>Loss对$w_j$求导为：</p><script type="math/tex; mode=display">  \frac{\partial L}{\partial w_j}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-x_j)+\lambda w_j</script></li><li></li></ul><script type="math/tex; mode=display">\frac{\partial L}{\partial b}=\frac1m\sum\limits_{i=0}^{m-1}(\hat{y}^i-w_jx_j-b)(-1)</script><ul><li><p>则参数更新为：</p><script type="math/tex; mode=display">  \begin{align}  w_j:=w_j-\eta \frac{\partial L}{\partial w_j} \newline  b:=b-\eta \frac{\partial L}{\partial b}  \end{align}</script><p>  其中将$w_j$的更新转换为向量运算：</p><script type="math/tex; mode=display">  \begin{bmatrix}  w_0 \newline \vdots \newline w_8   \end{bmatrix}  :=  \begin{bmatrix}  w_0 \newline \vdots \newline w_8   \end{bmatrix}  -\eta  \begin{bmatrix}  \frac{\partial L}{\partial w_0} \newline \vdots \newline \frac{\partial L}{\partial w_8}   \end{bmatrix}</script></li><li><p>Optimizer的选择：Adagrad。更新方式为：</p></li></ul><script type="math/tex; mode=display">\begin{align}w^1=&w^0-\frac{\eta^0}{\sigma^0}g^0 \newline \vdots \newlinew^{t+1}=&w^t-\frac{\eta^t}{\sigma^t}g^t\end{align}</script><p>​                其中$g^t$为$w^t$的梯度值，而且有：</p><script type="math/tex; mode=display">\begin{align}\sigma^t=&\sqrt{\frac{1}{t+1}\sum\limits_{i=0}^t(g^i)^2} \newline\eta^t=&\frac{\eta}{t+1}\end{align}</script><p>​                带入后，则有：</p><script type="math/tex; mode=display">w^{t+1}=w^t-\frac{\eta}{\sqrt{\sum\limits_{i=0}^t(g^i)^2} }</script><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p style="text-indent:2em">本次训练结果损失值为nan是因为涉及到log0的运算，结果为无穷大，则打印出来的Loss为nan。但是在这个模型当中，可以看到在经过4000次训练后，训练集分类的正确率能达到90%，验证集分类的正确率能达到87.2%。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后1000项作为validation data，所以可能并没有达到很好的效果。</p><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span>data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, header=<span class="hljs-number">0</span>)<span class="hljs-comment"># 删除无关特征</span>data.drop([<span class="hljs-string">"日期"</span>,<span class="hljs-string">"測站"</span>,<span class="hljs-string">"測項"</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataProcess</span><span class="hljs-params">(data)</span>:</span>    <span class="hljs-string">''' 数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数：</span><span class="hljs-string">        data: 原始训练集数据</span><span class="hljs-string">         </span><span class="hljs-string">    返回：</span><span class="hljs-string">        x: 一天中PM2.5前9项特征构成的训练集。</span><span class="hljs-string">        y: 一天中PM2.5第10项特征构成的目标集。</span><span class="hljs-string">        data: 将原始数据集的每个数据值均转换为float类型后的数据集。</span><span class="hljs-string">    '''</span>    x_list, y_list = [], []    data = data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    <span class="hljs-comment"># 将所有数据转换成float类型</span>    data = np.array(data).astype(float)    <span class="hljs-comment"># 将数据集拆分为多个数据帧</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>): <span class="hljs-comment"># 18 * 240 = 4320</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">15</span>): <span class="hljs-comment"># 24-9=15(组)</span>            <span class="hljs-comment"># 截取PM2.5前9项作为训练数据</span>            sample = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j:j+<span class="hljs-number">9</span>]            <span class="hljs-comment"># 截取PM2.5第10项作为目标数据</span>            label = data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>][j+<span class="hljs-number">9</span>]            x_list.append(sample)            y_list.append(label)    x = np.array(x_list)    y = np.array(y_list)    <span class="hljs-keyword">return</span> x, y, data</code></pre><h3 id="3-2-模型训练"><a href="#3-2-模型训练" class="headerlink" title="3.2 模型训练"></a>3.2 模型训练</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainModel</span><span class="hljs-params">(x_data, y_data, epoch)</span>:</span>    <span class="hljs-string">''' 训练模型，从训练集中拿出3000个数据用来训练，剩余600个数据用于验证。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_data: </span><span class="hljs-string">        y_data:</span><span class="hljs-string">        epoch:</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        weight: 权重</span><span class="hljs-string">        bias：偏置</span><span class="hljs-string">        </span><span class="hljs-string">    '''</span>    <span class="hljs-comment"># 初始化偏置</span>    bias = <span class="hljs-number">0</span>    <span class="hljs-comment"># 初始化权重,生成一个9列的行向量，并全部初始化为1。</span>    weight = np.ones(<span class="hljs-number">9</span>)    <span class="hljs-comment"># 初始化学习率为1。</span>    learning_rate = <span class="hljs-number">1</span>    <span class="hljs-comment"># 初始化正则项系数为0.001。</span>    reg_rate = <span class="hljs-number">0.001</span>    <span class="hljs-comment"># 用于存放偏置的梯度平方和。</span>    bias_sum = <span class="hljs-number">0</span>    <span class="hljs-comment"># 用于存放权重的梯度平方和。</span>    weight_sum = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):        <span class="hljs-comment"># 偏置梯度平均值。</span>        b_g = <span class="hljs-number">0</span>        <span class="hljs-comment"># 权重梯度平均值</span>        w_g = np.zeros(<span class="hljs-number">9</span>)        <span class="hljs-comment"># 在所有数据上计算w和b的梯度。</span>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):            b_g += (y_data[j] - weight.dot(x_data[j]) - bias) * (<span class="hljs-number">-1</span>) <span class="hljs-comment"># 如果2个一维向量dot，则结果为它们的内积。</span>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                w_g[k] += (y_data[j] - weight.dot(x_data[j]) - bias) * (-x_data[j, k])        <span class="hljs-comment"># 求平均值</span>        b_g /= <span class="hljs-number">3000</span>        w_g /= <span class="hljs-number">3000</span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):            w_g[k] += reg_rate * weight[k]                <span class="hljs-comment"># adagrad优化</span>        bias_sum += b_g ** <span class="hljs-number">2</span>        weight_sum += w_g ** <span class="hljs-number">2</span>        bias -= learning_rate / (bias_sum ** <span class="hljs-number">0.5</span>) * b_g        weight -= learning_rate / (weight_sum ** <span class="hljs-number">0.5</span>) * w_g        <span class="hljs-comment"># 每训练200次输出一次误差</span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            loss = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):                loss += (y_data[j] - weight.dot(x_data[j]) - bias) ** <span class="hljs-number">2</span>            loss /= <span class="hljs-number">3000</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">9</span>):                loss += reg_rate * (weight[j] ** <span class="hljs-number">2</span>)            print(<span class="hljs-string">' &#123;&#125; 次训练后, 训练集的损失为&#123;:.2f&#125;'</span>.format(i, loss / <span class="hljs-number">2</span>))        <span class="hljs-keyword">return</span> weight, bias</code></pre><h3 id="3-3-验证模型"><a href="#3-3-验证模型" class="headerlink" title="3.3 验证模型"></a>3.3 验证模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validateModel</span><span class="hljs-params">(x_val, y_val, weight, bias)</span>:</span>    <span class="hljs-string">''' 验证模型，返回损失值。</span><span class="hljs-string">    </span><span class="hljs-string">    参数</span><span class="hljs-string">        x_val: </span><span class="hljs-string">        y_val: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        loss: 平均损失值。</span><span class="hljs-string">    '''</span>    loss = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">600</span>):        loss += (y_val[i] - weight.dot(x_val[i]) - bias) ** <span class="hljs-number">2</span>        <span class="hljs-keyword">return</span> loss / <span class="hljs-number">600</span></code></pre><h3 id="3-4-测试数据预处理"><a href="#3-4-测试数据预处理" class="headerlink" title="3.4 测试数据预处理"></a>3.4 测试数据预处理</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testDataProcess</span><span class="hljs-params">(test_data)</span>:</span>    <span class="hljs-string">''' 测试数据预处理</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        test_data: 测试数据集。</span><span class="hljs-string">    </span><span class="hljs-string">    返回</span><span class="hljs-string">        testList: 测试数据集。</span><span class="hljs-string">    '''</span>    testList = []    test_data = test_data.replace([<span class="hljs-string">'NR'</span>], [<span class="hljs-number">0.0</span>])    test_data = np.array(test_data).astype(float)    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">4320</span>, <span class="hljs-number">18</span>):        testList.append(test_data[i:i+<span class="hljs-number">18</span>][<span class="hljs-number">9</span>])    <span class="hljs-keyword">return</span> np.array(testList)</code></pre><h3 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 训练 '''</span>    <span class="hljs-comment"># 读取训练数据，并将其转换为列表形式。</span>    data = pd.read_csv(<span class="hljs-string">"train.csv"</span>, usecols=np.arange(<span class="hljs-number">3</span>, <span class="hljs-number">27</span>).tolist())    x_data, y_data, data = dataProcess(data)        x_train, y_train = x_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>], y_data[<span class="hljs-number">0</span>:<span class="hljs-number">3000</span>]    x_val, y_val = x_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>], y_data[<span class="hljs-number">3000</span>:<span class="hljs-number">3600</span>]    weight, bias = trainModel(x_train, y_train, <span class="hljs-number">2000</span>)    savePre(weight, bias)    print(<span class="hljs-string">"训练得到的模型的weight为&#123;&#125;"</span>.format(weight))    print(<span class="hljs-string">"训练得到的模型的bias为&#123;&#125;"</span>.format(bias))    loss = validateModel(x_val, y_val, weight, bias)    print(<span class="hljs-string">"模型模型在验证集的loss为&#123;:.2f&#125;"</span>.format(loss))</code></pre><h3 id="3-6-测试模型"><a href="#3-6-测试模型" class="headerlink" title="3.6 测试模型"></a>3.6 测试模型</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testModel</span><span class="hljs-params">(x_test, weight, bias)</span>:</span>    <span class="hljs-string">''' 用测试数据集测试模型，并将结果保存到output.csv中</span><span class="hljs-string"></span><span class="hljs-string">    参数</span><span class="hljs-string">        x_test: </span><span class="hljs-string">        weight: 权重。</span><span class="hljs-string">        bias: 偏置。</span><span class="hljs-string">    '''</span>    f = open(<span class="hljs-string">"output.csv"</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>, newline=<span class="hljs-string">""</span>)    csv_write = csv.writer(f)    csv_write.writerow([<span class="hljs-string">"id"</span>, <span class="hljs-string">"value"</span>])    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(x_test)):        output = weight.dot(x_test[i]) + bias        csv_write.writerow([<span class="hljs-string">"id_"</span> + str(i), str(output)])    f.close()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">''' 测试 '''</span>    <span class="hljs-comment"># 读取测试数据集</span>    pre = pd.read_csv(<span class="hljs-string">"pre.csv"</span>)    preList = list(pre.replace(<span class="hljs-string">","</span>, <span class="hljs-string">" "</span>))    weight = []    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list(preList[<span class="hljs-number">0</span>].split(<span class="hljs-string">","</span>)):        weight.append(float(i))    bias = float(preList[<span class="hljs-number">1</span>])    test_data = pd.read_csv(<span class="hljs-string">"test.csv"</span>,header=<span class="hljs-literal">None</span>,usecols=np.arange(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>).tolist())    x_test = testDataProcess(test_data)    print(x_test.shape)    print(x_test)    testModel(x_test, np.array(weight), bias)</code></pre><h3 id="3-7-实验结果"><a href="#3-7-实验结果" class="headerlink" title="3.7 实验结果"></a>3.7 实验结果</h3><pre><code class="hljs python"><span class="hljs-number">0</span> 次训练后, 训练集的损失为<span class="hljs-number">477.36</span> <span class="hljs-number">200</span> 次训练后, 训练集的损失为<span class="hljs-number">25.08</span> <span class="hljs-number">400</span> 次训练后, 训练集的损失为<span class="hljs-number">23.30</span> <span class="hljs-number">600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.67</span> <span class="hljs-number">800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.37</span> <span class="hljs-number">1000</span> 次训练后, 训练集的损失为<span class="hljs-number">22.22</span> <span class="hljs-number">1200</span> 次训练后, 训练集的损失为<span class="hljs-number">22.14</span> <span class="hljs-number">1400</span> 次训练后, 训练集的损失为<span class="hljs-number">22.10</span> <span class="hljs-number">1600</span> 次训练后, 训练集的损失为<span class="hljs-number">22.08</span> <span class="hljs-number">1800</span> 次训练后, 训练集的损失为<span class="hljs-number">22.07</span>训练得到的模型的weight为[ <span class="hljs-number">0.00194352</span> <span class="hljs-number">-0.02562139</span>  <span class="hljs-number">0.18138721</span> <span class="hljs-number">-0.19572767</span> <span class="hljs-number">-0.0230436</span>   <span class="hljs-number">0.40959797</span> <span class="hljs-number">-0.51985702</span>  <span class="hljs-number">0.06835844</span>  <span class="hljs-number">1.03413381</span>]训练得到的模型的bias为<span class="hljs-number">1.948721373892966</span>模型模型在验证集的loss为<span class="hljs-number">39.17</span></code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p style="text-indent:2em">在这个模型当中，可以看到在经过2000次训练后，模型在验证集上的损失已达到了39.17。不过由于本次任务选择的模型较为简单，而且训练数据集也并没有按照正规方法分为training data和validation data，只是简单地将前3000项作为training data，后600项作为validation data，所以可能并没有达到很好的效果。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2015-CLA-NextCloures Parallel Computation of the Canonical Base</title>
    <link href="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/"/>
    <url>/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/</url>
    
    <content type="html"><![CDATA[<h1 id="NextClosures-Parallel-Computation-of-the-Canonical-Base"><a href="#NextClosures-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="NextClosures: Parallel Computation of the Canonical Base"></a>NextClosures: Parallel Computation of the Canonical Base</h1><div class="note note-primary">            <p>属性探索、并行算法、NextCloures</p>          </div> <a id="more"></a><!-- <p class='note note-info'>论文、属性探索</p> --><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1. 作者：Francesco Kriegel 、Daniel Borchmann</p><p>​            2. 会议：CLA</p><p>​            3. 时间：2015</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>前人所提的计算主基的算法大多均为串行，一次只计算一个伪内涵。</li><li>本文引入一种并行方式计算主基。</li></ul><p>词汇积累：</p><ul><li>canonical base: 主基，正则基，规范基。</li><li>remedies: 救济方法，弥补手段。</li><li>deficit: 缺陷。</li><li>is proportional to: 与之成正比。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ol><li><p>有两种串行算法$^{[6, 12]}$，即它们一个接一个的计算蕴涵式。</p></li><li><p>目前为止，还不知道能否在多项式时间内计算主基。</p></li><li><p>已有并行计算形式背景的方法$^{[5, 13]}$。</p></li></ol><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><ol><li>A model of $X\rightarrow Y$ is a set $T\subseteq M\Leftrightarrow X\subseteq T\Rightarrow Y\subseteq T$.</li></ol><p>2.A model of $L$ is a model of all implications in $L$, and $X^L$ is the superset of $X$ that is a model of $L$.</p><p>3.$X^L$的计算方式如下：</p><script type="math/tex; mode=display">\begin{align}X^L:=\bigcup_{n\geq 1}X^{L_n}\ where\ X^{L_1}:&=X\bigcup \{B\mid A\rightarrow B\in L\ and\ A\subseteq X\} \newlineand\ X^{L_{n+1} }:&=(X^{L_n})^{L_1} for\ all\ n\in N\end{align}</script><p>4.内涵：$B=B^{II}$。</p><p>伪内涵：$P\neq P^{II}$，且$P$的每一个伪内涵子集$Q，Q\subsetneq P$，满足$Q^{II}\subseteq P$。将形式背景$K$上的所有伪内涵集合记为$PsInt(K)$。</p><p>蕴涵式：${P\rightarrow P^{II}\mid P\in PsInt(K)}$。</p><h2 id="3-Parallel-Computation-of-the-Canonical-Base"><a href="#3-Parallel-Computation-of-the-Canonical-Base" class="headerlink" title="3 Parallel Computation of the Canonical Base"></a>3 Parallel Computation of the Canonical Base</h2><p>1.$NextCloures$算法由Ganter$^{[6]}$提出，用于枚举主基。</p><ul><li>算法思想：以一定的线性顺序(即选择顺序)计算形式背景K的所有内涵和伪内涵。</li><li>优势：下一个(伪)意图是唯一确定的，但我们可能需要回溯才能找到它。</li><li>该算法本质上是顺序的，即不可能将其并行化。</li></ul><p>2.我们的方法按照子集序枚举所有的内涵和伪内涵。</p><ul><li>易于并行化枚举。</li><li>多线程实现中，不同线程之间不需要通信。</li></ul><p>原因：检测$P$是否为伪内涵时，只需判断$P$的伪内涵子集$Q\subsetneq P$，是否满足满足$Q^{II}\subseteq P$。即可按基数递增来判断伪内涵。</p><p>算法思想的基本工作流程：</p><p>​            1. 判断$\varnothing=\varnothing^{II}\Rightarrow \varnothing$是内涵/伪内涵。</p><ol><li>假设基数$&lt;k$的所有伪内涵均已确定，然后就可正确判断出属性集$P，P\subseteq M，|P|=k$是内涵/伪内涵。</li></ol><p>具体的工作流程：</p><p>基本定义：</p><ul><li>$K$：有限的形式背景。</li><li>$k$：当前候选集的基数。候选集为储存当前层级的内涵/伪内涵。</li><li>$C$：候选集合。</li><li>$\mathcal{B}$：形式概念集合。</li><li>$L$：蕴涵式集合。</li></ul><p>具体流程：</p><p>1.$k:=0，C:={\varnothing}，\mathcal{B}:=\varnothing，L:=\varnothing$.</p><p>2.并行化：对于每个基数$|C|=k$的候选集合C $\in C$，确定它是否是$L^<em>-closed$。如果不是，则将它的$L^</em>-closure$加入候选集合$C$，跳到5。如果是$L^*-closed$，则跳到3。</p><p>3.如果C是形式背景$K$的内涵，那么将形式概念$(C^I，C)$加入集合$\mathcal{B}$中。否则C必为伪内涵，则将蕴涵式$C\rightarrow C^{II}$加入集合$L$，并且将形式概念$(C^I，C^{II})$加入集合$\mathcal{B}$中。</p><p>4.对得到的每一个内涵$C^{II}$，将它的上层邻居$C^{II}\bigcup {m}且m\notin C^{II}$加入候选集合$C$中。</p><p>5.等待基数为$k$的所有候选集合都处理完毕，如果$k&lt;M$则递增$k$，并且跳到2；否则算法结束，返回形式概念集合$\mathcal{B}$和蕴涵式集合$L$。</p><img src="/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335.jpg" srcset="/img/loading.gif" class><p><img src="/peerless.github.io/2020/07/01/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/images/2015-CLA-NextCloures%20Parallel%20Computation%20of%20the%20Canonical%20Base/1593623335-1593786388801.jpg" srcset="/img/loading.gif" alt></p><p>词汇积累：</p><ul><li>backtracking：回溯。</li><li>level-wise：逐级。</li><li>coincide：一致。</li><li>inductive：归纳的；感应的，诱导的。</li><li>suffices：满足。</li></ul><h2 id="4-Benchmarks"><a href="#4-Benchmarks" class="headerlink" title="4 Benchmarks"></a>4 Benchmarks</h2><p style="text-indent:2em">本节主要目的在于将我们并行式的算法与线性序的$NextCloures$算法在计算主基时进行定性与定量的比较分析。 </p><p style="text-indent:2em">结果：并行式算法在一定的极限下，算法的运行时间与可用CPU的数量成正比的减少。 </p><p>​        特点：同一层的候选集不能彼此影响，即线程之间不需要通信。</p><p>​        异常情况：在某些情况下，当使用所有可用的CPU时，计算时间会增加。（原因未知）—-可能是由于平台或操作系统的一些技术细节，比如基准测试过程中执行的一些后台任务，或者线程维护带来的开销。</p><p>词汇积累：</p><ul><li>benchmarks：基准测试。</li><li>overhead caused：导致的开销。</li></ul><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p style="text-indent:2em">本文介绍了计算正则基的并行式算法NextCloures。该算法以层次化的顺序（即增加基数），自下而上的构造给定形式背景的所有内涵和伪内涵的概念格。</p><p style="text-indent:2em">由于概念格中某一层的元素可以独立计算，也可以并行枚举，从而产生了计算正则基的并行算法。</p><p>可能的扩展：</p><ul><li>处理一组蕴涵或约束闭包算子给出的背景知识。</li><li>属性探索：包含专家交互，以探索部分已知形式背景的规范基。<ul><li>可以让几位专家同时回答问题。</li><li>问题的难度即前提基数与经典属性探索线性序提出的问题相比不断增加。</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
      <category>属性探索</category>
      
    </categories>
    
    
    <tags>
      
      <tag>属性探索</tag>
      
      <tag>并行算法</tag>
      
      <tag>NextCloures</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-计算机科学-并行计算学科发展历程</title>
    <link href="/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/"/>
    <url>/2020/06/30/2020-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%AD%A6%E7%A7%91%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="并行计算学科发展历程"><a href="#并行计算学科发展历程" class="headerlink" title="并行计算学科发展历程"></a>并行计算学科发展历程</h1><h2 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h2><p>​            1.作者：陈国良、张玉杰</p><p>​            2.期刊：计算机科学</p><p>​            3.时间：2020/06/11</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>回顾在并行计算学科发展所做的工作。</li><li>对非数值计算的计算方法进行介绍。</li><li>新型非冯诺依曼结构计算机体系结构的介绍。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>并行计算课程发展的5个阶段：</p><p style="text-indent:2em">非数值计算的并行算法$\rightarrow$新型非冯诺依曼计算机结构$\rightarrow$改革计算机基础课程的计算思维$\rightarrow$数据科学$\rightarrow$大数据计算理论研究。</p><p>完整的并行算法学科体系：算法理论-算法设计-算法实现-算法应用。</p><p>一体化的并行计算研究方法：并行机结构-并行算法-并行编程。</p><h2 id="2-非数值计算中的计算方法"><a href="#2-非数值计算中的计算方法" class="headerlink" title="2 非数值计算中的计算方法"></a>2 非数值计算中的计算方法</h2><p>从计算科学角度数值计算内容主要有：</p><ul><li>矩阵运算。</li><li>线性方程组的求解。</li><li>快速傅里叶变换等。</li></ul><p>非数值计算中的并行算法基本设计策略包括：</p><ul><li>串行算法的直接并行化。</li><li>从问题描述开始设计全新的并行算法。</li><li>借用已有的算法。</li><li>利用已求解问题与待求解问题两者之间的内在相似性来求解新问题。</li></ul><h2 id="3-新型非冯诺依曼计算机体系结构"><a href="#3-新型非冯诺依曼计算机体系结构" class="headerlink" title="3 新型非冯诺依曼计算机体系结构"></a>3 新型非冯诺依曼计算机体系结构</h2><p style="text-indent:2em">传统的冯诺依曼体系结构：第一代计算机（电子管计算机）、第二代计算机（晶体管计算机）、第三代计算机（集成电路计算机）、第四代计算机（大规模超大规模集成电路）</p><p>一些先进新型计算机系统结构：</p><ul><li>微程序控制器设计：<ul><li>设计了“八位运控模型”，并采用了自行提出的“寄存器传输操作语言”进行形式化描述。</li></ul></li><li>直接执行高级语言的计算机：<ul><li>研究了“直接执行的高级语言FORTRAN”机      器，介绍了对标准FORTRAN语言所作的一些限制和补充，简述了该计算机体系结构，列举典型的FORTEAN语言的直接执行过程，并自行提出了可重组结构与之配合。</li></ul></li><li>数据库计算机：<ul><li>研究了数据库计算机，实现RDF查询语言和SQL语言的转换并在此基础上实现一个对用户透明的、建立在关系数据库之上的RDF搜索引擎，以提高其海量存储和查找效率。</li></ul></li><li>光计算机：<ul><li>通过垂直偏振光、水平偏振光和无强光3个稳定的光状态表示信息的三值光计算机原理$^{[7]}$，提出基于光原理三值逻辑计算机。</li></ul></li><li>生物计算机：<ul><li>研究了基于字符串匹配原理的生物序列比对的生物计算机，在纳米计算模型上实现了DNA序列模体发现算法$^{[8]}$。</li></ul></li><li>可重构可变计算机：<ul><li>研究了“可变结构计算机系统”及其结构中的资源间相互通信问题$^{[9]}$。</li></ul></li><li>数据流计算机：<ul><li>根据MIT提出的数据流计算机概念，分析了曼彻斯特大学的数据流计算机，在国内分布式计算计算会议上发表了“数据流计算机体系结构解析”$^{[10]}$一文。</li></ul></li><li>神经计算机：<ul><li>研究神经网络在组合优化中的应用的同时，自行构建了基于Transputer阵列的“通用并行神经网络模拟系统（GP2N2S2）”$^{[10]}$，提供了高级神经网络描述语言及其编辑和编译器的执行环境，实现了程序的自动化执行。</li></ul></li><li>Transpute阵列机：<ul><li>搭建了Transputer阵列机，在中科院计算所进行了组装、调试和运行，并在其上实现了通用的Rohoman应用平台。</li></ul></li><li>量子计算机：<ul><li>在中国科大开展了量子计算研究，讨论量子计算机模型及其物理实现方案、量子计算过程、量子计算模型和量子并行算法，分析量子指数级存储容量和指数加速特征等，并在保密通信、密码安全等领域对量子信息技术进行研究$^{[12, 13]}$。</li></ul></li></ul><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p style="text-indent:2em">大数据、物联网、云计算和区块链是新一代信息技术发展中的华彩乐章。物联网使成千上万的网络传感器嵌入到现实世界中，云计算为物联网产生的海量数据提供了存储空间和在线处理模式，而大数据则让海量数据产生了价值，区块链促进海量信息可靠交互保障生产要素在区域内有序高效地流通。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2018-CS-A General Form of Attribute Exploration </title>
    <link href="/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/"/>
    <url>/2020/06/28/2018-CS-A%20General%20Form%20of%20Attribute%20Exploration%20/</url>
    
    <content type="html"><![CDATA[<!-- * @Author: peerless * @Date: 2020-06-28 18:25:20 * @LastEditTime: 2020-06-30 21:27:52 * @LastEditors: Please set LastEditors * @Description: A General Form of Attribute Exploration * @FilePath: \peer-less.github.io\source\_posts\2012CS-A General Form of Attribute Exploration .md   title:         A General Form of Attribute Exploration # 标题   subtitle:                                              # 副标题   date:          2020-06-30                              # 时间   author         peerless                                # 作者   heaeder-img:   img/post-bg-.jpg                        # 这篇文章标题背景图片   catalog:       true                                    # 是否归档   tags:                                                  # 标签      - 学术      - 属性探索         --> <h1 id="A-General-Form-of-Attribute-Exploration"><a href="#A-General-Form-of-Attribute-Exploration" class="headerlink" title="A General Form of Attribute Exploration"></a>A General Form of Attribute Exploration</h1><h2 id="行文思路简要总结"><a href="#行文思路简要总结" class="headerlink" title="行文思路简要总结"></a>行文思路简要总结</h2><p>问题：如何从经典属性探索出发获得通用的属性探索？</p><p>​            1.回顾经典属性探索算法。</p><p>​            2.通过引入3个条件扩展属性探索算法：</p><ul><li>引入两个闭包算子$c<em>{cert}(A)$与$c</em>{univ}(A)$。</li><li>不明确指定提供的反例。</li><li>只向专家询问满足$c<em>{cert}(A)\subsetneq B\subset c</em>{univ}(A)$的蕴涵式$A\rightarrow B$。</li></ul><p>​            3.对通用属性探索算法进行非冗余性与完备性验证。</p><p>​            4.对通用算法询问蕴涵式时进行改进，使算法向专家询问的蕴涵式的数量是最小的。</p><ul><li>改进方法：若属性集$A=c<em>{cert}(A)\subsetneq c</em>{univ}(A)$，则向专家询问蕴涵式$A\rightarrow c_{univ}(A)$。</li></ul><p>总结：</p><p>​            1. 该通用属性算法首先保留了经典属性探索算法的大多性质：完备性、非冗余性和向专家询问蕴涵式的数量最少等。</p><p>​            2. 该通用属性探索算法能够处理抽象给定的闭包算子和部分形式背景下给出的反例。</p><p>问题：</p><ol><li>理论层面：该通用属性探索算法理论上的时间复杂度仍为指数级，无法在多项式时间内计算主基。</li><li>应用层面：该通用属性探索算法并未说明在实际的应用领域中效果如何。</li></ol><h2 id="论文基本信息："><a href="#论文基本信息：" class="headerlink" title="论文基本信息："></a>论文基本信息：</h2><p>​            1.作者：Daniel Borchmann（德累斯顿州立大学数学与科学学院代数研究所丹尼尔·博尔赫曼）</p><p>​            2.期刊：Computer Science</p><p>​            3.时间：2018/11/15</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>提出一种属性探索的一般形式。</li><li>扩展属性探索的适用性。</li><li>将属性探索的现有变种转换为一般形式，简化理论。</li></ul><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>1.属性探索的变种：</p><ul><li>部分形式背景的属性探索$^{[3]}$。</li><li>描述逻辑模型的探索$^{[1, 2]}$。</li></ul><font color="red">研究问题：寻找一种将所有变种都包含在内的一般属性探索。</font><p>可行原因：</p><pre><code>     1.属性探索算法的整体结构均保持不变。     2.属性探索的所有重要属性均保留了下来。</code></pre><p>单词积累:</p><pre><code>     discourse: 论述、谈话、演讲。</code></pre><p>​         have a close look: 仔细研究。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2 Preliminaries"></a>2 Preliminaries</h2><p>蕴涵式</p><script type="math/tex; mode=display">A\rightarrow B \Leftrightarrow A^{'}\subseteq B^{'} \Leftrightarrow B\subseteq A^{''}</script><p>一些声明：</p><ul><li><p>$Imp(M)$：$M$上的所有蕴涵式集合。</p></li><li><p>$Imp(K)$：形式背景$K$上的所有蕴涵式集合。</p></li><li><p>$Th(K)$：形式背景$K$上的所有成立的蕴涵式集合。</p></li><li><p>$L\subseteq Imp(K)$，$A\subseteq M$。如果对于所有的蕴涵式$(X\rightarrow Y)\in L$，都有$X\subsetneq A或Y\subseteq A$成立，则集合$A$在$L$下为封闭集合。则可做如下定义：</p><script type="math/tex; mode=display">  \begin{align}  L^0(A):&=A \newline  L^1(A):&=\bigcup{ \{Y\mid (X\rightarrow Y)\in L, X\subseteq A\} } \newline  L^i(A):&=L^1(L^{i-1}(A))\ for\ i > 1 \newline  L(A):&=\bigcup\limits_{i\in N}{L^i(A)}  \end{align}</script><p>  $L(A)$是基于$L$下比集合A大的最小集合。</p><p>  由上可知$L^N(A)$表示集合$A$的子集所能推出来的属性集合的子集所能推出来的属性集合。</p><p>  $Cn(L)$：在$L$下成立的所有蕴涵式集合。</p><ul><li>$B\subseteq Imp(K)$在$L$下是非冗余的。$\Leftrightarrow B\subseteq Cn(L)$。</li><li>$B\subseteq Imp(K)$在$L$下是完备的。$\Leftrightarrow Cn(B)\supseteq L$。</li><li><p>$B$是主基。$\Leftrightarrow Cn(B)=Cn(L)$。</p><p>$P$在$L$下是伪闭集，如果以下条件成立：</p></li><li><p>$P\neq L(P)$。</p></li><li><p>对于所有的伪闭集$Q\subsetneq P$有$L(Q)\subseteq P$成立。</p><p>特别的，若$L=Th(K)$，则$P$称为形式背景$K$的伪内涵。则主基可定义为：$Can(L):={P\rightarrow L(P)\mid P 在L下是伪闭集}$。</p><p>由于不知道对象$g$是否具有属性$m$，因此需要引入部分形式背景：存在一个属性集的有序对$(A, B), A, B\in M且A\bigcap B=\varnothing$所构成的集合即为部分形式背景。</p></li><li><p>若$A\bigcup B=M$，则集合称为全局对象描述，即相应对象明确具有的属性集。</p></li><li>若$A\bigcup B\neq M$，则集合称为部分对象描述，即相应对象明确不具有的属性集。</li></ul></li></ul><h2 id="3-Classical-Attribute-Exploration"><a href="#3-Classical-Attribute-Exploration" class="headerlink" title="3 Classical Attribute Exploration"></a>3 Classical Attribute Exploration</h2><p>$M$是一个有限的属性集合，$K$是基于$M$的形式背景，$p$是基于$M$的领域专家。</p><p>1.对属性集通过字典序进行初始化，并获得第一个属性$\varnothing / P$.</p><p>2.如果$P^{‘}=P$，跳到第5步；否则，令$r:=(P\rightarrow P^{“})$</p><p>3.如果专家认为$r$成立，把$r$加入蕴涵集$Imp(K)$中。</p><p>4.如果专家认为$r$不成立，给出相应的一个反例$C$，将$C$（及其相应的属性）作为新对象加入当前工作形式背景$K$中。</p><p>5.找到字典序$P$的下一个属性集$Q$，若不存在下一个，则算法终止，否则，将$P$设置为$Q$。</p><h2 id="4-Generalizing-Attribute-Exploration"><a href="#4-Generalizing-Attribute-Exploration" class="headerlink" title="4 Generalizing Attribute Exploration"></a>4 Generalizing Attribute Exploration</h2><font color="red">推广目的：用更抽象的术语来描述属性探索，以允许该算法在经典属性探索算法之外的应用。</font><p>3个扩展：</p><p>1.提出两个闭包操作符：$c<em>{univ}、c</em>{cert}$。</p><ul><li><p>$c<em>{univ}$：我们已知的全部领域知识。$c</em>{univ}(A)$可以从$A$推出的属性集。</p></li><li><p>$c<em>{cert}$：我们已知的某些知识。$c</em>{cert}(A)$确定可以从$A$推出的属性集。</p></li></ul><p>2.采用如下方法对算法进行扩展：</p><ul><li><p>提供反例时，不需要完全指定。</p></li><li><p>只需要所提反例所拥有的信息与所给的蕴涵式相矛盾即可。</p></li><li><p>提供关于该对象具有哪些属性以及不具有的属性信息即可。</p></li></ul><p>3.我们向专家提出的蕴涵式是一种特殊的形式：</p><ul><li>搜索关于$c<em>{cert}$和$c</em>{univ}$未确定的蕴涵式$A\rightarrow B$，即$c<em>{cert}(A)\subsetneq B\subseteq c</em>{univ}(A)$。对于这样的蕴涵式，我们不能从$c<em>{cert}$和$c</em>{univ}$推断出属性集$c_{univ}(A) \backslash B$是否能从$A$推出或不能推出，因此，我们需要向专家询问。</li></ul><p>Algorithm(General Attribute Exploration)</p><p>   $q$为部分领域专家。  </p><p>   ​    1. $K = \varnothing$<br>   ​    2. 对于有限属性集$A\subseteq M$，若存在有限属性集$B$，有$c<em>{cert}(A)\subsetneq B\subseteq c</em>{univ}(A)$成立，则考虑蕴涵式$A\rightarrow B$；如果不存在，则算法终止，输出$K$与$c_{cert}$。</p><p>   ​    3. 若$q$认为$A\rightarrow B$成立，那么更新$c<em>{cert}^{‘}=X\longmapsto c</em>{cert}(L(c_{cert}(X) ) )$。</p><p>​        4. 否则，$(C, D)=q(A\rightarrow B)$作为反例，加入形式背景$K$。</p><p>​        5. 对所有的反例$(C, D)$有：</p><script type="math/tex; mode=display">\begin{align}C':&=c_{cert}(C) \newlineD':&=D\bigcup \{ {m\in M\backslash D\mid c_{cert}(C\bigcup \{m\})\bigcap D\neq \varnothing}\}\end{align}</script><p>​        6. 对所有的$X\subseteq M，X\longmapsto c<em>{univ}(X)\bigcap K(X)$，即将$c</em>{univ}$更新为$c<em>{univ}^{‘}(X)=c</em>{univ}(X)\bigcap K(X)$。</p><p>​        7. 跳转到2。</p><p>两条性质：非冗余性与完备性。</p><h2 id="5-Computing-Undecided-Implications"><a href="#5-Computing-Undecided-Implications" class="headerlink" title="5 Computing Undecided Implications"></a>5 Computing Undecided Implications</h2><p>目的：使向专家询问蕴涵式的数量是最小的。</p><p>原因：该通用属性探索算法对询问蕴涵式的顺序未做约束。（可能询问以确定的蕴涵式）</p><p>经典属性探索下计算未确定的蕴涵式：</p><p>​        在基于已知蕴涵式$k$的条件下，计算属性集$P$字典序之后的最小属性集$Q(Q\subseteq M，且Q不是当前工作形式背景的内涵)$。那么就需要向专家询问蕴涵式$Q\rightarrow Q^{“}$。</p><p>通用属性探索下计算未确定的蕴涵式：</p><p>​        为了保证向专家询问的蕴涵式的数量是最小的，将通用属性探索算法的第二步更改为：$对于有限属性集A\subseteq M$，有$A=c<em>{cert}(A)\subsetneq c</em>{univ}(A)$并且$A$是$\subseteq-minimal$成立，则考虑蕴涵式$A\rightarrow c_{univ}(A)$。</p><p>在给出算法总是能得到最小数量的询问蕴涵式前，先给出如下定义：</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h2><p style="text-indent:2em">从使用领域专家的经典的属性探索中推导出一种更加通用的属性探索算法。它能够处理抽象给定的闭包算子，并且可以处理部分给定的反例。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
